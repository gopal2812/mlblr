{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_using_LSTM_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopal2812/mlblr/blob/master/Assignment774classSentiment_Analysis_using_LSTM_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYiRsFGD6iUC"
      },
      "source": [
        "# 0 TorchText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXHZoEJ8z3ru",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "fe19ee67-593a-4017-d9e2-6ee0bd454011"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3bf74fdd-61aa-4661-9b96-c0149d8605ef\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3bf74fdd-61aa-4661-9b96-c0149d8605ef\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp5IzBGsPGHs"
      },
      "source": [
        "## Dataset Preview\n",
        "\n",
        "Your first step to deep learning in NLP. We will be mostly using PyTorch. Just like torchvision, PyTorch provides an official library, torchtext, for handling text-processing pipelines. \n",
        "\n",
        "We will be using standford sentiment Treebank dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIZ5b8Gz6KJ_",
        "outputId": "5c3b710c-b09a-405e-c656-99bb6d7f17b5"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')\r\n",
        "\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3gHS9sz7mb-",
        "outputId": "47fbed80-2078-40a6-bc52-6709e5cdb370"
      },
      "source": [
        "!ls /content/gdrive/MyDrive/end/stanfordSentimentTreebank.zip\r\n",
        "!cp '/content/gdrive/MyDrive/end/stanfordSentimentTreebank.zip' stanfordSentimentTreebank.zip\r\n",
        "!unzip -q -o stanfordSentimentTreebank.zip "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/end/stanfordSentimentTreebank.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImEfBDFR9pMo",
        "outputId": "62b621fb-6571-4758-f583-06645523f545"
      },
      "source": [
        "#!rm -rf stanfordSentimentTreebank\r\n",
        "!ls -l stanfordSentimentTreebank/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 19920\n",
            "-rwxr-xr-x 1 root root  1290263 Oct  9  2013 datasetSentences.txt\n",
            "-rwxr-xr-x 1 root root    83764 Oct  9  2013 datasetSplit.txt\n",
            "-rwxr-xr-x 1 root root 12010637 Oct  9  2013 dictionary.txt\n",
            "-rwxr-xr-x 1 root root  1195613 Feb  2  2013 original_rt_snippets.txt\n",
            "-rwxr-xr-x 1 root root     2357 Oct  9  2013 README.txt\n",
            "-rwxr-xr-x 1 root root  3263577 Oct  9  2013 sentiment_labels.txt\n",
            "-rwxr-xr-x 1 root root  1226029 Feb  2  2013 SOStr.txt\n",
            "-rwxr-xr-x 1 root root  1308918 Feb  2  2013 STree.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "_PZIPCpVEyjj",
        "outputId": "138421fd-b8a7-46d3-dc89-bab1235a45e4"
      },
      "source": [
        "import pandas as pd\r\n",
        "df = pd.read_csv('stanfordSentimentTreebank/datasetSentences.txt',sep='\\t')\r\n",
        "df.tail()\r\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11850</th>\n",
              "      <td>11851</td>\n",
              "      <td>A real snooze .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11851</th>\n",
              "      <td>11852</td>\n",
              "      <td>No surprises .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11852</th>\n",
              "      <td>11853</td>\n",
              "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11853</th>\n",
              "      <td>11854</td>\n",
              "      <td>Her fans walked out muttering words like `` ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11854</th>\n",
              "      <td>11855</td>\n",
              "      <td>In this case zero .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_index                                           sentence\n",
              "11850           11851                                    A real snooze .\n",
              "11851           11852                                     No surprises .\n",
              "11852           11853  We 've seen the hippie-turned-yuppie plot befo...\n",
              "11853           11854  Her fans walked out muttering words like `` ho...\n",
              "11854           11855                                In this case zero ."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "HexrEBYVM6fv",
        "outputId": "a505ea5e-5454-4cf6-fa98-0559b78d57c3"
      },
      "source": [
        "df = pd.read_csv('stanfordSentimentTreebank/datasetSplit.txt',sep='\\t')\r\n",
        "df.tail()\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index,splitset_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11850</th>\n",
              "      <td>11851,1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11851</th>\n",
              "      <td>11852,1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11852</th>\n",
              "      <td>11853,1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11853</th>\n",
              "      <td>11854,1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11854</th>\n",
              "      <td>11855,1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      sentence_index,splitset_label\n",
              "11850                       11851,1\n",
              "11851                       11852,1\n",
              "11852                       11853,1\n",
              "11853                       11854,1\n",
              "11854                       11855,1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "84HQbq7uNMab",
        "outputId": "e8f05b55-ff04-4bd5-9a20-fe1b3341ba16"
      },
      "source": [
        "df = pd.read_csv('stanfordSentimentTreebank/SOStr.txt',sep='\\t')\r\n",
        "df.tail()\r\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11849</th>\n",
              "      <td>A|real|snooze|.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11850</th>\n",
              "      <td>No|surprises|.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11851</th>\n",
              "      <td>We|'ve|seen|the|hippie-turned-yuppie|plot|befo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11852</th>\n",
              "      <td>Her|fans|walked|out|muttering|words|like|``|ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11853</th>\n",
              "      <td>In|this|case|zero|.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\n",
              "11849                                    A|real|snooze|.                                                                                                                                   \n",
              "11850                                     No|surprises|.                                                                                                                                   \n",
              "11851  We|'ve|seen|the|hippie-turned-yuppie|plot|befo...                                                                                                                                   \n",
              "11852  Her|fans|walked|out|muttering|words|like|``|ho...                                                                                                                                   \n",
              "11853                                In|this|case|zero|.                                                                                                                                   "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "7DiO6b_uOBY9",
        "outputId": "557117f1-c36a-4639-e5dd-41bfd70bba70"
      },
      "source": [
        "df = pd.read_csv('stanfordSentimentTreebank/sentiment_labels.txt',sep='\\t')\r\n",
        "df.tail()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phrase ids|sentiment values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>239227</th>\n",
              "      <td>239227|0.36111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239228</th>\n",
              "      <td>239228|0.38889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239229</th>\n",
              "      <td>239229|0.33333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239230</th>\n",
              "      <td>239230|0.88889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239231</th>\n",
              "      <td>239231|0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       phrase ids|sentiment values\n",
              "239227              239227|0.36111\n",
              "239228              239228|0.38889\n",
              "239229              239229|0.33333\n",
              "239230              239230|0.88889\n",
              "239231                  239231|0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "NZ4yCv_qOCeo",
        "outputId": "34096cd5-f1ab-4bb9-8cf8-857da3343d0b"
      },
      "source": [
        "df = pd.read_csv('stanfordSentimentTreebank/dictionary.txt',sep='\\t')\r\n",
        "df.tail()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>!|0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>239226</th>\n",
              "      <td>zoning ordinances to protect your community fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239227</th>\n",
              "      <td>zzzzzzzzz|179256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239228</th>\n",
              "      <td>élan|220442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239229</th>\n",
              "      <td>É|220443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239230</th>\n",
              "      <td>É um passatempo descompromissado|220444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      !|0\n",
              "239226  zoning ordinances to protect your community fr...\n",
              "239227                                   zzzzzzzzz|179256\n",
              "239228                                        élan|220442\n",
              "239229                                           É|220443\n",
              "239230            É um passatempo descompromissado|220444"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDR7Tg_v9FBa"
      },
      "source": [
        "import sys\r\n",
        "import pickle\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "dir_corpus = ''\r\n",
        "fname_valence = './valence.pkl'\r\n",
        "\r\n",
        "def read_text():\r\n",
        "\tifname = dir_corpus + 'stanfordSentimentTreebank/SOStr.txt'\r\n",
        "\tlines = open(ifname, 'r').read().split('\\n')\r\n",
        "\r\n",
        "\ttexts = []\r\n",
        "\tfor line in lines:\r\n",
        "\t\tparams = line.split('|')\r\n",
        "\t\tif len(params) > 1:\r\n",
        "\t\t\ttext = ' '.join(params)\r\n",
        "\t\t\ttexts.append(text)\r\n",
        "\r\n",
        "\treturn texts\r\n",
        "\r\n",
        "def read_splitlabel():\r\n",
        "\tifname = dir_corpus + 'stanfordSentimentTreebank/datasetSplit.txt'\r\n",
        "\tlines = open(ifname, 'r').read().split('\\n')\r\n",
        "\r\n",
        "\tsplitlabels = []\r\n",
        "\tfor line in lines[1:]:\r\n",
        "\t\tparams = line.split(',')\r\n",
        "\t\tif len(params) == 2:\r\n",
        "\t\t\tsplitlabels.append(int(params[1]))\r\n",
        "\t\r\n",
        "\treturn splitlabels\r\n",
        "\r\n",
        "def read_sentiscore():\r\n",
        "\tifname = dir_corpus + 'stanfordSentimentTreebank/sentiment_labels.txt'\r\n",
        "\tlines = open(ifname, 'r').read().split('\\n')\r\n",
        "\r\n",
        "\tsentiscores = []\r\n",
        "\tfor line in lines[1:]:\r\n",
        "\t\tparams = line.split('|')\r\n",
        "\t\tif len(params) == 2:\r\n",
        "\t\t\tsentiscores.append(float(params[1]))\r\n",
        "\r\n",
        "\treturn sentiscores\r\n",
        "\r\n",
        "def read_phraseid():\r\n",
        "\tifname = dir_corpus + 'stanfordSentimentTreebank/dictionary.txt'\r\n",
        "\tlines = open(ifname, 'r').read().split('\\n')\r\n",
        "\r\n",
        "\tphraseid = {}\r\n",
        "\tfor line in lines:\r\n",
        "\t\tparams = line.split('|')\r\n",
        "\t\tif len(params) == 2:\r\n",
        "\t\t\tphraseid[params[0]] = int(params[1])\r\n",
        "\r\n",
        "\treturn phraseid\r\n",
        "\r\n",
        "def prepare_valence():\r\n",
        "\ttexts = read_text()\r\n",
        "\tsplitlabels = read_splitlabel()\r\n",
        "\tsentiscores = read_sentiscore()\r\n",
        "\tphraseid = read_phraseid()\r\n",
        "\r\n",
        "\ttrain_text = []\r\n",
        "\ttrain_label = []\r\n",
        "\t\r\n",
        "\tvalid_text = []\r\n",
        "\tvalid_label = []\r\n",
        "\r\n",
        "\ttest_text = []\r\n",
        "\ttest_label = []\r\n",
        "\r\n",
        "\tn_sample = len(texts)\r\n",
        "\tif n_sample == len(splitlabels) and len(sentiscores) == len(phraseid):\r\n",
        "\t\tprint('%d samples'%(n_sample))\r\n",
        "\telse:\r\n",
        "\t\tprint('reading fail')\r\n",
        "\t#for i, didx in enumerate(splitlabels):\r\n",
        "\t#\tprint(didx)\r\n",
        "\tfor i, didx in enumerate(splitlabels):\r\n",
        "\t\tif didx == 1:\r\n",
        "\t\t\tlist_text = train_text\r\n",
        "\t\t\tlist_label = train_label\r\n",
        "\t\telif didx == 3:\r\n",
        "\t\t\t#print(\"3\")\r\n",
        "\t\t\tlist_text = valid_text\r\n",
        "\t\t\tlist_label = valid_label\r\n",
        "\t\telif didx == 2:\r\n",
        "\t\t\tlist_text = test_text\r\n",
        "\t\t\tlist_label = test_label\r\n",
        "\t\tlist_text.append(texts[i])\r\n",
        "\t\tlist_label.append(sentiscores[phraseid[texts[i]]])\r\n",
        "\t\t#print(\"length %d %d\",len(train_text), len(valid_text))\r\n",
        "\treturn ((train_text, train_label), (valid_text, valid_label), (test_text, test_label))\r\n",
        " \r\n",
        "\t\t#with open(fname_valence, 'wb') as tokens:\r\n",
        "\t\t#\t\tpickle.dump(dataset, tokens)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lhxScgi_CfU",
        "outputId": "e4433fe9-45f0-4e8d-9b51-710e6e4065ea"
      },
      "source": [
        "#!ls\r\n",
        "(train, train_label), (valid, valid_label), (test, test_label) = prepare_valence()\r\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11855 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpFek-iwH0bW",
        "outputId": "35f320e6-bb91-4459-dc1b-94f48140b468"
      },
      "source": [
        "(len(train), len(valid), len(test))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8544, 1101, 2210)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ6o_79ISSVb"
      },
      "source": [
        "## Defining Fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e63g08ijOrf7"
      },
      "source": [
        "Now we shall be defining LABEL as a LabelField, which is a subclass of Field that sets sequen tial to False (as it’s our numerical category class). TWEET is a standard Field object, where we have decided to use the spaCy tokenizer and convert all the text to lower‐ case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk8IP4SK1Lrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fa44c34-6a85-4bfe-d4fa-3d8299776d67"
      },
      "source": [
        "# Import Library\n",
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext import data \n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6192363c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6bKQax2Mf_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd731d5-53e4-4fa8-e9b6-65ae88279580"
      },
      "source": [
        "Sentence = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
        "Label = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)\n",
        "\n",
        "print(Sentence)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torchtext.data.field.Field object at 0x7f6189cc77b8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX-lYIe_O7Vy"
      },
      "source": [
        "Having defined those fields, we now need to produce a list that maps them onto the list of rows that are in the CSV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VawdWq36O6td"
      },
      "source": [
        "fields = [('sentence', Sentence),('labels',Label)]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbtZ-Ph2P1xL"
      },
      "source": [
        "Armed with our declared fields, lets convert from pandas to list to torchtext. We could also use TabularDataset to apply that definition to the CSV directly but showing an alternative approach too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3OLcJ5B7rHz"
      },
      "source": [
        "example_train = [data.Example.fromlist([train[i], train_label[i]], fields) for i in range(len(train))]\r\n",
        "example_valid = [data.Example.fromlist([valid[i], valid_label[i]], fields) for i in range(len(valid))] "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT-flpH-P1cd"
      },
      "source": [
        "# Creating dataset\n",
        "#twitterDataset = data.TabularDataset(path=\"tweets.csv\", format=\"CSV\", fields=fields, skip_header=True)\n",
        "\n",
        "trainDataset = data.Dataset(example_train, fields)\n",
        "validDataset = data.Dataset(example_valid, fields)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6ZnyCPaR08F"
      },
      "source": [
        "Finally, we can split into training, testing, and validation sets by using the split() method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykvsCGQMR6UD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8094568a-0909-4b67-c047-5e229978fac8"
      },
      "source": [
        "(len(trainDataset), len(validDataset))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8544, 1101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kix8P2IKSBaV"
      },
      "source": [
        "An example from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUpEOQruR9JL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e11833-349b-46f0-cdc9-8785199dc2ff"
      },
      "source": [
        "vars(trainDataset.examples[10])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 0.90278,\n",
              " 'sentence': ['Good',\n",
              "  'fun',\n",
              "  ',',\n",
              "  'good',\n",
              "  'action',\n",
              "  ',',\n",
              "  'good',\n",
              "  'acting',\n",
              "  ',',\n",
              "  'good',\n",
              "  'dialogue',\n",
              "  ',',\n",
              "  'good',\n",
              "  'pace',\n",
              "  ',',\n",
              "  'good',\n",
              "  'cinematography',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKdllP3FST4N"
      },
      "source": [
        "## Building Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuvWQ-SpSmSz"
      },
      "source": [
        "At this point we would have built a one-hot encoding of each word that is present in the dataset—a rather tedious process. Thankfully, torchtext will do this for us, and will also allow a max_size parameter to be passed in to limit the vocabu‐ lary to the most common words. This is normally done to prevent the construction of a huge, memory-hungry model. We don’t want our GPUs too overwhelmed, after all. \n",
        "\n",
        "Let’s limit the vocabulary to a maximum of 5000 words in our training set:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx955u93SGeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de74d84-38ae-4bc7-f577-e535aea0d5a5"
      },
      "source": [
        "#Tweet.build_vocab(train)\n",
        "#Label.build_vocab(train)\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "Sentence.build_vocab(trainDataset, \n",
        "                     max_size = MAX_VOCAB_SIZE, \n",
        "                     vectors = \"glove.6B.100d\", \n",
        "                     unk_init = torch.Tensor.normal_)\n",
        "\n",
        "Label.build_vocab(trainDataset)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:42, 2.14MB/s]                           \n",
            "100%|█████████▉| 399442/400000 [00:16<00:00, 26421.49it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvyEeEjXTGhX"
      },
      "source": [
        "By default, torchtext will add two more special tokens, <unk> for unknown words and <pad>, a padding token that will be used to pad all our text to roughly the same size to help with efficient batching on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA3tIESdcJdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89bdc00f-1c79-4800-869a-27be96d765a5"
      },
      "source": [
        "print('Size of input vocab : ', len(Sentence.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Sentence.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  17200\n",
            "Size of label vocab :  74\n",
            "Top 10 words appreared repeatedly : [('.', 8041), (',', 7131), ('the', 6087), ('and', 4474), ('of', 4446), ('a', 4423), ('to', 3024), ('-', 2739), (\"'s\", 2544), ('is', 2540)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7f6189d7f1e0>, {0.77778: 0, 0.72222: 1, 0.27778: 2, 0.22222: 3, 0.5: 4, 0.83333: 5, 0.66667: 6, 0.33333: 7, 0.75: 8, 0.80556: 9, 0.25: 10, 0.76389: 11, 0.79167: 12, 0.73611: 13, 0.16667: 14, 0.69444: 15, 0.20833: 16, 0.44444: 17, 0.30556: 18, 0.31944: 19, 0.38889: 20, 0.88889: 21, 0.68056: 22, 0.81944: 23, 0.29167: 24, 0.61111: 25, 0.26389: 26, 0.23611: 27, 0.70833: 28, 0.63889: 29, 0.34722: 30, 0.55556: 31, 0.11111: 32, 0.65278: 33, 0.18056: 34, 0.375: 35, 0.84722: 36, 0.51389: 37, 0.875: 38, 0.19444: 39, 0.36111: 40, 0.86111: 41, 0.40278: 42, 0.43056: 43, 0.625: 44, 0.41667: 45, 0.45833: 46, 0.59722: 47, 0.58333: 48, 0.13889: 49, 0.48611: 50, 0.125: 51, 0.15278: 52, 0.52778: 53, 0.54167: 54, 0.47222: 55, 0.56944: 56, 0.055556: 57, 0.90278: 58, 0.083333: 59, 0.91667: 60, 0.097222: 61, 0.93056: 62, 0.94444: 63, 0.069444: 64, 0.041667: 65, 1.0: 66, 0.013889: 67, 0.027778: 68, 0.95833: 69, 0.0: 70, 0.97222: 71, 0.98611: 72, 0.63194: 73})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwjD2-ebTeUX"
      },
      "source": [
        "**Lots of stopwords!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLWW221gTpNs"
      },
      "source": [
        "Now we need to create a data loader to feed into our training loop. Torchtext provides the BucketIterator method that will produce what it calls a Batch, which is almost, but not quite, like the data loader we used on images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQqMhMoDUDmn"
      },
      "source": [
        "But at first declare the device we are using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfo2QhGJUK4l"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK2ORoqdTNsM"
      },
      "source": [
        "train_iterator, valid_iterator = data.BucketIterator.splits((trainDataset, validDataset), batch_size = 32, \n",
        "                                                            sort_key = lambda x: len(x.sentence),\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg7gTFQO4fby"
      },
      "source": [
        "Save the vocabulary for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niE9Cc6-2bD_"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Sentence.vocab.stoi, tokens)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AbsQwqkVyAy"
      },
      "source": [
        "## Defining Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4PED4HJWH4t"
      },
      "source": [
        "We use the Embedding and LSTM modules in PyTorch to build a simple model for classifying tweets.\n",
        "\n",
        "In this model we create three layers. \n",
        "1. First, the words in our tweets are pushed into an Embedding layer, which we have established as a 300-dimensional vector embedding. \n",
        "2. That’s then fed into a 2 stacked-LSTMs with 100 hidden features (again, we’re compressing down from the 300-dimensional input like we did with images). We are using 2 LSTMs for using the dropout.\n",
        "3. Finally, the output of the LSTM (the final hidden state after processing the incoming tweet) is pushed through a standard fully connected layer with three outputs to correspond to our three possible classes (negative, positive, or neutral)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43pVRccMT0bT"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, bidirectional, pad_idx):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           bidirectional = bidirectional,\n",
        "                           batch_first=True)\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.dropout(self.fc(hidden))   \n",
        "        \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwBoGE_X_Fl8"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Sentence.vocab)\n",
        "embedding_dim = 100\n",
        "num_hidden_nodes = 256\n",
        "num_output_nodes = 74\n",
        "num_layers = 2\n",
        "\n",
        "dropout = 0.2\n",
        "bidirectional = True\n",
        "PAD_IDX = Sentence.vocab.stoi[Sentence.pad_token]\n",
        "\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout, bidirectional, PAD_IDX)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-pOMqzJ3eTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac21a049-492b-4e91-be78-66f9e0b8afaf"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(17200, 100, padding_idx=1)\n",
            "  (encoder): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=74, bias=True)\n",
            ")\n",
            "The model has 4,049,162 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXajorf5Xz7t"
      },
      "source": [
        "## Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrE9RpMtZ1Vs"
      },
      "source": [
        "First define the optimizer and loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u86JWdlXvu5"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6phd4xoUeM-",
        "outputId": "f00d0111-de05-463c-9d0a-32ed0358d8b9"
      },
      "source": [
        "pretrained_embeddings = Sentence.vocab.vectors\r\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([17200, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwnVC8shUlqV",
        "outputId": "579d2954-9c1c-4ab6-aaae-6c37cc0d69fc"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0166, -0.4668,  2.0909,  ..., -1.4692,  0.4476, -0.7223],\n",
              "        [-0.0791, -0.2089, -0.3442,  ...,  0.4657,  0.6297, -1.7395],\n",
              "        [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
              "        ...,\n",
              "        [ 0.5732, -1.0756, -0.1600,  ...,  0.4548,  0.2344,  0.0364],\n",
              "        [ 1.0372,  1.5194, -1.0455,  ..., -0.6074, -1.3146,  0.4511],\n",
              "        [-0.1327, -0.8911,  0.2346,  ..., -0.1750, -0.1385,  0.6226]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgwrhf3VU1Dr",
        "outputId": "7c511f0b-a116-4431-c8bb-da6550fab399"
      },
      "source": [
        "UNK_IDX = Sentence.vocab.stoi[Sentence.unk_token]\r\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\r\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(embedding_dim)\r\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
            "        ...,\n",
            "        [ 0.5732, -1.0756, -0.1600,  ...,  0.4548,  0.2344,  0.0364],\n",
            "        [ 1.0372,  1.5194, -1.0455,  ..., -0.6074, -1.3146,  0.4511],\n",
            "        [-0.1327, -0.8911,  0.2346,  ..., -0.1750, -0.1385,  0.6226]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brPLMOxAUarP"
      },
      "source": [
        "# push to cuda if available\r\n",
        "model = model.to(device)\r\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VCJtNb3Zt8w"
      },
      "source": [
        "The main thing to be aware of in this new training loop is that we have to reference `batch.tweets` and `batch.labels` to get the particular fields we’re interested in; they don’t fall out quite as nicely from the enumerator as they do in torchvision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WjEPLKsAiS_"
      },
      "source": [
        "**Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDWNnGK3Y5oJ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        tweet, tweet_lengths = batch.sentence   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(tweet, tweet_lengths).squeeze()  \n",
        "        \n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.labels)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.labels)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZcHhkkvAsCt"
      },
      "source": [
        "**Evaluation Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHEe-zSVAriL"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            tweet, tweet_lengths = batch.sentence\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(tweet, tweet_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "            acc = binary_accuracy(predictions, batch.labels)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6LJFW7HaJoV"
      },
      "source": [
        "**Let's Train and Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq330XlnaEU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "549d4242-32a2-44b7-e0a1-1450a00225cc"
      },
      "source": [
        "N_EPOCHS = 15\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "#freeze embeddings\n",
        "model.embedding.weight.requires_grad = unfrozen = False\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 399442/400000 [00:30<00:00, 26421.49it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 4.299 | Train Acc: 2.48%\n",
            "\t Val. Loss: 4.296 |  Val. Acc: 2.41% \n",
            "\n",
            "\tTrain Loss: 4.295 | Train Acc: 2.63%\n",
            "\t Val. Loss: 4.293 |  Val. Acc: 3.48% \n",
            "\n",
            "\tTrain Loss: 4.294 | Train Acc: 2.98%\n",
            "\t Val. Loss: 4.292 |  Val. Acc: 3.26% \n",
            "\n",
            "\tTrain Loss: 4.293 | Train Acc: 2.80%\n",
            "\t Val. Loss: 4.290 |  Val. Acc: 2.95% \n",
            "\n",
            "\tTrain Loss: 4.290 | Train Acc: 3.18%\n",
            "\t Val. Loss: 4.304 |  Val. Acc: 2.05% \n",
            "\n",
            "\tTrain Loss: 4.296 | Train Acc: 2.90%\n",
            "\t Val. Loss: 4.291 |  Val. Acc: 2.86% \n",
            "\n",
            "\tTrain Loss: 4.291 | Train Acc: 3.23%\n",
            "\t Val. Loss: 4.288 |  Val. Acc: 3.88% \n",
            "\n",
            "\tTrain Loss: 4.290 | Train Acc: 3.17%\n",
            "\t Val. Loss: 4.289 |  Val. Acc: 2.95% \n",
            "\n",
            "\tTrain Loss: 4.290 | Train Acc: 3.17%\n",
            "\t Val. Loss: 4.289 |  Val. Acc: 3.30% \n",
            "\n",
            "\tTrain Loss: 4.289 | Train Acc: 3.64%\n",
            "\t Val. Loss: 4.288 |  Val. Acc: 3.12% \n",
            "\n",
            "\tTrain Loss: 4.288 | Train Acc: 3.66%\n",
            "\t Val. Loss: 4.290 |  Val. Acc: 2.95% \n",
            "\n",
            "\tTrain Loss: 4.286 | Train Acc: 3.73%\n",
            "\t Val. Loss: 4.288 |  Val. Acc: 2.77% \n",
            "\n",
            "\tTrain Loss: 4.288 | Train Acc: 3.63%\n",
            "\t Val. Loss: 4.288 |  Val. Acc: 3.39% \n",
            "\n",
            "\tTrain Loss: 4.286 | Train Acc: 3.89%\n",
            "\t Val. Loss: 4.291 |  Val. Acc: 3.12% \n",
            "\n",
            "\tTrain Loss: 4.285 | Train Acc: 4.03%\n",
            "\t Val. Loss: 4.285 |  Val. Acc: 3.70% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZgzB0ZkHVTI"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZZfnWo0abRx"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    \n",
        "    categories = {0: \"Negative\", 1:\"Positive\", 2:\"Neutral\"}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTkHLEipIlM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "629d1d6d-47ba-48fe-c5fd-e581f564984a"
      },
      "source": [
        "classify_tweet(\"A valid explanation for why Trump won't let women on the golf course.\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Neutral'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVjCuKK_LVEF"
      },
      "source": [
        "## Discussion on Data Augmentation Techniques \n",
        "\n",
        "You might wonder exactly how you can augment text data. After all, you can’t really flip it horizontally as you can an image! :D \n",
        "\n",
        "In contrast to data augmentation in images, augmentation techniques on data is very specific to final product you are building. As its general usage on any type of textual data doesn't provides a significant performance boost, that's why unlike torchvision, torchtext doesn’t offer a augmentation pipeline. Due to powerful models as transformers, augmentation tecnhiques are not so preferred now-a-days. But its better to know about some techniques with text that will provide your model with a little more information for training. \n",
        "\n",
        "### Synonym Replacement\n",
        "\n",
        "First, you could replace words in the sentence with synonyms, like so:\n",
        "\n",
        "    The dog slept on the mat\n",
        "\n",
        "could become\n",
        "\n",
        "    The dog slept on the rug\n",
        "\n",
        "Aside from the dog's insistence that a rug is much softer than a mat, the meaning of the sentence hasn’t changed. But mat and rug will be mapped to different indices in the vocabulary, so the model will learn that the two sentences map to the same label, and hopefully that there’s a connection between those two words, as everything else in the sentences is the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_uEfWJpL6Nq"
      },
      "source": [
        "### Random Insertion\n",
        "A random insertion technique looks at a sentence and then randomly inserts synonyms of existing non-stopwords into the sentence n times. Assuming you have a way of getting a synonym of a word and a way of eliminating stopwords (common words such as and, it, the, etc.), shown, but not implemented, in this function via get_synonyms() and get_stopwords(), an implementation of this would be as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Alm5D7WIvAC"
      },
      "source": [
        "def random_insertion(sentence, n): \n",
        "    words = remove_stopwords(sentence) \n",
        "    for _ in range(n):\n",
        "        new_synonym = get_synonyms(random.choice(words))\n",
        "        sentence.insert(randrange(len(sentence)+1), new_synonym) \n",
        "    return sentence"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqLWzwJ3Mm8h"
      },
      "source": [
        "## Random Deletion\n",
        "As the name suggests, random deletion deletes words from a sentence. Given a probability parameter p, it will go through the sentence and decide whether to delete a word or not based on that random probability. Consider of it as pixel dropouts while treating images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Dz7JJfMqyC"
      },
      "source": [
        "def random_deletion(words, p=0.5): \n",
        "    if len(words) == 1: # return if single word\n",
        "        return words\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
        "    if len(remaining) == 0: # if not left, sample a random word\n",
        "        return [random.choice(words)] \n",
        "    else:\n",
        "        return remaining"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOIbi5WzO5OU"
      },
      "source": [
        "### Random Swap\n",
        "The random swap augmentation takes a sentence and then swaps words within it n times, with each iteration working on the previously swapped sentence. Here we sample two random numbers based on the length of the sentence, and then just keep swapping until we hit n."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnkbG15HO3Yj"
      },
      "source": [
        "def random_swap(sentence, n=5): \n",
        "    length = range(len(sentence)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return sentence"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "599NpwfMR5Vm"
      },
      "source": [
        "For more on this please go through this [paper](https://arxiv.org/pdf/1901.11196.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5aeKuNCRGip"
      },
      "source": [
        "### Back Translation\n",
        "\n",
        "Another popular approach for augmenting text datasets is back translation. This involves translating a sentence from our target language into one or more other languages and then translating all of them back to the original language. We can use the Python library googletrans for this purpose. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHhNBbYrRXNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14897257-3bf8-4db5-9f6a-d8992bbce110"
      },
      "source": [
        "!!pip install googletrans==3.1.0a0\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Collecting googletrans==3.1.0a0',\n",
              " '  Downloading https://files.pythonhosted.org/packages/19/3d/4e3a1609bf52f2f7b00436cc751eb977e27040665dde2bd57e7152989672/googletrans-3.1.0a0.tar.gz',\n",
              " 'Collecting httpx==0.13.3',\n",
              " '\\x1b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)',\n",
              " '',\n",
              " '\\x1b[K     |██████                          | 10kB 25.2MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████                    | 20kB 33.5MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████████▉              | 30kB 25.9MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████████████▉        | 40kB 29.8MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████████████████████▊  | 51kB 30.5MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████████████████| 61kB 9.7MB/s ',\n",
              " '\\x1b[?25hCollecting httpcore==0.9.*',\n",
              " '\\x1b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)',\n",
              " '',\n",
              " '\\x1b[K     |███████▊                        | 10kB 31.2MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████▍                | 20kB 38.8MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████████████         | 30kB 41.8MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████████████████▊ | 40kB 27.0MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████████████████| 51kB 7.0MB/s ',\n",
              " '\\x1b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.11.8)',\n",
              " 'Collecting hstspreload',\n",
              " '\\x1b[?25l  Downloading https://files.pythonhosted.org/packages/d3/3c/cdeaf9ab0404853e77c45d9e8021d0d2c01f70a1bb26e460090926fe2a5e/hstspreload-2020.11.21-py3-none-any.whl (981kB)',\n",
              " '',\n",
              " '\\x1b[K     |▍                               | 10kB 27.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |▊                               | 20kB 34.2MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█                               | 30kB 39.1MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█▍                              | 40kB 42.8MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█▊                              | 51kB 32.4MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██                              | 61kB 27.8MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██▍                             | 71kB 27.4MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██▊                             | 81kB 28.4MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███                             | 92kB 29.8MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███▍                            | 102kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███▊                            | 112kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████                            | 122kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████▍                           | 133kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████▊                           | 143kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████                           | 153kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████▍                          | 163kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████▊                          | 174kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████                          | 184kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████▍                         | 194kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████▊                         | 204kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████                         | 215kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████▍                        | 225kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████▊                        | 235kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████                        | 245kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████▍                       | 256kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████▊                       | 266kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████                       | 276kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████▍                      | 286kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████▊                      | 296kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████                      | 307kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████▍                     | 317kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████▊                     | 327kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████                     | 337kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████▍                    | 348kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████▊                    | 358kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████                    | 368kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████▍                   | 378kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████▊                   | 389kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████                   | 399kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████▍                  | 409kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████▊                  | 419kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████                  | 430kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████▍                 | 440kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████▊                 | 450kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████                 | 460kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████▍                | 471kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████▊                | 481kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████                | 491kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████▍               | 501kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████▊               | 512kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████████               | 522kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████████▍              | 532kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████████▊              | 542kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████              | 552kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████▍             | 563kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████▊             | 573kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████████             | 583kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████████▍            | 593kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████████▊            | 604kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████            | 614kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████▍           | 624kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████▊           | 634kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████████████           | 645kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████████████▍          | 655kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████████████▊          | 665kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████████          | 675kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████████▍         | 686kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████████▊         | 696kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████████████         | 706kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████████████▍        | 716kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████████████▊        | 727kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████████        | 737kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████████▍       | 747kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████████▊       | 757kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████████████████       | 768kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████████████████▍      | 778kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████████████████▊      | 788kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████████████      | 798kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████████████▍     | 808kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████████████▊     | 819kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████████████████     | 829kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████████████████▍    | 839kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████████████████▊    | 849kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████████████    | 860kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████████████▍   | 870kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████████████▊   | 880kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████████████████████   | 890kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████████████████████▍  | 901kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████████████████████▊  | 911kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████████████████  | 921kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████████████████▍ | 931kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████████████████▊ | 942kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████████████████████ | 952kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████████████████████▍| 962kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████████████████████▊| 972kB 31.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████████████████| 983kB 31.3MB/s ',\n",
              " '\\x1b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)',\n",
              " 'Requirement already satisfied: idna==2.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)',\n",
              " 'Collecting sniffio',\n",
              " '  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl',\n",
              " 'Collecting rfc3986<2,>=1.3',\n",
              " '  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl',\n",
              " 'Collecting h11<0.10,>=0.8',\n",
              " '\\x1b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)',\n",
              " '',\n",
              " '\\x1b[K     |██████▏                         | 10kB 32.4MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████▎                   | 20kB 40.1MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████▍             | 30kB 46.8MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████████▌       | 40kB 50.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████████████████▋ | 51kB 52.4MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████████████████| 61kB 11.7MB/s ',\n",
              " '\\x1b[?25hCollecting h2==3.*',\n",
              " '\\x1b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)',\n",
              " '',\n",
              " '\\x1b[K     |█████                           | 10kB 30.6MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████                      | 20kB 41.1MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████▏                | 30kB 48.0MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████▏           | 40kB 51.6MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████████████████▏      | 51kB 54.4MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████████████████▎ | 61kB 57.5MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████████████████| 71kB 13.1MB/s ',\n",
              " '\\x1b[?25hCollecting contextvars>=2.1; python_version < \"3.7\"',\n",
              " '  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz',\n",
              " 'Collecting hpack<4,>=3.0',\n",
              " '  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl',\n",
              " 'Collecting hyperframe<6,>=5.2.0',\n",
              " '  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl',\n",
              " 'Collecting immutables>=0.9',\n",
              " '\\x1b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)',\n",
              " '',\n",
              " '\\x1b[K     |███▎                            | 10kB 26.0MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████▋                         | 20kB 34.6MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████                      | 30kB 42.4MB/s eta 0:00:01',\n",
              " '\\x1b[K     |█████████████▎                  | 40kB 47.1MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████▋               | 51kB 50.0MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████            | 61kB 53.5MB/s eta 0:00:01',\n",
              " '\\x1b[K     |███████████████████████▎        | 71kB 54.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████████████▋     | 81kB 55.2MB/s eta 0:00:01',\n",
              " '\\x1b[K     |██████████████████████████████  | 92kB 56.3MB/s eta 0:00:01',\n",
              " '\\x1b[K     |████████████████████████████████| 102kB 16.2MB/s ',\n",
              " '\\x1b[?25hBuilding wheels for collected packages: googletrans, contextvars',\n",
              " '  Building wheel for googletrans (setup.py) ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Created wheel for googletrans: filename=googletrans-3.1.0a0-cp36-none-any.whl size=16369 sha256=acc3c0c922bdbe095048d333437b9fb2f255f07f6883dd35a7974d21a38c1809',\n",
              " '  Stored in directory: /root/.cache/pip/wheels/27/7a/a0/aff3babbb775549ce6813cb8fa7ff3c0848c4dc62c20f8fdac',\n",
              " '  Building wheel for contextvars (setup.py) ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7666 sha256=26cc70a3e0e86a24f161f93639df6e878a053223f5836f77e0223b3806551c31',\n",
              " '  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de',\n",
              " 'Successfully built googletrans contextvars',\n",
              " 'Installing collected packages: h11, immutables, contextvars, sniffio, hpack, hyperframe, h2, httpcore, hstspreload, rfc3986, httpx, googletrans',\n",
              " 'Successfully installed contextvars-2.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.11.21 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 immutables-0.14 rfc3986-1.4.0 sniffio-1.2.0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuLdqv7A3yDO",
        "outputId": "1075aad6-34de-43a4-d33d-37c3f84d53bf"
      },
      "source": [
        "import random\r\n",
        "import googletrans\r\n",
        "from googletrans import Translator\r\n",
        "#import googletrans.Translator\r\n",
        "\r\n",
        "translator = Translator()\r\n",
        "sentence = ['The dog slept on the rug']\r\n",
        "\r\n",
        "available_langs = list(googletrans.LANGUAGES.keys()) \r\n",
        "trans_lang = random.choice(available_langs) \r\n",
        "print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")\r\n",
        "\r\n",
        "translations = translator.translate(sentence, dest=trans_lang) \r\n",
        "t_text = [t.text for t in translations]\r\n",
        "print(t_text)\r\n",
        "\r\n",
        "translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \r\n",
        "en_text = [t.text for t in translations_en_random]\r\n",
        "print(en_text)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translating to norwegian\n",
            "['Hunden sov på teppet']\n",
            "['The dog slept on the blanket']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}