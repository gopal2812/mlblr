{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "capastone_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopal2812/mlblr/blob/master/python_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGG7-Ga5V0f0"
      },
      "source": [
        "# Transformer based model to translate English text to Python code\r\n",
        "\r\n",
        "The goal is to  write a transformer-based model that can translats English text to python code(with proper whitespace indentations)\r\n",
        "\r\n",
        "The training dataset contains around 4600+ examples of English text to python code. \r\n",
        "- must use transformers with self-attention, multi-head, and scaled-dot product attention in the model\r\n",
        "- There is no limit on the number of training epochs or total number of parameters in the model\r\n",
        "- should have trained a separate embedding layer for python keywords and paid special attention to whitespaces, colon and other things (like comma etc)\r\n",
        "- model should to do proper indentation\r\n",
        "- model should to use newline properly\r\n",
        "- model should understand how to use colon (:)\r\n",
        "- model should generate proper python code that can run on a Python interpreter and produce proper results\r\n",
        "\r\n",
        "\r\n",
        "Some preprocessing checks on the dataset should be carried out like:\r\n",
        " - the dataset provided is divided into English and \"python-code\" pairs properly\r\n",
        "the dataset does not have anomalies w.r.t. indentations (like a mixed-use of tabs and spaces, or use of either 4 or 3 spaces, it should be 4 spaces only). Either use tabs only or 4 spaces only, not both\r\n",
        "- the length of the \"python-code\" generated is not out of your model's capacity\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMNU5vKeIDTD",
        "outputId": "bdc1c3ae-cb9f-4cff-940a-341238aa8213"
      },
      "source": [
        "!pip install -U torchtext==0.8.0\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torchtext==0.8.0 in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.8.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.8.0) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj3-MKeYhqlM"
      },
      "source": [
        "import torch\r\n",
        "from torch.jit import script, trace\r\n",
        "import torch.nn as nn\r\n",
        "from torch import optim\r\n",
        "import torch.nn.functional as F\r\n",
        "import csv\r\n",
        "import random\r\n",
        "import re\r\n",
        "import os\r\n",
        "import unicodedata\r\n",
        "import codecs\r\n",
        "from io import open\r\n",
        "import itertools\r\n",
        "import math\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from torchtext.data import Field, BucketIterator, LabelField, TabularDataset\r\n",
        "\r\n",
        "\r\n",
        "USE_CUDA = torch.cuda.is_available()\r\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7K58TvqsZfX",
        "outputId": "bb14f310-cbf5-4223-98c8-2d9182c44292"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkQxdOXFpze2",
        "outputId": "8f85e0b3-6bb2-47ef-c183-070c1e7d5b52"
      },
      "source": [
        "!ls -l drive/MyDrive/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 3228465\n",
            "-rw------- 1 root root      83296 Apr 21  2014  13033148255_BVRxxxxx9Q_A1.zip\n",
            "drwx------ 2 root root       4096 Jul 26  2014 '13033148255_BVRxxxxx9Q_A1.zip (Unzipped Files)'\n",
            "-rw------- 1 root root        151 Sep  3  2013 '20130901  po_paymentdetails members 33 to 40.gsheet'\n",
            "-rw------- 1 root root      31354 Jul  9  2017  20170707_OrderTrades_25068.pdf\n",
            "-rw------- 1 root root      54690 Dec 16  2013 'A82542 dec.pdf'\n",
            "-rw------- 1 root root      54576 Dec 16  2013 'A82542 jan.pdf'\n",
            "-rw------- 1 root root        151 Sep  2  2013 'additional information on demant letters.gsheet'\n",
            "-rw------- 1 root root     534516 Dec 16  2013 'Aircel P 2 Offer Letter.pdf'\n",
            "-rw------- 1 root root     771981 Dec 16  2013 'Aircel page 1.pdf'\n",
            "-rw------- 1 root root        151 Jul 22  2012  Amit_Cv_sample.doc.gdoc\n",
            "-rw------- 1 root root     313338 Apr  4  2017  aonla2.jpg\n",
            "-rw------- 1 root root        151 Mar  3  2009 '_AVG certification_.gdoc'\n",
            "-rw------- 1 root root        151 Nov  9  2019 'Booting_ARM_Linux_SMP_on_MPCore (1).gdoc'\n",
            "-rw------- 1 root root   25866675 Feb  8  2019  buildingapplicationswithcassandrapart11549070367188.pdf\n",
            "-rw------- 1 root root    3437061 Feb  8  2019  buildingapplicationswithcassandrapart21549070462026.pdf\n",
            "-rw------- 1 root root     252696 Mar  9 17:07  clean_data_ex_1.txt\n",
            "-rw------- 1 root root     254202 Mar  9 17:05  clean_data_ex.txt\n",
            "-rw------- 1 root root    1157291 Mar  9 15:16  clean_data.txt\n",
            "-rw------- 1 root root    1171733 Mar  8 17:17  cleaned_data.txt\n",
            "-rw------- 1 root root      94168 Mar 21  2017  CM_N58761_20032017_COMBMARGIN.pdf\n",
            "-rw------- 1 root root      94788 Feb 25  2017  CM_N58761_23022017_COMBMARGIN.pdf\n",
            "drwx------ 2 root root       4096 Apr 15  2018 'Colab Notebooks'\n",
            "-rw------- 1 root root  814496025 Mar  3 07:17 'Copy of Module 6-20210210T084940Z-001.zip'\n",
            "-rw------- 1 root root 2104340758 Mar  3 07:17 'Copy of Module 8-20210210T091931Z-002.zip'\n",
            "-rw------- 1 root root    9916637 Feb  5 18:21  cornell_movie_dialogs_corpus.zip\n",
            "-rw------- 1 root root        151 Jan 28  2009 'DANCE LORDS.gdoc'\n",
            "drwx------ 2 root root       4096 Jan 20  2020  datavisual\n",
            "-rw------- 1 root root     342832 Jan 10 06:55  DEC2020_AA03925739_TXN.pdf\n",
            "-rw------- 1 root root     504891 Dec 16  2013 'Document (10).pdf'\n",
            "-rw------- 1 root root     259485 Dec 16  2013 'Document (11).pdf'\n",
            "-rw------- 1 root root     237335 Dec 16  2013 'Document (12).pdf'\n",
            "-rw------- 1 root root     269964 Dec 16  2013 'Document (13).pdf'\n",
            "-rw------- 1 root root     268665 Dec 16  2013 'Document (14).pdf'\n",
            "-rw------- 1 root root     229784 Dec 16  2013 'Document (15).pdf'\n",
            "-rw------- 1 root root     220460 Dec 16  2013 'Document (16).pdf'\n",
            "-rw------- 1 root root     106210 Dec 16  2013 'Document (17).pdf'\n",
            "-rw------- 1 root root      72319 Dec 16  2013 'Document (19).pdf'\n",
            "-rw------- 1 root root      88987 Dec 16  2013 'Document (20).pdf'\n",
            "-rw------- 1 root root     551717 Dec 16  2013 'Document (9).pdf'\n",
            "-rw------- 1 root root        151 Aug  7  2013 'download-1374905829774(1).gsheet'\n",
            "-rw------- 1 root root     159641 Mar  9  2015  EAadhaar_1118500272553220130822105620_08022014141131_872282.pdf\n",
            "-rw------- 1 root root  189155756 Jan 19 12:36 'Educative.io - Grokking the Coding Interview - Patterns for Coding Questions-20200328T032529Z-001.zip'\n",
            "drwx------ 2 root root       4096 Dec  9 19:13  end\n",
            "-rw------- 1 root root    1122316 Mar  8 10:22  english_python_data.txt\n",
            "-rw------- 1 root root        151 Jun 11  2013 'excelsheet_gopal (1).gsheet'\n",
            "-rw------- 1 root root        151 Sep  2  2013  excelsheet_gopal.gsheet\n",
            "drwx------ 2 root root       4096 Jan 16  2020 'Exercise Files'\n",
            "drwx------ 2 root root       4096 Nov 29  2019  Ex_Files_DSF_DataMining\n",
            "drwx------ 2 root root       4096 Jan  2  2020  Ex_Files_Machine_Learning_EssT_ValueEstimate\n",
            "-rw------- 1 root root   92107420 Jun 16  2019 '[Gayle_Laakmann_McDowell]_Cracking_the_Coding_Inte(b-ok.org)6thedition.pdf'\n",
            "-rw------- 1 root root        151 Jul 24  2009 'Gmail - schedule.gdoc'\n",
            "-rw------- 1 root root     882201 Oct  9  2015  gopalgupta_akbpg4721f.zip\n",
            "-rw------- 1 root root    2317355 Jun 30  2018  GOPAL_GUPTA_BATCHA_IMAGE_CAPTIONINGv1.zip\n",
            "-rw------- 1 root root    2200027 Jun 30  2018  GOPAL_GUPTA_BATCHA_IMAGE_CAPTIONING.zip\n",
            "-rw------- 1 root root     241351 Nov 16 14:27  gopal_gupta.pdf\n",
            "-rw------- 1 root root   15064007 Jan 29 05:36 'Grokking the Object Oriented Design Interview.zip'\n",
            "-rw------- 1 root root    3479132 Jun 12  2016  IMG_5292.JPG\n",
            "-rw------- 1 root root   17105436 Jun 16  2019 '[John_Mongan,_Noah_Suojanen_Kindler,_Eric_Giguere](b-ok.cc).pdf'\n",
            "-rw------- 1 root root        151 Jun 11  2013 'Letter to PO.doc.gdoc'\n",
            "-rw------- 1 root root        151 Jun 11  2013 'member list.gsheet'\n",
            "-rw------- 1 root root     629763 Dec 16  2013 'Meriton Offer Letter.pdf'\n",
            "-rw------- 1 root root     517099 Dec 16  2013 'Meriton - RL.pdf'\n",
            "-rw------- 1 root root        151 Nov 21  2018  N58761_07-02-2017_21-11-2018_Security_Report.gsheet\n",
            "-rw------- 1 root root        151 Jul 11  2011 'Neha Gupta_CV.doc.gdoc'\n",
            "-rw------- 1 root root        151 May  8  2012 'Nutan_Gupta_(1).doc.gdoc'\n",
            "-rw------- 1 root root        151 Jan  2  2014 'nutan-pay slip (2).xlsx new.gsheet'\n",
            "drwx------ 2 root root       4096 May  8  2018 'OReilly-Spark-2018-Q2.dbc (Unzipped Files)'\n",
            "-rw------- 1 root root        151 Sep 10  2013 'palm group members current status-100913.gsheet'\n",
            "-rw------- 1 root root     209609 Jul 23  2014 'PFR Phase 2 Pricing - 5800.pdf'\n",
            "-rw------- 1 root root        151 Aug  1  2016 'Purva Whitehall Availability  with blocked New.gsheet'\n",
            "-rw------- 1 root root      10023 Nov 26  2013  RAJATGUPTA.pdf\n",
            "drwx------ 2 root root       4096 Jan  5  2020  recommendation\n",
            "-rw------- 1 root root        151 Mar  5  2013  resume_vishal.doc.gdoc\n",
            "-rw------- 1 root root      22173 Dec 16  2013 'RL Aircel.pdf'\n",
            "-rw------- 1 root root        151 Aug  5  2010  RulesRatesTakeChance.gdoc\n",
            "-rw------- 1 root root     125065 Jan 18  2017 'sealifebangkok.com: BOOKING NUMBER # 201701180018438.pdf'\n",
            "-rw------- 1 root root     156424 Sep  8  2019  STYLEBEE.png\n",
            "-rw------- 1 root root     284764 Feb 25  2014  TelephoneBill_1671856_284746308.pdf\n",
            "-rw------- 1 root root      15470 Aug 22  2018  ThresholdLearningAlgorithm.docx\n",
            "-rw------- 1 root root        151 Dec 24  2013  timelines_version_1.gsheet\n",
            "-rw------- 1 root root    6661780 Jan  6 18:19  Train.csv\n",
            "-rw------- 1 root root        151 Aug 27  2013 'TUITONS AVAILABLE UPTO 7th Class.docx.gdoc'\n",
            "-rw------- 1 root root      12293 Mar 31  2017  unknown.pdf\n",
            "-rw------- 1 root root     461735 May 14  2018 'Untitled1 (2).ipynb'\n",
            "-rw------- 1 root root        151 Oct  4 08:02 'Untitled spreadsheet.gsheet'\n",
            "-rw------- 1 root root    6865400 Jan  6 18:19  Validation.csv\n",
            "-rw------- 1 root root        151 Jul 18  2012 'Vivek Gupta.docx.gdoc'\n",
            "-rw------- 1 root root        151 Aug 21  2018  wif_adapt_index.gdoc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7WcRrmo1BM7"
      },
      "source": [
        "datasets = [[]]\r\n",
        "file_name = '/content/drive/MyDrive/clean_data_ex_1.txt'\r\n",
        "\r\n",
        "with open(file_name) as f:\r\n",
        "  #my_dict = {\"description\":[],\"code\":[]}\r\n",
        "  for line in f:\r\n",
        "    if line.startswith('#'):\r\n",
        "      comment = line.split('\\n#')\r\n",
        "      if datasets[-1] != []:\r\n",
        "        # we are in a new block\r\n",
        "        datasets.append(comment)\r\n",
        "    else:\r\n",
        "      stripped_line = line#.strip()\r\n",
        "      if stripped_line:\r\n",
        "        datasets[-1].append(stripped_line)\r\n",
        "# datasets[0].insert(0,'# write a python program to add two numbers ')        "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwhevQBOTPoc"
      },
      "source": [
        "raw_data = {'Description' : [re.sub(r\"^#(\\d)*\",'',x[0]).strip() for x in datasets], 'Code': [''.join(x[1:]) for x in datasets]}\r\n",
        "df = pd.DataFrame(raw_data, columns=[\"Description\", \"Code\"])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P64W2zTDcUe-"
      },
      "source": [
        "df['Description'][0] = \" write a python program to add two numbers\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxXXhkonTg8q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7f189645-435a-402e-c80b-68ca62afcf04"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Description</th>\n",
              "      <th>Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>write a python program to add two numbers</td>\n",
              "      <td>\\nimport string\\nfrom itertools import permuta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>write a python function to add two user provid...</td>\n",
              "      <td>\\n\\ndef add_two_numbers(num1, num2):\\n    sum ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>write a program to find and print the largest ...</td>\n",
              "      <td>\\nnum1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &gt;=...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>write a program to find and print the smallest...</td>\n",
              "      <td>\\nnum1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &lt;=...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Write a python function to merge two given lis...</td>\n",
              "      <td>\\n\\ndef merge_lists(l1, l2):\\n    return l1 + ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Description                                               Code\n",
              "0          write a python program to add two numbers  \\nimport string\\nfrom itertools import permuta...\n",
              "1  write a python function to add two user provid...  \\n\\ndef add_two_numbers(num1, num2):\\n    sum ...\n",
              "2  write a program to find and print the largest ...  \\nnum1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 >=...\n",
              "3  write a program to find and print the smallest...  \\nnum1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 <=...\n",
              "4  Write a python function to merge two given lis...  \\n\\ndef merge_lists(l1, l2):\\n    return l1 + ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlaGjIFLeh99"
      },
      "source": [
        "df['Code'].replace(\"\", float(\"NaN\"), inplace=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnqkdzWbg2H-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "1ed408a5-4439-4dfd-b1df-b8a899c5f826"
      },
      "source": [
        "df[df.isna().any(axis=1)]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Description</th>\n",
              "      <th>Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Description, Code]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxQVB1MEgZZQ"
      },
      "source": [
        "df.dropna(subset = [\"Code\"], inplace=True)\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWWDmOTDQYhf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b015b9ba-9235-43b1-a534-76a6554a20ed"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1018 entries, 0 to 1017\n",
            "Data columns (total 2 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Description  1018 non-null   object\n",
            " 1   Code         1018 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 23.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OyrtWssHCKh"
      },
      "source": [
        "\r\n",
        "# Dividing the data into train and validation dataset\r\n",
        "\r\n",
        "train_df = df.sample(frac = 0.80) \r\n",
        "  \r\n",
        "# Creating dataframe with rest of the 20% values \r\n",
        "valid_df = df.drop(train_df.index)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts-QRvzPdLKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52911321-1d69-43c8-c182-f5835ad7f2b8"
      },
      "source": [
        "print(f'train df {train_df}')\r\n",
        "print(f'Valid df {valid_df}')\r\n",
        "\r\n",
        "train_df.to_csv('train.csv', index=False)\r\n",
        "valid_df.to_csv('valid.csv', index=False)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train df                                            Description                                               Code\n",
            "387  write a program to find the frequency of words...  \\ntest_str = 'times of india times new india e...\n",
            "45          Write a lambda function to add two numbers         \\n\\ndef add(a, b):\\n    return a + b\\n\\n\\n\n",
            "206  Write a function to return the sum of the root...  def sum_of_roots(a: float, c: float):\\n    if ...\n",
            "143  write a python program to replace blank space ...  def f12(x):\\n    yield x + 1\\n    print(\"test\"...\n",
            "541  Write a python function that Capitalize the Fi...  \\n\\ndef capitalize(fname):\\n    with open(fnam...\n",
            "..                                                 ...                                                ...\n",
            "757                                     usage of break  for i in range(5):\\n    if i == 1:\\n        br...\n",
            "357  Write a function that returns derivative deriv...  def derivative_relu(x: float) -> float:\\n    x...\n",
            "800  write the python program to generate a random ...  \\n\\ndef read_csv(input_file):\\n    with open(i...\n",
            "438  47 write a program to check if the number is a...  \\nnum = int(input(\"Enter a number: \"))\\n\\nif n...\n",
            "654                            47 first class function  \\n\\ndef call_func(x, func):\\n    return fn(x)\\...\n",
            "\n",
            "[814 rows x 2 columns]\n",
            "Valid df                                             Description                                               Code\n",
            "3     write a program to find and print the smallest...  \\nnum1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 <=...\n",
            "5     Write a program to check whether a number is p...  \\nnum = 337\\nif num > 1:\\n    for i in range(2...\n",
            "11    Write a program to filter the numbers in a lis...  \\nmy_list = [11, 45, 74, 89, 132, 239, 721, 21...\n",
            "16        Write a program to print the length of a list  \\nmy_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\\nprint...\n",
            "19    Write a python function to remove all the odd ...  \\n\\ndef remove_odd(my_list):\\n    result = lis...\n",
            "...                                                 ...                                                ...\n",
            "1005  48 write a python program to merge a list of d...  result = {}\\nfor d in L:\\n    result.update(d)...\n",
            "1010  53  write a python program to permutations of ...  s = \"GEEK\"\\na = string.ascii_letters\\np = perm...\n",
            "1013  56 Write a Python function to find three numbe...  def three_Sum(num):\\n    if len(num) < 3:\\n   ...\n",
            "1014  57 Write a Python function to find the single ...  def single_number(arr):\\n    result = 0\\n    f...\n",
            "1016  59 Write a function program to add the digits ...  def add_digits(num):\\n    return (num - 1) % 9...\n",
            "\n",
            "[204 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ze0bFFIWyQP"
      },
      "source": [
        "# import io\r\n",
        "# from io import BytesIO\r\n",
        "# from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP, tok_name\r\n",
        "\r\n",
        "# def tokenize_code(text):\r\n",
        "#     result = []\r\n",
        "#     for tok in tokenize(io.BytesIO(text.encode('utf-8')).readline):\r\n",
        "#         if tok_name[tok.exact_type] == 'NAME':\r\n",
        "#             result.append(tok.string)\r\n",
        "#         else:\r\n",
        "#             result.append(tok_name[tok.exact_type])\r\n",
        "#     return result"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQE5bFuMyIwe"
      },
      "source": [
        "# tokenize_code(df['Code'][1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIbH2-jX8P1l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "7e8cb678-823b-4539-b6d4-dc684101ed6a"
      },
      "source": [
        "'''\r\n",
        "ENDMARKER = 0\r\n",
        "NAME = 1\r\n",
        "NUMBER = 2\r\n",
        "STRING = 3\r\n",
        "NEWLINE = 4\r\n",
        "INDENT = 5\r\n",
        "DEDENT = 6\r\n",
        "LPAR = 7\r\n",
        "RPAR = 8\r\n",
        "LSQB = 9\r\n",
        "RSQB = 10\r\n",
        "COLON = 11\r\n",
        "COMMA = 12\r\n",
        "SEMI = 13\r\n",
        "PLUS = 14\r\n",
        "MINUS = 15\r\n",
        "STAR = 16\r\n",
        "SLASH = 17\r\n",
        "VBAR = 18\r\n",
        "AMPER = 19\r\n",
        "LESS = 20\r\n",
        "GREATER = 21\r\n",
        "EQUAL = 22\r\n",
        "DOT = 23\r\n",
        "PERCENT = 24\r\n",
        "LBRACE = 25\r\n",
        "RBRACE = 26\r\n",
        "EQEQUAL = 27\r\n",
        "NOTEQUAL = 28\r\n",
        "LESSEQUAL = 29\r\n",
        "GREATEREQUAL = 30\r\n",
        "TILDE = 31\r\n",
        "CIRCUMFLEX = 32\r\n",
        "LEFTSHIFT = 33\r\n",
        "RIGHTSHIFT = 34\r\n",
        "DOUBLESTAR = 35\r\n",
        "PLUSEQUAL = 36\r\n",
        "MINEQUAL = 37\r\n",
        "STAREQUAL = 38\r\n",
        "SLASHEQUAL = 39\r\n",
        "PERCENTEQUAL = 40\r\n",
        "AMPEREQUAL = 41\r\n",
        "VBAREQUAL = 42\r\n",
        "CIRCUMFLEXEQUAL = 43\r\n",
        "LEFTSHIFTEQUAL = 44\r\n",
        "RIGHTSHIFTEQUAL = 45\r\n",
        "DOUBLESTAREQUAL = 46\r\n",
        "DOUBLESLASH = 47\r\n",
        "DOUBLESLASHEQUAL = 48\r\n",
        "AT = 49\r\n",
        "ATEQUAL = 50\r\n",
        "RARROW = 51\r\n",
        "ELLIPSIS = 52\r\n",
        "COLONEQUAL = 53\r\n",
        "OP = 54\r\n",
        "AWAIT = 55\r\n",
        "ASYNC = 56\r\n",
        "TYPE_IGNORE = 57\r\n",
        "TYPE_COMMENT = 58\r\n",
        "# These aren't used by the C tokenizer but are needed for tokenize.py\r\n",
        "ERRORTOKEN = 59\r\n",
        "COMMENT = 60\r\n",
        "NL = 61\r\n",
        "ENCODING = 62\r\n",
        "N_TOKENS = 63\r\n",
        "# Special definitions for cooperation with parser\r\n",
        "NT_OFFSET = 256\r\n",
        "'''"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nENDMARKER = 0\\nNAME = 1\\nNUMBER = 2\\nSTRING = 3\\nNEWLINE = 4\\nINDENT = 5\\nDEDENT = 6\\nLPAR = 7\\nRPAR = 8\\nLSQB = 9\\nRSQB = 10\\nCOLON = 11\\nCOMMA = 12\\nSEMI = 13\\nPLUS = 14\\nMINUS = 15\\nSTAR = 16\\nSLASH = 17\\nVBAR = 18\\nAMPER = 19\\nLESS = 20\\nGREATER = 21\\nEQUAL = 22\\nDOT = 23\\nPERCENT = 24\\nLBRACE = 25\\nRBRACE = 26\\nEQEQUAL = 27\\nNOTEQUAL = 28\\nLESSEQUAL = 29\\nGREATEREQUAL = 30\\nTILDE = 31\\nCIRCUMFLEX = 32\\nLEFTSHIFT = 33\\nRIGHTSHIFT = 34\\nDOUBLESTAR = 35\\nPLUSEQUAL = 36\\nMINEQUAL = 37\\nSTAREQUAL = 38\\nSLASHEQUAL = 39\\nPERCENTEQUAL = 40\\nAMPEREQUAL = 41\\nVBAREQUAL = 42\\nCIRCUMFLEXEQUAL = 43\\nLEFTSHIFTEQUAL = 44\\nRIGHTSHIFTEQUAL = 45\\nDOUBLESTAREQUAL = 46\\nDOUBLESLASH = 47\\nDOUBLESLASHEQUAL = 48\\nAT = 49\\nATEQUAL = 50\\nRARROW = 51\\nELLIPSIS = 52\\nCOLONEQUAL = 53\\nOP = 54\\nAWAIT = 55\\nASYNC = 56\\nTYPE_IGNORE = 57\\nTYPE_COMMENT = 58\\n# These aren't used by the C tokenizer but are needed for tokenize.py\\nERRORTOKEN = 59\\nCOMMENT = 60\\nNL = 61\\nENCODING = 62\\nN_TOKENS = 63\\n# Special definitions for cooperation with parser\\nNT_OFFSET = 256\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioQrKfTs8Ti4"
      },
      "source": [
        "import io\r\n",
        "from io import BytesIO\r\n",
        "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP, tok_name\r\n",
        "#https://docs.python.org/3/library/tokenize.html\r\n",
        "def tokenize_python(code_snippet):\r\n",
        "    tokens = tokenize(io.BytesIO(code_snippet.encode('utf-8')).readline)\r\n",
        "    parsed = []\r\n",
        "    for token in tokens:\r\n",
        "        if token.type not in [0,59,60,61,62,63,256]:\r\n",
        "            parsed.append(token.string)\r\n",
        "    return parsed"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8k7Lf6YDHo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "390f3b61-2045-4e4e-eafa-222e57d467d3"
      },
      "source": [
        "tokenize_python(df['Code'][1])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['utf-8',\n",
              " '\\n',\n",
              " '\\n',\n",
              " 'def',\n",
              " 'add_two_numbers',\n",
              " '(',\n",
              " 'num1',\n",
              " ',',\n",
              " 'num2',\n",
              " ')',\n",
              " ':',\n",
              " '\\n',\n",
              " '    ',\n",
              " 'sum',\n",
              " '=',\n",
              " 'num1',\n",
              " '+',\n",
              " 'num2',\n",
              " '\\n',\n",
              " 'return',\n",
              " 'sum',\n",
              " '\\n',\n",
              " '\\n',\n",
              " '\\n',\n",
              " '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oC-bUR5h9-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8701f2c-64d1-4fcf-d219-a377de37b4fa"
      },
      "source": [
        "import spacy\r\n",
        "spacy_en = spacy.load('en')\r\n",
        "\r\n",
        "def tokenize_en(text):\r\n",
        "    \"\"\"\r\n",
        "    Tokenizes English text from a string into a list of strings\r\n",
        "    \"\"\"\r\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\r\n",
        "\r\n",
        "SRC = Field(tokenize= tokenize_en, \r\n",
        "            init_token='<sos>', \r\n",
        "            eos_token='<eos>', \r\n",
        "            lower=True,\r\n",
        "            batch_first=True)\r\n",
        "\r\n",
        "TRG = Field(tokenize = tokenize_python, \r\n",
        "            init_token='<sos>', \r\n",
        "            eos_token='<eos>', \r\n",
        "            lower=False,\r\n",
        "            batch_first=True)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "schGM4YMkRXl"
      },
      "source": [
        "fields = [('Description', SRC),('Code',TRG)]\r\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K63Pacf9dP1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "780eedb2-5181-48dc-a9fa-853bc470bffe"
      },
      "source": [
        "# Using tabular dataset to process the text\r\n",
        "\r\n",
        "train_data, test_data = TabularDataset.splits(\r\n",
        "                                path = '',   \r\n",
        "                                train = './train.csv',\r\n",
        "                                test = './valid.csv',\r\n",
        "                                format = 'csv',\r\n",
        "                                fields = fields)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSyYnqeOhiZN"
      },
      "source": [
        "BATCH_SIZE = 16\r\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frL3JFUIkoeW"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 3,max_size= 10000)\r\n",
        "TRG.build_vocab(test_data, min_freq = 3,max_size= 10000)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reHXLuvMkvTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "983e4a92-f33a-4d28-d546-71c7e7f2b2fc"
      },
      "source": [
        "train_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, test_data), \r\n",
        "    batch_size = BATCH_SIZE,\r\n",
        "    sort_key = lambda x: len(x.Description),\r\n",
        "    device = device)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG_rKtAbmKPg"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 encoder, \r\n",
        "                 decoder, \r\n",
        "                 src_pad_idx, \r\n",
        "                 trg_pad_idx, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.src_pad_idx = src_pad_idx\r\n",
        "        self.trg_pad_idx = trg_pad_idx\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "    def make_src_mask(self, src):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        \r\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "\r\n",
        "        return src_mask\r\n",
        "    \r\n",
        "    def make_trg_mask(self, trg):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        \r\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "        \r\n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\r\n",
        "        \r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        \r\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\r\n",
        "        \r\n",
        "        #trg_sub_mask = [trg len, trg len]\r\n",
        "            \r\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\r\n",
        "        \r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        \r\n",
        "        return trg_mask\r\n",
        "\r\n",
        "    def forward(self, src, trg):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "                \r\n",
        "        src_mask = self.make_src_mask(src)\r\n",
        "        trg_mask = self.make_trg_mask(trg)\r\n",
        "        \r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "\r\n",
        "\r\n",
        "        enc_src = self.encoder(src, src_mask)\r\n",
        "        \r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "                \r\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]        \r\n",
        "        return output, attention"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fllX7D6mqDqM"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 input_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim,\r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 2000):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\r\n",
        "        \r\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \r\n",
        "                                                  n_heads, \r\n",
        "                                                  pf_dim,\r\n",
        "                                                  dropout, \r\n",
        "                                                  device) \r\n",
        "                                     for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        \r\n",
        "        batch_size = src.shape[0]\r\n",
        "        src_len = src.shape[1]\r\n",
        "        \r\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        \r\n",
        "        #pos = [batch size, src len]\r\n",
        "\r\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\r\n",
        "\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        for layer in self.layers:\r\n",
        "            src = layer(src, src_mask)\r\n",
        "            \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        " \r\n",
        "            \r\n",
        "        return src"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wgnIzCiqEp2"
      },
      "source": [
        "class EncoderLayer(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 hid_dim, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim,  \r\n",
        "                 dropout, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \r\n",
        "                                                                     pf_dim, \r\n",
        "                                                                     dropout)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        #src_mask = [batch size, 1, 1, src len] \r\n",
        "                \r\n",
        "        #self attention\r\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        #positionwise feedforward\r\n",
        "        _src = self.positionwise_feedforward(src)\r\n",
        "        \r\n",
        "        #dropout, residual and layer norm\r\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        return src"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzFPJ5YpqNOT"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\r\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        assert hid_dim % n_heads == 0\r\n",
        "        \r\n",
        "        self.hid_dim = hid_dim\r\n",
        "        self.n_heads = n_heads\r\n",
        "        self.head_dim = hid_dim // n_heads\r\n",
        "        \r\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\r\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\r\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, query, key, value, mask = None):\r\n",
        "        \r\n",
        "        batch_size = query.shape[0]\r\n",
        "        \r\n",
        "        #query = [batch size, query len, hid dim]\r\n",
        "        #key = [batch size, key len, hid dim]\r\n",
        "        #value = [batch size, value len, hid dim]\r\n",
        "                \r\n",
        "        Q = self.fc_q(query)\r\n",
        "        K = self.fc_k(key)\r\n",
        "        V = self.fc_v(value)\r\n",
        "        \r\n",
        "        #Q = [batch size, query len, hid dim]\r\n",
        "        #K = [batch size, key len, hid dim]\r\n",
        "        #V = [batch size, value len, hid dim]\r\n",
        "                \r\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        \r\n",
        "        #Q = [batch size, n heads, query len, head dim]\r\n",
        "        #K = [batch size, n heads, key len, head dim]\r\n",
        "        #V = [batch size, n heads, value len, head dim]\r\n",
        "                \r\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\r\n",
        "        \r\n",
        "        #energy = [batch size, n heads, query len, key len]\r\n",
        "        \r\n",
        "        if mask is not None:\r\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\r\n",
        "        \r\n",
        "        attention = torch.softmax(energy, dim = -1)\r\n",
        "                \r\n",
        "        #attention = [batch size, n heads, query len, key len]\r\n",
        "                \r\n",
        "        x = torch.matmul(self.dropout(attention), V)\r\n",
        "        \r\n",
        "        #x = [batch size, n heads, query len, head dim]\r\n",
        "        \r\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\r\n",
        "        \r\n",
        "        #x = [batch size, query len, n heads, head dim]\r\n",
        "        \r\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\r\n",
        "        \r\n",
        "        #x = [batch size, query len, hid dim]\r\n",
        "        \r\n",
        "        x = self.fc_o(x)\r\n",
        "        \r\n",
        "        #x = [batch size, query len, hid dim]\r\n",
        "        \r\n",
        "        return x, attention"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqcTMtF8qQQl"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\r\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\r\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, hid dim]\r\n",
        "        \r\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, pf dim]\r\n",
        "        \r\n",
        "        x = self.fc_2(x)\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, hid dim]\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOAuxKl2qVoy"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 output_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim, \r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 2000):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\r\n",
        "        \r\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \r\n",
        "                                                  n_heads, \r\n",
        "                                                  pf_dim, \r\n",
        "                                                  dropout, \r\n",
        "                                                  device)\r\n",
        "                                     for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "                \r\n",
        "        batch_size = trg.shape[0]\r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        \r\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "                            \r\n",
        "        #pos = [batch size, trg len]\r\n",
        "            \r\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\r\n",
        "                \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        for layer in self.layers:\r\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        output = self.fc_out(trg)\r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "            \r\n",
        "        return output, attention"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5BAIFqIqcGp"
      },
      "source": [
        "class DecoderLayer(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 hid_dim, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim, \r\n",
        "                 dropout, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \r\n",
        "                                                                     pf_dim, \r\n",
        "                                                                     dropout)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        \r\n",
        "        #self attention\r\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "            \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "            \r\n",
        "        #encoder attention\r\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\r\n",
        "        # query, key, value\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "                    \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        #positionwise feedforward\r\n",
        "        _trg = self.positionwise_feedforward(trg)\r\n",
        "        \r\n",
        "        #dropout, residual and layer norm\r\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        return trg, attention"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfSlm0Z0mUK1"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\r\n",
        "OUTPUT_DIM = len(TRG.vocab)\r\n",
        "HID_DIM = 256\r\n",
        "ENC_LAYERS = 2\r\n",
        "DEC_LAYERS = 2\r\n",
        "ENC_HEADS = 8\r\n",
        "DEC_HEADS = 8\r\n",
        "ENC_PF_DIM = 512\r\n",
        "DEC_PF_DIM = 512\r\n",
        "ENC_DROPOUT = 0.1\r\n",
        "DEC_DROPOUT = 0.1\r\n",
        "\r\n",
        "enc = Encoder(INPUT_DIM, \r\n",
        "              HID_DIM, \r\n",
        "              ENC_LAYERS, \r\n",
        "              ENC_HEADS, \r\n",
        "              ENC_PF_DIM, \r\n",
        "              ENC_DROPOUT, \r\n",
        "              device)\r\n",
        "\r\n",
        "\r\n",
        "dec = Decoder(OUTPUT_DIM, \r\n",
        "              HID_DIM, \r\n",
        "              DEC_LAYERS, \r\n",
        "              DEC_HEADS, \r\n",
        "              DEC_PF_DIM, \r\n",
        "              DEC_DROPOUT,\r\n",
        "              device)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2DLlhJSmXpU"
      },
      "source": [
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\r\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n",
        "\r\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey774CaLmgsI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd9d0d9-5850-4682-f90c-d9d6c2acb210"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 3,902,743 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2wspF4Cmnpz"
      },
      "source": [
        "def initialize_weights(m):\r\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\r\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfvJg4d9mr7c"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\r\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oKo8njwmt_g"
      },
      "source": [
        "model.apply(initialize_weights);\r\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i22PXjHmysa"
      },
      "source": [
        "\r\n",
        "LEARNING_RATE = 0.0005\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A-uyR-vm2y1"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "        \r\n",
        "        src = batch.Description\r\n",
        "        trg = batch.Code\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        output, _ = model(src, trg[:,:-1])\r\n",
        "\r\n",
        "                \r\n",
        "        #output = [batch size, trg len - 1, output dim]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "            \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "\r\n",
        "            \r\n",
        "        output = output.contiguous().view(-1, output_dim)\r\n",
        "        trg = trg[:,1:].contiguous().view(-1)\r\n",
        "                \r\n",
        "        #output = [batch size * trg len - 1, output dim]\r\n",
        "        #trg = [batch size * trg len - 1]\r\n",
        "            \r\n",
        "        loss = criterion(output, trg)\r\n",
        "        #loss = maskNLLLoss(output, trg,model.trg_pad_idx)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRMf8Glpm5zj"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "\r\n",
        "            src = batch.Description\r\n",
        "            trg = batch.Code\r\n",
        "\r\n",
        "            output, _ = model(src, trg[:,:-1])\r\n",
        "            \r\n",
        "            #output = [batch size, trg len - 1, output dim]\r\n",
        "            #trg = [batch size, trg len]\r\n",
        "            \r\n",
        "            output_dim = output.shape[-1]\r\n",
        "           \r\n",
        "            \r\n",
        "            output = output.contiguous().view(-1, output_dim)\r\n",
        "            trg = trg[:,1:].contiguous().view(-1)\r\n",
        "            \r\n",
        "            #output = [batch size * trg len - 1, output dim]\r\n",
        "            #trg = [batch size * trg len - 1]\r\n",
        "            \r\n",
        "            \r\n",
        "            loss = criterion(output, trg)\r\n",
        "            #loss = maskNLLLoss(output, trg,model.trg_pad_idx)\r\n",
        "\r\n",
        "            #loss,_ = maskNLLLoss(output, trg, mask)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H25R2prm9Dc"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAWxVNo_m__A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c5f0cd-3d99-4b15-975e-b7e45d389ace"
      },
      "source": [
        "import time\r\n",
        "N_EPOCHS = 20\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 2s\n",
            "\tTrain Loss: 3.157 | Train PPL:  23.493\n",
            "\t Val. Loss: 2.758 |  Val. PPL:  15.772\n",
            "Epoch: 02 | Time: 0m 1s\n",
            "\tTrain Loss: 2.246 | Train PPL:   9.448\n",
            "\t Val. Loss: 2.376 |  Val. PPL:  10.757\n",
            "Epoch: 03 | Time: 0m 1s\n",
            "\tTrain Loss: 1.967 | Train PPL:   7.151\n",
            "\t Val. Loss: 2.194 |  Val. PPL:   8.973\n",
            "Epoch: 04 | Time: 0m 1s\n",
            "\tTrain Loss: 1.777 | Train PPL:   5.914\n",
            "\t Val. Loss: 2.097 |  Val. PPL:   8.140\n",
            "Epoch: 05 | Time: 0m 1s\n",
            "\tTrain Loss: 1.647 | Train PPL:   5.189\n",
            "\t Val. Loss: 2.013 |  Val. PPL:   7.486\n",
            "Epoch: 06 | Time: 0m 1s\n",
            "\tTrain Loss: 1.535 | Train PPL:   4.641\n",
            "\t Val. Loss: 1.962 |  Val. PPL:   7.115\n",
            "Epoch: 07 | Time: 0m 1s\n",
            "\tTrain Loss: 1.446 | Train PPL:   4.246\n",
            "\t Val. Loss: 1.937 |  Val. PPL:   6.936\n",
            "Epoch: 08 | Time: 0m 1s\n",
            "\tTrain Loss: 1.373 | Train PPL:   3.946\n",
            "\t Val. Loss: 1.891 |  Val. PPL:   6.624\n",
            "Epoch: 09 | Time: 0m 1s\n",
            "\tTrain Loss: 1.306 | Train PPL:   3.690\n",
            "\t Val. Loss: 1.920 |  Val. PPL:   6.819\n",
            "Epoch: 10 | Time: 0m 1s\n",
            "\tTrain Loss: 1.246 | Train PPL:   3.475\n",
            "\t Val. Loss: 1.883 |  Val. PPL:   6.573\n",
            "Epoch: 11 | Time: 0m 1s\n",
            "\tTrain Loss: 1.187 | Train PPL:   3.276\n",
            "\t Val. Loss: 1.843 |  Val. PPL:   6.317\n",
            "Epoch: 12 | Time: 0m 1s\n",
            "\tTrain Loss: 1.136 | Train PPL:   3.115\n",
            "\t Val. Loss: 1.860 |  Val. PPL:   6.426\n",
            "Epoch: 13 | Time: 0m 1s\n",
            "\tTrain Loss: 1.090 | Train PPL:   2.974\n",
            "\t Val. Loss: 1.838 |  Val. PPL:   6.286\n",
            "Epoch: 14 | Time: 0m 1s\n",
            "\tTrain Loss: 1.060 | Train PPL:   2.886\n",
            "\t Val. Loss: 1.845 |  Val. PPL:   6.328\n",
            "Epoch: 15 | Time: 0m 1s\n",
            "\tTrain Loss: 1.016 | Train PPL:   2.763\n",
            "\t Val. Loss: 1.873 |  Val. PPL:   6.505\n",
            "Epoch: 16 | Time: 0m 1s\n",
            "\tTrain Loss: 0.959 | Train PPL:   2.610\n",
            "\t Val. Loss: 1.864 |  Val. PPL:   6.451\n",
            "Epoch: 17 | Time: 0m 1s\n",
            "\tTrain Loss: 0.935 | Train PPL:   2.547\n",
            "\t Val. Loss: 1.872 |  Val. PPL:   6.499\n",
            "Epoch: 18 | Time: 0m 1s\n",
            "\tTrain Loss: 0.914 | Train PPL:   2.494\n",
            "\t Val. Loss: 1.859 |  Val. PPL:   6.420\n",
            "Epoch: 19 | Time: 0m 1s\n",
            "\tTrain Loss: 0.884 | Train PPL:   2.421\n",
            "\t Val. Loss: 1.891 |  Val. PPL:   6.626\n",
            "Epoch: 20 | Time: 0m 1s\n",
            "\tTrain Loss: 0.849 | Train PPL:   2.338\n",
            "\t Val. Loss: 1.906 |  Val. PPL:   6.729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTTR5lEOnDoq"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 100):\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "        \r\n",
        "    if isinstance(sentence, str):\r\n",
        "        nlp = spacy.load('en')\r\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\r\n",
        "    else:\r\n",
        "        tokens = [token.lower() for token in sentence]\r\n",
        "\r\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\r\n",
        "        \r\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\r\n",
        "\r\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\r\n",
        "    src_mask = model.make_src_mask(src_tensor)\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        enc_src = model.encoder(src_tensor,src_mask)\r\n",
        "\r\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\r\n",
        "\r\n",
        "    for i in range(max_len):\r\n",
        "\r\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\r\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\r\n",
        "        with torch.no_grad():\r\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        pred_token = output.argmax(2)[:,-1].item()\r\n",
        "        \r\n",
        "        trg_indexes.append(pred_token)\r\n",
        "\r\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\r\n",
        "            break\r\n",
        "    \r\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\r\n",
        "    \r\n",
        "    return trg_tokens[1:]#, attention"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mysK3vRHKLQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60560af1-179f-49fb-9fe3-ee266eb1c719"
      },
      "source": [
        "sentence = \"write a program to find and print the largest among three numbers\"\r\n",
        "code = translate_sentence(sentence, SRC, TRG, model, device)\r\n",
        "print(f'predicted trg = {code}')\r\n",
        "#print(f'predicted trg = {\" \".join(code)}')\r\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['utf-8', '\\n', 'a', '=', 'float', '(', 'input', '(', '<unk>', ')', ')', '\\n', 'b', '=', 'float', '(', 'input', '(', '<unk>', ')', ')', '\\n', 'b', '=', 'float', '(', '<unk>', ')', '\\n', 'c', '=', 'float', '(', 'input', '(', '<unk>', ')', ')', '\\n', '\\n', 'if', '(', 'a', '>', 'b', 'and', 'c', '>', 'c', ')', ':', '\\n', '    ', 'print', '(', 'a', 'and', 'c', ')', '\\n', '', 'else', ':', '\\n', '    ', 'print', '(', '<unk>', ',', 'c', ')', '', '', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i7MaYQIKqdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c975b08-3a40-4e5b-dc79-81b19aa6b987"
      },
      "source": [
        "print(\"\".join(code))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "utf-8\n",
            "a=float(input(<unk>))\n",
            "b=float(input(<unk>))\n",
            "b=float(<unk>)\n",
            "c=float(input(<unk>))\n",
            "\n",
            "if(a>bandc>c):\n",
            "    print(aandc)\n",
            "else:\n",
            "    print(<unk>,c)<eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWjPAUJ6QA9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a813dcd2-cf44-4fe0-8015-0baa676dbef4"
      },
      "source": [
        "sentence = \"write a program to add two numbers\"\r\n",
        "code = translate_sentence(sentence, SRC, TRG, model, device)\r\n",
        "print(f'predicted trg = {code}')\r\n",
        "print(f'predicted trg = {\" \".join(code)}')\r\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['utf-8', 'num1', '=', '<unk>', '\\n', 'num2', '=', '<unk>', '\\n', 'sum', '=', 'num1', '+', 'num2', '\\n', 'print', '(', 'num1', ',', 'num2', ')', '', '<eos>']\n",
            "predicted trg = utf-8 num1 = <unk> \n",
            " num2 = <unk> \n",
            " sum = num1 + num2 \n",
            " print ( num1 , num2 )  <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di3dwK8RQS0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09b63cd5-df3c-4ffa-8f34-42157def2ede"
      },
      "source": [
        "sentence = \"write a program to multiply two numbers\"\r\n",
        "code = translate_sentence(sentence, SRC, TRG, model, device)\r\n",
        "print(f'predicted trg = {code}\\n')\r\n",
        "print(\" \".join(code))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['utf-8', '\\n', '\\n', 'def', '<unk>', '(', 'a', ')', ':', '\\n', '    ', 'sum1', '=', '<unk>', '\\n', 'for', 'i', 'in', 'range', '(', '1', ',', '2', ')', ':', '\\n', '        ', 'if', '(', 'a', '%', 'i', '==', '0', ')', ':', '\\n', '            ', '<unk>', '+=', '1', '\\n', '', '', 'return', 'False', '', '<eos>']\n",
            "\n",
            "utf-8 \n",
            " \n",
            " def <unk> ( a ) : \n",
            "      sum1 = <unk> \n",
            " for i in range ( 1 , 2 ) : \n",
            "          if ( a % i == 0 ) : \n",
            "              <unk> += 1 \n",
            "   return False  <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "638D43sYQkUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa9efac-73af-4f2f-dbe0-fc35b4e09117"
      },
      "source": [
        "sentence = \"write a program to print factorial of a number\"\r\n",
        "code = translate_sentence(sentence, SRC, TRG, model, device)\r\n",
        "print(f'predicted trg = {code}\\n')\r\n",
        "print(\" \".join(code))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['utf-8', '\\n', 'num', '=', 'int', '(', 'input', '(', '<unk>', ')', ')', '\\n', 'for', 'i', 'in', 'range', '(', '1', ',', '<unk>', ')', ':', '\\n', '    ', 'print', '(', 'num', ',', '<unk>', ')', '', '', '<eos>']\n",
            "\n",
            "utf-8 \n",
            " num = int ( input ( <unk> ) ) \n",
            " for i in range ( 1 , <unk> ) : \n",
            "      print ( num , <unk> )   <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfyKB9JDQw0y"
      },
      "source": [
        ""
      ],
      "execution_count": 52,
      "outputs": []
    }
  ]
}