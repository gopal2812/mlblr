{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_TextClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopal2812/mlblr/blob/master/Transformer_TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p_N-dKf1OT1"
      },
      "source": [
        "## Using Transformer for Classification\n",
        "- Only encoder part of Transformer model is used for classification\n",
        "- No residual connection, no layer normalization\n",
        "- No need for masking\n",
        "- Multihead attention and positionwise feedforward network to extract features\n",
        "- Then, linear layer to get logits\n",
        "\n",
        "## Implementation Details\n",
        "- N = 1, i.e. one layer is used\n",
        "- d_model = 256\n",
        "- maximum sequence lenght = 60\n",
        "- Adam optimizer is used with initial learning rate 0.0003. Reducing by half every 1/3 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S3ExEyJ1rgM"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4tLQ73T1uaY"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "from torchtext.legacy import data\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import copy\n",
        "import math\n",
        "import sys\n",
        "from copy import deepcopy"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV7YzmhQ31C0"
      },
      "source": [
        "## Dataset Class\n",
        "\n",
        "Based on [AG News Dataset.](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)\n",
        "\n",
        "Example:\n",
        "```\n",
        "__label__2 , brown in line after livingston sack preston , livingston have sacked allan preston as manager . the former hearts and st johnstone defender and his assistant , alan kernaghan , were dismissed after a run of seven defeats left the club \n",
        "__label__4 , lockheed profit jumps on it , jet demand , &lt p&gt \\&lt /p&gt &lt p&gt new york ( reuters ) - no . 1 u . s . defense contractor lockheed\\martin corp . &lt lmt . n&gt reported a 41 percent rise in quarterly\\profit on tuesday , beating wall street forecasts , as demand\\soared for its combat aircraft and information technology\\services . &lt /p&gt \n",
        "__label__3 , japan narrowly escapes recession , new figures show japan ' s economy is barely staying out of recession with annual growth of just 0 . 2 in the third quarter . \n",
        "__label__4 , tiny telescope detects a giant planet , a tiny telescope has spotted a giant planet circling a faraway star , using a technique that could open a new phase of planetary discovery , scientists say . \n",
        "__label__3 , news corp . net profit soars 27 , news corp . saw healthy gains in profit and revenue in the fiscal first quarter - helped by growth in advertising at the fox news channel and the fox broadcast network , as \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tEoGEut0BOy"
      },
      "source": [
        "class Dataset(object):\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.train_iterator = None\n",
        "        self.test_iterator = None\n",
        "        self.val_iterator = None\n",
        "        self.vocab = []\n",
        "        self.word_embeddings = {}\n",
        "    \n",
        "    def parse_label(self, label):\n",
        "        '''\n",
        "        Get the actual labels from label string\n",
        "        Input:\n",
        "            label (string) : labels of the form '__label__2'\n",
        "        Returns:\n",
        "            label (int) : integer value corresponding to label string\n",
        "        '''\n",
        "        return int(label.strip()[-1])\n",
        "\n",
        "    def get_pandas_df(self, filename):\n",
        "        '''\n",
        "        Load the data into Pandas.DataFrame object\n",
        "        This will be used to convert data to torchtext object\n",
        "        '''\n",
        "        with open(filename, 'r') as datafile:     \n",
        "            data = [line.strip().split(',', maxsplit=1) for line in datafile]\n",
        "            data_text = list(map(lambda x: x[1], data))\n",
        "            data_label = list(map(lambda x: self.parse_label(x[0]), data))\n",
        "\n",
        "        full_df = pd.DataFrame({\"text\":data_text, \"label\":data_label})\n",
        "        return full_df\n",
        "    \n",
        "    def load_data(self, train_file, test_file=None, val_file=None):\n",
        "        '''\n",
        "        Loads the data from files\n",
        "        Sets up iterators for training, validation and test data\n",
        "        Also create vocabulary and word embeddings based on the data\n",
        "        \n",
        "        Inputs:\n",
        "            train_file (String): path to training file\n",
        "            test_file (String): path to test file\n",
        "            val_file (String): path to validation file\n",
        "        '''\n",
        "\n",
        "        NLP = spacy.load('en')\n",
        "        tokenizer = lambda sent: [x.text for x in NLP.tokenizer(sent) if x.text != \" \"]\n",
        "        \n",
        "        # Creating Field for data\n",
        "        TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=self.config.max_sen_len)\n",
        "        LABEL = data.Field(sequential=False, use_vocab=False)\n",
        "        datafields = [(\"text\",TEXT),(\"label\",LABEL)]\n",
        "        \n",
        "        # Load data from pd.DataFrame into torchtext.data.Dataset\n",
        "        train_df = self.get_pandas_df(train_file)\n",
        "        train_examples = [data.Example.fromlist(i, datafields) for i in train_df.values.tolist()]\n",
        "        train_data = data.Dataset(train_examples, datafields)\n",
        "        \n",
        "        test_df = self.get_pandas_df(test_file)\n",
        "        test_examples = [data.Example.fromlist(i, datafields) for i in test_df.values.tolist()]\n",
        "        test_data = data.Dataset(test_examples, datafields)\n",
        "        \n",
        "        # If validation file exists, load it. Otherwise get validation data from training data\n",
        "        if val_file:\n",
        "            val_df = self.get_pandas_df(val_file)\n",
        "            val_examples = [data.Example.fromlist(i, datafields) for i in val_df.values.tolist()]\n",
        "            val_data = data.Dataset(val_examples, datafields)\n",
        "        else:\n",
        "            train_data, val_data = train_data.split(split_ratio=0.8)\n",
        "        \n",
        "        TEXT.build_vocab(train_data)\n",
        "        self.vocab = TEXT.vocab\n",
        "        \n",
        "        self.train_iterator = data.BucketIterator(\n",
        "            (train_data),\n",
        "            batch_size=self.config.batch_size,\n",
        "            sort_key=lambda x: len(x.text),\n",
        "            repeat=False,\n",
        "            shuffle=True)\n",
        "        \n",
        "        self.val_iterator, self.test_iterator = data.BucketIterator.splits(\n",
        "            (val_data, test_data),\n",
        "            batch_size=self.config.batch_size,\n",
        "            sort_key=lambda x: len(x.text),\n",
        "            repeat=False,\n",
        "            shuffle=False)\n",
        "        \n",
        "        print (\"Loaded {} training examples\".format(len(train_data)))\n",
        "        print (\"Loaded {} test examples\".format(len(test_data)))\n",
        "        print (\"Loaded {} validation examples\".format(len(val_data)))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBM7Plop4WAI"
      },
      "source": [
        "## Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46pH5XAW6I3B"
      },
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UU-DJTy6LmS"
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    '''\n",
        "    Usual Embedding layer with weights multiplied by sqrt(d_model)\n",
        "    '''\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)\n",
        "    \n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(torch.as_tensor(position.numpy() * div_term.unsqueeze(0).numpy()))\n",
        "        pe[:, 1::2] = torch.cos(torch.as_tensor(position.numpy() * div_term.unsqueeze(0).numpy()))#torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ytLZ-gT6GdI"
      },
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Implementation of Scaled dot product attention\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Multi-head attention\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM-eMPzG5mPk"
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layer normalization module.\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "    \n",
        "class SublayerOutput(nn.Module):\n",
        "    '''\n",
        "    A residual connection followed by a layer norm.\n",
        "    '''\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerOutput, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVd6plnT6TXH"
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Positionwise feed-forward network.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"Implements FFN equation.\"\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLuecbPN5YQm"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Transformer Encoder\n",
        "    \n",
        "    It is a stack of N layers.\n",
        "    '''\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    '''\n",
        "    An encoder layer\n",
        "    \n",
        "    Made up of self-attention and a feed forward layer.\n",
        "    Each of these sublayers have residual and layer norm, implemented by SublayerOutput.\n",
        "    '''\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer_output = clones(SublayerOutput(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"Transformer Encoder\"\n",
        "        x = self.sublayer_output[0](x, lambda x: self.self_attn(x, x, x, mask)) # Encoder self-attention\n",
        "        return self.sublayer_output[1](x, self.feed_forward)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRyWJrOV5E1s"
      },
      "source": [
        "def evaluate_model(model, iterator):\n",
        "    all_preds = []\n",
        "    all_y = []\n",
        "    for idx,batch in enumerate(iterator):\n",
        "        if torch.cuda.is_available():\n",
        "            x = batch.text.cuda()\n",
        "        else:\n",
        "            x = batch.text\n",
        "        y_pred = model(x)\n",
        "        predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n",
        "        all_preds.extend(predicted.numpy())\n",
        "        all_y.extend(batch.label.numpy())\n",
        "    score = accuracy_score(all_y, np.array(all_preds).flatten())\n",
        "    return score"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJkneYI74VlR"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config, src_vocab):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        h, N, dropout = self.config.h, self.config.N, self.config.dropout\n",
        "        d_model, d_ff = self.config.d_model, self.config.d_ff\n",
        "        \n",
        "        attn = MultiHeadedAttention(h, d_model)\n",
        "        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        position = PositionalEncoding(d_model, dropout)\n",
        "        \n",
        "        self.encoder = Encoder(EncoderLayer(config.d_model, deepcopy(attn), deepcopy(ff), dropout), N)\n",
        "        self.src_embed = nn.Sequential(Embeddings(config.d_model, src_vocab), deepcopy(position)) # Embeddings followed by PE\n",
        "\n",
        "        # Fully-Connected Layer\n",
        "        self.fc = nn.Linear(\n",
        "            self.config.d_model,\n",
        "            self.config.output_size\n",
        "        )\n",
        "        \n",
        "        # Softmax non-linearity\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded_sents = self.src_embed(x.permute(1,0)) # shape = (batch_size, sen_len, d_model)\n",
        "        encoded_sents = self.encoder(embedded_sents)\n",
        "        \n",
        "        # Convert input to (batch_size, d_model) for linear layer\n",
        "        final_feature_map = encoded_sents[:,-1,:]\n",
        "        final_out = self.fc(final_feature_map)\n",
        "        return self.softmax(final_out)\n",
        "    \n",
        "    def add_optimizer(self, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "    def add_loss_op(self, loss_op):\n",
        "        self.loss_op = loss_op\n",
        "    \n",
        "    def reduce_lr(self):\n",
        "        print(\"Reducing LR\")\n",
        "        for g in self.optimizer.param_groups:\n",
        "            g['lr'] = g['lr'] / 2\n",
        "                \n",
        "    def run_epoch(self, train_iterator, val_iterator, epoch):\n",
        "        train_losses = []\n",
        "        val_accuracies = []\n",
        "        losses = []\n",
        "        \n",
        "        # Reduce learning rate as number of epochs increase\n",
        "        if (epoch == int(self.config.max_epochs/3)) or (epoch == int(2*self.config.max_epochs/3)):\n",
        "            self.reduce_lr()\n",
        "            \n",
        "        for i, batch in enumerate(train_iterator):\n",
        "            self.optimizer.zero_grad()\n",
        "            if torch.cuda.is_available():\n",
        "                x = batch.text.cuda()\n",
        "                y = (batch.label - 1).type(torch.cuda.LongTensor)\n",
        "            else:\n",
        "                x = batch.text\n",
        "                y = (batch.label - 1).type(torch.LongTensor)\n",
        "            y_pred = self.__call__(x)\n",
        "            loss = self.loss_op(y_pred, y)\n",
        "            loss.backward()\n",
        "            losses.append(loss.data.cpu().numpy())\n",
        "            self.optimizer.step()\n",
        "    \n",
        "            if i % 100 == 0:\n",
        "                print(\"Iter: {}\".format(i+1))\n",
        "                avg_train_loss = np.mean(losses)\n",
        "                train_losses.append(avg_train_loss)\n",
        "                print(\"\\tAverage training loss: {:.5f}\".format(avg_train_loss))\n",
        "                losses = []\n",
        "                \n",
        "                # Evalute Accuracy on validation set\n",
        "                val_accuracy = evaluate_model(self, val_iterator)\n",
        "                print(\"\\tVal Accuracy: {:.4f}\".format(val_accuracy))\n",
        "                self.train()\n",
        "                \n",
        "        return train_losses, val_accuracies"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePD11fvg3ffU"
      },
      "source": [
        "## Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le5Jz71Q4Aqb"
      },
      "source": [
        "class Config(object):\n",
        "    N = 1 # 6 in Transformer Paper\n",
        "    d_model = 256 # 512 in Transformer Paper\n",
        "    d_ff = 512 # 2048 in Transformer Paper\n",
        "    h = 8\n",
        "    dropout = 0.1\n",
        "    output_size = 4\n",
        "    lr = 0.0003\n",
        "    max_epochs = 35\n",
        "    batch_size = 128\n",
        "    max_sen_len = 60"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBdCIMjFPadr",
        "outputId": "8effb42f-7075-4138-c0dd-275cd02a37a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCHDkU-tPwQD",
        "outputId": "e10a7462-3551-40ac-91e5-7b8403087d58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ls -l /content/drive/MyDrive/ag_news.train"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 30877815 Mar 11 05:59 /content/drive/MyDrive/ag_news.train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcMo5CJ83Fr8",
        "outputId": "8978befe-1538-448f-f303-0bd11a3e1594"
      },
      "source": [
        "config = Config()\n",
        "train_file = '/content/drive/MyDrive/ag_news.train'\n",
        "test_file = '/content/drive/MyDrive/ag_news.test'\n",
        "\n",
        "dataset = Dataset(config)\n",
        "dataset.load_data(train_file, test_file)\n",
        "\n",
        "# Create Model with specified optimizer and loss function\n",
        "model = Transformer(config, len(dataset.vocab))\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "model.train()\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "NLLLoss = nn.NLLLoss()\n",
        "model.add_optimizer(optimizer)\n",
        "model.add_loss_op(NLLLoss)\n",
        "\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for i in range(config.max_epochs):\n",
        "    print (\"Epoch: {}\".format(i))\n",
        "    train_loss,val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n",
        "    train_losses.append(train_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "train_acc = evaluate_model(model, dataset.train_iterator)\n",
        "val_acc = evaluate_model(model, dataset.val_iterator)\n",
        "test_acc = evaluate_model(model, dataset.test_iterator)\n",
        "\n",
        "print ('Final Training Accuracy: {:.4f}'.format(train_acc))\n",
        "print ('Final Validation Accuracy: {:.4f}'.format(val_acc))\n",
        "print ('Final Test Accuracy: {:.4f}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 96000 training examples\n",
            "Loaded 7600 test examples\n",
            "Loaded 24000 validation examples\n",
            "Epoch: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter: 1\n",
            "\tAverage training loss: -0.24737\n",
            "\tVal Accuracy: 0.2497\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.26642\n",
            "\tVal Accuracy: 0.2991\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.30204\n",
            "\tVal Accuracy: 0.3130\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.31729\n",
            "\tVal Accuracy: 0.3287\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.34812\n",
            "\tVal Accuracy: 0.3847\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.41199\n",
            "\tVal Accuracy: 0.4641\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.47166\n",
            "\tVal Accuracy: 0.5219\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.52699\n",
            "\tVal Accuracy: 0.5315\n",
            "Epoch: 1\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.59612\n",
            "\tVal Accuracy: 0.5581\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.55966\n",
            "\tVal Accuracy: 0.5705\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.57889\n",
            "\tVal Accuracy: 0.5872\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.59531\n",
            "\tVal Accuracy: 0.6016\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.60241\n",
            "\tVal Accuracy: 0.6198\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.62172\n",
            "\tVal Accuracy: 0.6369\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.63492\n",
            "\tVal Accuracy: 0.6440\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.65032\n",
            "\tVal Accuracy: 0.6540\n",
            "Epoch: 2\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.68375\n",
            "\tVal Accuracy: 0.6645\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.66645\n",
            "\tVal Accuracy: 0.6687\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.67965\n",
            "\tVal Accuracy: 0.6794\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.69142\n",
            "\tVal Accuracy: 0.6954\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.70813\n",
            "\tVal Accuracy: 0.7035\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.70449\n",
            "\tVal Accuracy: 0.7077\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.71129\n",
            "\tVal Accuracy: 0.7145\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.72375\n",
            "\tVal Accuracy: 0.7215\n",
            "Epoch: 3\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.75620\n",
            "\tVal Accuracy: 0.7222\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.73899\n",
            "\tVal Accuracy: 0.7347\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.74762\n",
            "\tVal Accuracy: 0.7422\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.75242\n",
            "\tVal Accuracy: 0.7500\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.75266\n",
            "\tVal Accuracy: 0.7476\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.76249\n",
            "\tVal Accuracy: 0.7572\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.75779\n",
            "\tVal Accuracy: 0.7639\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.76233\n",
            "\tVal Accuracy: 0.7624\n",
            "Epoch: 4\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.75168\n",
            "\tVal Accuracy: 0.7714\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.78086\n",
            "\tVal Accuracy: 0.7718\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.77919\n",
            "\tVal Accuracy: 0.7785\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.79106\n",
            "\tVal Accuracy: 0.7797\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.78453\n",
            "\tVal Accuracy: 0.7801\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.79490\n",
            "\tVal Accuracy: 0.7921\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.79092\n",
            "\tVal Accuracy: 0.7891\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.79714\n",
            "\tVal Accuracy: 0.7957\n",
            "Epoch: 5\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.85586\n",
            "\tVal Accuracy: 0.7954\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.80838\n",
            "\tVal Accuracy: 0.8005\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.80926\n",
            "\tVal Accuracy: 0.7963\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.81131\n",
            "\tVal Accuracy: 0.8007\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.81528\n",
            "\tVal Accuracy: 0.8050\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.81666\n",
            "\tVal Accuracy: 0.8046\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.82133\n",
            "\tVal Accuracy: 0.8154\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.82474\n",
            "\tVal Accuracy: 0.8197\n",
            "Epoch: 6\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.87400\n",
            "\tVal Accuracy: 0.8169\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.83445\n",
            "\tVal Accuracy: 0.8240\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.83065\n",
            "\tVal Accuracy: 0.8198\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.83064\n",
            "\tVal Accuracy: 0.8245\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.83779\n",
            "\tVal Accuracy: 0.8226\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.83173\n",
            "\tVal Accuracy: 0.8260\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.83699\n",
            "\tVal Accuracy: 0.8270\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.83785\n",
            "\tVal Accuracy: 0.8314\n",
            "Epoch: 7\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.88040\n",
            "\tVal Accuracy: 0.8310\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.83577\n",
            "\tVal Accuracy: 0.8275\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.84605\n",
            "\tVal Accuracy: 0.8360\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.85046\n",
            "\tVal Accuracy: 0.8344\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.85056\n",
            "\tVal Accuracy: 0.8347\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.84977\n",
            "\tVal Accuracy: 0.8342\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.85077\n",
            "\tVal Accuracy: 0.8397\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.84633\n",
            "\tVal Accuracy: 0.8315\n",
            "Epoch: 8\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.86912\n",
            "\tVal Accuracy: 0.8319\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.84696\n",
            "\tVal Accuracy: 0.8408\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.85784\n",
            "\tVal Accuracy: 0.8429\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.85962\n",
            "\tVal Accuracy: 0.8376\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.85460\n",
            "\tVal Accuracy: 0.8370\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.85884\n",
            "\tVal Accuracy: 0.8469\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.85990\n",
            "\tVal Accuracy: 0.8495\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.85594\n",
            "\tVal Accuracy: 0.8462\n",
            "Epoch: 9\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.86608\n",
            "\tVal Accuracy: 0.8417\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.86146\n",
            "\tVal Accuracy: 0.8385\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.86042\n",
            "\tVal Accuracy: 0.8498\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.86568\n",
            "\tVal Accuracy: 0.8484\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.86057\n",
            "\tVal Accuracy: 0.8526\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.86249\n",
            "\tVal Accuracy: 0.8447\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.86431\n",
            "\tVal Accuracy: 0.8517\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.85621\n",
            "\tVal Accuracy: 0.8523\n",
            "Epoch: 10\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.92394\n",
            "\tVal Accuracy: 0.8537\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.87278\n",
            "\tVal Accuracy: 0.8573\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.87083\n",
            "\tVal Accuracy: 0.8517\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.86701\n",
            "\tVal Accuracy: 0.8460\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.86755\n",
            "\tVal Accuracy: 0.8548\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.86924\n",
            "\tVal Accuracy: 0.8587\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.86673\n",
            "\tVal Accuracy: 0.8588\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.87375\n",
            "\tVal Accuracy: 0.8576\n",
            "Epoch: 11\n",
            "Reducing LR\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.83590\n",
            "\tVal Accuracy: 0.8598\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.87671\n",
            "\tVal Accuracy: 0.8600\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.87825\n",
            "\tVal Accuracy: 0.8605\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.87422\n",
            "\tVal Accuracy: 0.8625\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.87835\n",
            "\tVal Accuracy: 0.8606\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.87637\n",
            "\tVal Accuracy: 0.8612\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.87569\n",
            "\tVal Accuracy: 0.8609\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.87504\n",
            "\tVal Accuracy: 0.8645\n",
            "Epoch: 12\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.89382\n",
            "\tVal Accuracy: 0.8625\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.88293\n",
            "\tVal Accuracy: 0.8615\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.88339\n",
            "\tVal Accuracy: 0.8629\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.87972\n",
            "\tVal Accuracy: 0.8638\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.88174\n",
            "\tVal Accuracy: 0.8660\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.87878\n",
            "\tVal Accuracy: 0.8648\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.88574\n",
            "\tVal Accuracy: 0.8655\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.87856\n",
            "\tVal Accuracy: 0.8646\n",
            "Epoch: 13\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.84081\n",
            "\tVal Accuracy: 0.8644\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.88188\n",
            "\tVal Accuracy: 0.8635\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.88533\n",
            "\tVal Accuracy: 0.8638\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.88544\n",
            "\tVal Accuracy: 0.8652\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.88215\n",
            "\tVal Accuracy: 0.8681\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.88055\n",
            "\tVal Accuracy: 0.8656\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.88534\n",
            "\tVal Accuracy: 0.8647\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.88304\n",
            "\tVal Accuracy: 0.8667\n",
            "Epoch: 14\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.89555\n",
            "\tVal Accuracy: 0.8662\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.88399\n",
            "\tVal Accuracy: 0.8661\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.88432\n",
            "\tVal Accuracy: 0.8646\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.88644\n",
            "\tVal Accuracy: 0.8656\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.88399\n",
            "\tVal Accuracy: 0.8692\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.87929\n",
            "\tVal Accuracy: 0.8668\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.88601\n",
            "\tVal Accuracy: 0.8690\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.88765\n",
            "\tVal Accuracy: 0.8678\n",
            "Epoch: 15\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.93333\n",
            "\tVal Accuracy: 0.8684\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.88028\n",
            "\tVal Accuracy: 0.8667\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.88828\n",
            "\tVal Accuracy: 0.8664\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.88855\n",
            "\tVal Accuracy: 0.8661\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.88809\n",
            "\tVal Accuracy: 0.8702\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.88581\n",
            "\tVal Accuracy: 0.8651\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.88682\n",
            "\tVal Accuracy: 0.8701\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.89223\n",
            "\tVal Accuracy: 0.8680\n",
            "Epoch: 16\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.86299\n",
            "\tVal Accuracy: 0.8710\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.89273\n",
            "\tVal Accuracy: 0.8678\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.88510\n",
            "\tVal Accuracy: 0.8711\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.88972\n",
            "\tVal Accuracy: 0.8638\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.89146\n",
            "\tVal Accuracy: 0.8713\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.88784\n",
            "\tVal Accuracy: 0.8715\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.88840\n",
            "\tVal Accuracy: 0.8694\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.88767\n",
            "\tVal Accuracy: 0.8669\n",
            "Epoch: 17\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.88785\n",
            "\tVal Accuracy: 0.8691\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.88947\n",
            "\tVal Accuracy: 0.8687\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.89144\n",
            "\tVal Accuracy: 0.8702\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.89491\n",
            "\tVal Accuracy: 0.8721\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.88575\n",
            "\tVal Accuracy: 0.8698\n",
            "Iter: 501\n",
            "\tAverage training loss: -0.89012\n",
            "\tVal Accuracy: 0.8682\n",
            "Iter: 601\n",
            "\tAverage training loss: -0.88924\n",
            "\tVal Accuracy: 0.8722\n",
            "Iter: 701\n",
            "\tAverage training loss: -0.89189\n",
            "\tVal Accuracy: 0.8724\n",
            "Epoch: 18\n",
            "Iter: 1\n",
            "\tAverage training loss: -0.86841\n",
            "\tVal Accuracy: 0.8726\n",
            "Iter: 101\n",
            "\tAverage training loss: -0.89313\n",
            "\tVal Accuracy: 0.8686\n",
            "Iter: 201\n",
            "\tAverage training loss: -0.89450\n",
            "\tVal Accuracy: 0.8724\n",
            "Iter: 301\n",
            "\tAverage training loss: -0.89249\n",
            "\tVal Accuracy: 0.8737\n",
            "Iter: 401\n",
            "\tAverage training loss: -0.89684\n",
            "\tVal Accuracy: 0.8705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehjGW0-C-BAI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}