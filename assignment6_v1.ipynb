{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled34.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopal2812/mlblr/blob/master/assignment6_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk3hWKZrWyID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de86293b-b942-41c3-ed92-b54b8a28f140"
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import string\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from random import randint\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTHrbpjl58m6",
        "colab_type": "code",
        "outputId": "e434a782-40b3-4e63-a59e-bd6ccf802a52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHUSrF01aABw",
        "colab_type": "code",
        "outputId": "0962375a-cb97-4269-acd9-b044c89359f4",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-16a467b5-50bc-4236-b2b1-967acc063766\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-16a467b5-50bc-4236-b2b1-967acc063766\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving wonderland.txt to wonderland.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iRaOkbjd6KY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkgMn6T4LvyK",
        "colab_type": "text"
      },
      "source": [
        "2. Remove all the punctuation from the source text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgDjjWb7c1PX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# replace '--' with a space ' '\n",
        "\tdoc = doc.replace('--', ' ')\n",
        "  \n",
        " \t# replace '\\n' with a space ' '\n",
        "\tdoc = doc.replace('\\n', ' ')\n",
        "  \n",
        "\t# split into tokens by full stop\n",
        "\ttokens = doc.split('.')\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# make lower case\n",
        "\ttokens = [word.lower() for word in tokens]\n",
        "\treturn tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yaw3vLRqdRgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWTUX4yHdTFW",
        "colab_type": "code",
        "outputId": "042b0564-9ba8-404a-91d4-1d64ee57ec41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "in_filename = \"wonderland.txt\"\n",
        "doc = load_doc(in_filename)\n",
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['           alices adventures in wonderland  lewis carroll  the millennium fulcrum edition 3', '0     chapter i', ' down the rabbithole  alice was beginning to get very tired of sitting by her sister on the bank and of having nothing to do once or twice she had peeped into the book her sister was reading but it had no pictures or conversations in it and what is the use of a book thought alice without pictures or conversations  so she was considering in her own mind as well as she could for the hot day made her feel very sleepy and stupid whether the pleasure of making a daisychain would be worth the trouble of getting up and picking the daisies when suddenly a white rabbit with pink eyes ran close by her', '  there was nothing so very remarkable in that nor did alice think it so very much out of the way to hear the rabbit say to itself oh dear oh dear i shall be late when she thought it over afterwards it occurred to her that she ought to have wondered at this but at the time it all seemed quite natural but when the rabbit actually took a watch out of its waistcoatpocket and looked at it and then hurried on alice started to her feet for it flashed across her mind that she had never before seen a rabbit with either a waistcoatpocket or a watch to take out of it and burning with curiosity she ran across the field after it and fortunately was just in time to see it pop down a large rabbithole under the hedge', '  in another moment down went alice after it never once considering how in the world she was to get out again', '  the rabbithole went straight on like a tunnel for some way and then dipped suddenly down so suddenly that alice had not a moment to think about stopping herself before she found herself falling down a very deep well', '  either the well was very deep or she fell very slowly for she had plenty of time as she went down to look about her and to wonder what was going to happen next', ' first she tried to look down and make out what she was coming to but it was too dark to see anything then she looked at the sides of the well and noticed that they were filled with cupboards and bookshelves here and there she saw maps and pictures hung upon pegs', ' she took down a jar from one of the shelves as she passed it was labelled orange marmalade but to her great disappointment it was empty she did not like to drop the jar for fear of killing somebody so managed to put it into one of the cupboards as she fell past it', '  well thought alice to herself after such a fall as this i shall think nothing of tumbling down stairs how brave theyll all think me at home why i wouldnt say anything about it even if i fell off the top of the house which was very likely true', '  down down down', ' would the fall never come to an end i wonder how many miles ive fallen by this time she said aloud', ' i must be getting somewhere near the centre of the earth', ' let me see that would be four thousand miles down i think  for you see alice had learnt several things of this sort in her lessons in the schoolroom and though this was not a very good opportunity for showing off her knowledge as there was no one to listen to her still it was good practice to say it over  yes thats about the right distance but then i wonder what latitude or longitude ive got to alice had no idea what latitude was or longitude either but thought they were nice grand words to say', '  presently she began again', ' i wonder if i shall fall right through the earth how funny itll seem to come out among the people that walk with their heads downward the antipathies i think  she was rather glad there was no one listening this time as it didnt sound at all the right word  but i shall have to ask them what the name of the country is you know', ' please maam is this new zealand or australia and she tried to curtsey as she spoke fancy curtseying as youre falling through the air do you think you could manage it and what an ignorant little girl shell think me for asking no itll never do to ask perhaps i shall see it written up somewhere', '  down down down', ' there was nothing else to do so alice soon began talking again', ' dinahll miss me very much tonight i should think dinah was the cat', ' i hope theyll remember her saucer of milk at teatime', ' dinah my dear i wish you were down here with me there are no mice in the air im afraid but you might catch a bat and thats very like a mouse you know', ' but do cats eat bats i wonder and here alice began to get rather sleepy and went on saying to herself in a dreamy sort of way do cats eat bats do cats eat bats and sometimes do bats eat cats for you see as she couldnt answer either question it didnt much matter which way she put it', ' she felt that she was dozing off and had just begun to dream that she was walking hand in hand with dinah and saying to her very earnestly now dinah tell me the truth did you ever eat a bat when suddenly thump thump down she came upon a heap of sticks and dry leaves and the fall was over', '  alice was not a bit hurt and she jumped up on to her feet in a moment she looked up but it was all dark overhead before her was another long passage and the white rabbit was still in sight hurrying down it', ' there was not a moment to be lost away went alice like the wind and was just in time to hear it say as it turned a corner oh my ears and whiskers how late its getting she was close behind it when she turned the corner but the rabbit was no longer to be seen she found herself in a long low hall which was lit up by a row of lamps hanging from the roof', '  there were doors all round the hall but they were all locked and when alice had been all the way down one side and up the other trying every door she walked sadly down the middle wondering how she was ever to get out again', '  suddenly she came upon a little threelegged table all made of solid glass there was nothing on it except a tiny golden key and alices first thought was that it might belong to one of the doors of the hall but alas either the locks were too large or the key was too small but at any rate it would not open any of them', ' however on the second time round she came upon a low curtain she had not noticed before and behind it was a little door about fifteen inches high she tried the little golden key in the lock and to her great delight it fitted  alice opened the door and found that it led into a small passage not much larger than a rathole she knelt down and looked along the passage into the loveliest garden you ever saw', ' how she longed to get out of that dark hall and wander about among those beds of bright flowers and those cool fountains but she could not even get her head through the doorway and even if my head would go through thought poor alice it would be of very little use without my shoulders', ' oh how i wish i could shut up like a telescope i think i could if i only knew how to begin', ' for you see so many outoftheway things had happened lately that alice had begun to think that very few things indeed were really impossible', '  there seemed to be no use in waiting by the little door so she went back to the table half hoping she might find another key on it or at any rate a book of rules for shutting people up like telescopes this time she found a little bottle on it which certainly was not here before said alice and round the neck of the bottle was a paper label with the words drink me beautifully printed on it in large letters', '  it was all very well to say drink me but the wise little alice was not going to do that in a hurry', ' no ill look first she said and see whether its marked poison or not for she had read several nice little histories about children who had got burnt and eaten up by wild beasts and other unpleasant things all because they would not remember the simple rules their friends had taught them such as that a redhot poker will burn you if you hold it too long and that if you cut your finger very deeply with a knife it usually bleeds and she had never forgotten that if you drink much from a bottle marked poison it is almost certain to disagree with you sooner or later', '  however this bottle was not marked poison so alice ventured to taste it and finding it very nice it had in fact a sort of mixed flavour of cherrytart custard pineapple roast turkey toffee and hot buttered toast she very soon finished it off', '                                                                                    what a curious feeling said alice i must be shutting up like a telescope', '  and so it was indeed she was now only ten inches high and her face brightened up at the thought that she was now the right size for going through the little door into that lovely garden', ' first however she waited for a few minutes to see if she was going to shrink any further she felt a little nervous about this for it might end you know said alice to herself in my going out altogether like a candle', ' i wonder what i should be like then and she tried to fancy what the flame of a candle is like after the candle is blown out for she could not remember ever having seen such a thing', '  after a while finding that nothing more happened she decided on going into the garden at once but alas for poor alice when she got to the door she found she had forgotten the little golden key and when she went back to the table for it she found she could not possibly reach it she could see it quite plainly through the glass and she tried her best to climb up one of the legs of the table but it was too slippery and when she had tired herself out with trying the poor little thing sat down and cried', '  come theres no use in crying like that said alice to herself rather sharply i advise you to leave off this minute she generally gave herself very good advice though she very seldom followed it and sometimes she scolded herself so severely as to bring tears into her eyes and once she remembered trying to box her own ears for having cheated herself in a game of croquet she was playing against herself for this curious child was very fond of pretending to be two people', ' but its no use now thought poor alice to pretend to be two people why theres hardly enough of me left to make one respectable person  soon her eye fell on a little glass box that was lying under the table she opened it and found in it a very small cake on which the words eat me were beautifully marked in currants', ' well ill eat it said alice and if it makes me grow larger i can reach the key and if it makes me grow smaller i can creep under the door so either way ill get into the garden and i dont care which happens  she ate a little bit and said anxiously to herself which way which way holding her hand on the top of her head to feel which way it was growing and she was quite surprised to find that she remained the same size to be sure this generally happens when one eats cake but alice had got so much into the way of expecting nothing but outoftheway things to happen that it seemed quite dull and stupid for life to go on in the common way', '  so she set to work and very soon finished off the cake', '                                                                                       chapter ii', ' the pool of tears  curiouser and curiouser cried alice she was so much surprised that for the moment she quite forgot how to speak good english now im opening out like the largest telescope that ever was goodbye feet for when she looked down at her feet they seemed to be almost out of sight they were getting so far off', ' oh my poor little feet i wonder who will put on your shoes and stockings for you now dears im sure i shant be able i shall be a great deal too far off to trouble myself about you you must manage the best way you can but i must be kind to them thought alice or perhaps they wont walk the way i want to go let me see ill give them a new pair of boots every christmas', '  and she went on planning to herself how she would manage it', ' they must go by the carrier she thought and how funny itll seem sending presents to ones own feet and how odd the directions will look       alices right foot esq', '        hearthrug          near the fender            with alices love', '  oh dear what nonsense im talking  just then her head struck against the roof of the hall in fact she was now more than nine feet high and she at once took up the little golden key and hurried off to the garden door', '  poor alice it was as much as she could do lying down on one side to look through into the garden with one eye but to get through was more hopeless than ever she sat down and began to cry again', '  you ought to be ashamed of yourself said alice a great girl like you she might well say this to go on crying in this way stop this moment i tell you but she went on all the same shedding gallons of tears until there was a large pool all round her about four inches deep and reaching half down the hall', '  after a time she heard a little pattering of feet in the distance and she hastily dried her eyes to see what was coming', ' it was the white rabbit returning splendidly dressed with a pair of white kid gloves in one hand and a large fan in the other he came trotting along in a great hurry muttering to himself as he came oh the duchess the duchess oh wont she be savage if ive kept her waiting alice felt so desperate that she was ready to ask help of any one so when the rabbit came near her she began in a low timid voice if you please sir  the rabbit started violently dropped the white kid gloves and the fan and skurried away into the darkness as hard as he could go', '  alice took up the fan and gloves and as the hall was very hot she kept fanning herself all the time she went on talking dear dear how queer everything is today and yesterday things went on just as usual', ' i wonder if ive been changed in the night let me think was i the same when i got up this morning i almost think i can remember feeling a little different', ' but if im not the same the next question is who in the world am i ah thats the great puzzle and she began thinking over all the children she knew that were of the same age as herself to see if she could have been changed for any of them', '  im sure im not ada she said for her hair goes in such long ringlets and mine doesnt go in ringlets at all and im sure i cant be mabel for i know all sorts of things and she oh she knows such a very little besides shes she and im i and oh dear how puzzling it all is ill try if i know all the things i used to know', ' let me see four times five is twelve and four times six is thirteen and four times seven is oh dear i shall never get to twenty at that rate however the multiplication table doesnt signify lets try geography', ' london is the capital of paris and paris is the capital of rome and rome no thats all wrong im certain i must have been changed for mabel ill try and say how doth the little  and she crossed her hands on her lap as if she were saying lessons and began to repeat it but her voice sounded hoarse and strange and the words did not come the same as they used to do        how doth the little crocodile       improve his shining tail      and pour the waters of the nile       on every golden scale       how cheerfully he seems to grin       how neatly spread his claws      and welcome little fishes in       with gently smiling jaws  im sure those are not the right words said poor alice and her eyes filled with tears again as she went on i must be mabel after all and i shall have to go and live in that poky little house and have next to no toys to play with and oh ever so many lessons to learn no ive made up my mind about it if im mabel ill stay down here itll be no use their putting their heads down and saying come up again dear i shall only look up and say who am i then tell me that first and then if i like being that person ill come up if not ill stay down here till im somebody else but oh dear cried alice with a sudden burst of tears i do wish they would put their heads down i am so very tired of being all alone here  as she said this she looked down at her hands and was surprised to see that she had put on one of the rabbits little white kid gloves while she was talking', ' how can i have done that she thought', ' i must be growing small again', ' she got up and went to the table to measure herself by it and found that as nearly as she could guess she was now about two feet high and was going on shrinking rapidly she soon found out that the cause of this was the fan she was holding and she dropped it hastily just in time to avoid shrinking away altogether', '  that was a narrow escape said alice a good deal frightened at the sudden change but very glad to find herself still in existence and now for the garden and she ran with all speed back to the little door but alas the little door was shut again and the little golden key was lying on the glass table as before and things are worse than ever thought the poor child for i never was so small as this before never and i declare its too bad that it is  as she said these words her foot slipped and in another moment splash she was up to her chin in salt water', ' her first idea was that she had somehow fallen into the sea and in that case i can go back by railway she said to herself', ' alice had been to the seaside once in her life and had come to the general conclusion that wherever you go to on the english coast you find a number of bathing machines in the sea some children digging in the sand with wooden spades then a row of lodging houses and behind them a railway station', ' however she soon made out that she was in the pool of tears which she had wept when she was nine feet high', '  i wish i hadnt cried so much said alice as she swam about trying to find her way out', ' i shall be punished for it now i suppose by being drowned in my own tears that will be a queer thing to be sure however everything is queer today', '  just then she heard something splashing about in the pool a little way off and she swam nearer to make out what it was at first she thought it must be a walrus or hippopotamus but then she remembered how small she was now and she soon made out that it was only a mouse that had slipped in like herself', '  would it be of any use now thought alice to speak to this mouse everything is so outoftheway down here that i should think very likely it can talk at any rate theres no harm in trying', ' so she began o mouse do you know the way out of this pool i am very tired of swimming about here o mouse alice thought this must be the right way of speaking to a mouse she had never done such a thing before but she remembered having seen in her brothers latin grammar a mouse of a mouse to a mouse a mouse o mouse the mouse looked at her rather inquisitively and seemed to her to wink with one of its little eyes but it said nothing', '  perhaps it doesnt understand english thought alice i daresay its a french mouse come over with william the conqueror', ' for with all her knowledge of history alice had no very clear notion how long ago anything had happened', ' so she began again ou est ma chatte which was the first sentence in her french lessonbook', ' the mouse gave a sudden leap out of the water and seemed to quiver all over with fright', ' oh i beg your pardon cried alice hastily afraid that she had hurt the poor animals feelings', ' i quite forgot you didnt like cats', '  not like cats cried the mouse in a shrill passionate voice', ' would you like cats if you were me  well perhaps not said alice in a soothing tone dont be angry about it', ' and yet i wish i could show you our cat dinah i think youd take a fancy to cats if you could only see her', ' she is such a dear quiet thing alice went on half to herself as she swam lazily about in the pool and she sits purring so nicely by the fire licking her paws and washing her face and she is such a nice soft thing to nurse and shes such a capital one for catching mice oh i beg your pardon cried alice again for this time the mouse was bristling all over and she felt certain it must be really offended', ' we wont talk about her any more if youd rather not', '  we indeed cried the mouse who was trembling down to the end of his tail', ' as if i would talk on such a subject our family always hated cats nasty low vulgar things dont let me hear the name again  i wont indeed said alice in a great hurry to change the subject of conversation', ' are you are you fond of of dogs the mouse did not answer so alice went on eagerly there is such a nice little dog near our house i should like to show you a little brighteyed terrier you know with oh such long curly brown hair and itll fetch things when you throw them and itll sit up and beg for its dinner and all sorts of things i cant remember half of them and it belongs to a farmer you know and he says its so useful its worth a hundred pounds he says it kills all the rats and oh dear cried alice in a sorrowful tone im afraid ive offended it again for the mouse was swimming away from her as hard as it could go and making quite a commotion in the pool as it went', '  so she called softly after it mouse dear do come back again and we wont talk about cats or dogs either if you dont like them when the mouse heard this it turned round and swam slowly back to her its face was quite pale with passion alice thought and it said in a low trembling voice let us get to the shore and then ill tell you my history and youll understand why it is i hate cats and dogs', '  it was high time to go for the pool was getting quite crowded with the birds and animals that had fallen into it there were a duck and a dodo a lory and an eaglet and several other curious creatures', ' alice led the way and the whole party swam to the shore', '     chapter iii', ' a caucusrace and a long tale  they were indeed a queerlooking party that assembled on the bank the birds with draggled feathers the animals with their fur clinging close to them and all dripping wet cross and uncomfortable', '  the first question of course was how to get dry again they had a consultation about this and after a few minutes it seemed quite natural to alice to find herself talking familiarly with them as if she had known them all her life', ' indeed she had quite a long argument with the lory who at last turned sulky and would only say i am older than you and must know better and this alice would not allow without knowing how old it was and as the lory positively refused to tell its age there was no more to be said', '  at last the mouse who seemed to be a person of authority among them called out sit down all of you and listen to me ill soon make you dry enough they all sat down at once in a large ring with the mouse in the middle', ' alice kept her eyes anxiously fixed on it for she felt sure she would catch a bad cold if she did not get dry very soon', '  ahem said the mouse with an important air are you all ready this is the driest thing i know', ' silence all round if you please william the conqueror whose cause was favoured by the pope was soon submitted to by the english who wanted leaders and had been of late much accustomed to usurpation and conquest', ' edwin and morcar the earls of mercia and northumbria   ugh said the lory with a shiver', '  i beg your pardon said the mouse frowning but very politely did you speak  not i said the lory hastily', '  i thought you did said the mouse', '  i proceed', ' edwin and morcar the earls of mercia and northumbria declared for him and even stigand the patriotic archbishop of canterbury found it advisable   found what said the duck', '  found it the mouse replied rather crossly of course you know what it means', '  i know what it means well enough when i find a thing said the duck its generally a frog or a worm', ' the question is what did the archbishop find  the mouse did not notice this question but hurriedly went on  found it advisable to go with edgar atheling to meet william and offer him the crown', ' williams conduct at first was moderate', ' but the insolence of his normans  how are you getting on now my dear it continued turning to alice as it spoke', '  as wet as ever said alice in a melancholy tone it doesnt seem to dry me at all', '  in that case said the dodo solemnly rising to its feet i move that the meeting adjourn for the immediate adoption of more energetic remedies   speak english said the eaglet', ' i dont know the meaning of half those long words and whats more i dont believe you do either and the eaglet bent down its head to hide a smile some of the other birds tittered audibly', '  what i was going to say said the dodo in an offended tone was that the best thing to get us dry would be a caucusrace', '  what is a caucusrace said alice not that she wanted much to know but the dodo had paused as if it thought that somebody ought to speak and no one else seemed inclined to say anything', '  why said the dodo the best way to explain it is to do it', ' and as you might like to try the thing yourself some winter day i will tell you how the dodo managed it', '  first it marked out a racecourse in a sort of circle the exact shape doesnt matter it said and then all the party were placed along the course here and there', ' there was no one two three and away but they began running when they liked and left off when they liked so that it was not easy to know when the race was over', ' however when they had been running half an hour or so and were quite dry again the dodo suddenly called out the race is over and they all crowded round it panting and asking but who has won  this question the dodo could not answer without a great deal of thought and it sat for a long time with one finger pressed upon its forehead the position in which you usually see shakespeare in the pictures of him while the rest waited in silence', ' at last the dodo said everybody has won and all must have prizes', '  but who is to give the prizes quite a chorus of voices asked', '  why she of course said the dodo pointing to alice with one finger and the whole party at once crowded round her calling out in a confused way prizes prizes  alice had no idea what to do and in despair she put her hand in her pocket and pulled out a box of comfits luckily the salt water had not got into it and handed them round as prizes', ' there was exactly one apiece all round', '  but she must have a prize herself you know said the mouse', '  of course the dodo replied very gravely', ' what else have you got in your pocket he went on turning to alice', '  only a thimble said alice sadly', '  hand it over here said the dodo', '  then they all crowded round her once more while the dodo solemnly presented the thimble saying we beg your acceptance of this elegant thimble and when it had finished this short speech they all cheered', '  alice thought the whole thing very absurd but they all looked so grave that she did not dare to laugh and as she could not think of anything to say she simply bowed and took the thimble looking as solemn as she could', '  the next thing was to eat the comfits this caused some noise and confusion as the large birds complained that they could not taste theirs and the small ones choked and had to be patted on the back', ' however it was over at last and they sat down again in a ring and begged the mouse to tell them something more', '  you promised to tell me your history you know said alice and why it is you hate c and d she added in a whisper half afraid that it would be offended again', '  mine is a long and a sad tale said the mouse turning to alice and sighing', '  it is a long tail certainly said alice looking down with wonder at the mouses tail but why do you call it sad and she kept on puzzling about it while the mouse was speaking so that her idea of the tale was something like this            fury said to a          mouse that he         met in the        house      let us       both go to        law i will         prosecute          you', ' come            ill take no            denial we           must have a         trial for       really this      morning ive     nothing     to do', '      said the       mouse to the        cur such         a trial          dear sir             with           no jury         or judge        would be       wasting       our       breath', '        ill be         judge ill          be jury             said          cunning           old fury           ill           try the             whole             cause               and            condemn            you           to            death', '   you are not attending said the mouse to alice severely', ' what are you thinking of  i beg your pardon said alice very humbly you had got to the fifth bend i think  i had not cried the mouse sharply and very angrily', '  a knot said alice always ready to make herself useful and looking anxiously about her', ' oh do let me help to undo it  i shall do nothing of the sort said the mouse getting up and walking away', ' you insult me by talking such nonsense  i didnt mean it pleaded poor alice', ' but youre so easily offended you know  the mouse only growled in reply', '  please come back and finish your story alice called after it and the others all joined in chorus yes please do but the mouse only shook its head impatiently and walked a little quicker', '  what a pity it wouldnt stay sighed the lory as soon as it was quite out of sight and an old crab took the opportunity of saying to her daughter ah my dear let this be a lesson to you never to lose your temper hold your tongue ma said the young crab a little snappishly', ' youre enough to try the patience of an oyster  i wish i had our dinah here i know i do said alice aloud addressing nobody in particular', ' shed soon fetch it back  and who is dinah if i might venture to ask the question said the lory', '  alice replied eagerly for she was always ready to talk about her pet dinahs our cat', ' and shes such a capital one for catching mice you cant think and oh i wish you could see her after the birds why shell eat a little bird as soon as look at it  this speech caused a remarkable sensation among the party', ' some of the birds hurried off at once one old magpie began wrapping itself up very carefully remarking i really must be getting home the nightair doesnt suit my throat and a canary called out in a trembling voice to its children come away my dears its high time you were all in bed on various pretexts they all moved off and alice was soon left alone', '  i wish i hadnt mentioned dinah she said to herself in a melancholy tone', ' nobody seems to like her down here and im sure shes the best cat in the world oh my dear dinah i wonder if i shall ever see you any more and here poor alice began to cry again for she felt very lonely and lowspirited', ' in a little while however she again heard a little pattering of footsteps in the distance and she looked up eagerly half hoping that the mouse had changed his mind and was coming back to finish his story', '     chapter iv', ' the rabbit sends in a little bill  it was the white rabbit trotting slowly back again and looking anxiously about as it went as if it had lost something and she heard it muttering to itself the duchess the duchess oh my dear paws oh my fur and whiskers shell get me executed as sure as ferrets are ferrets where can i have dropped them i wonder alice guessed in a moment that it was looking for the fan and the pair of white kid gloves and she very goodnaturedly began hunting about for them but they were nowhere to be seen everything seemed to have changed since her swim in the pool and the great hall with the glass table and the little door had vanished completely', '  very soon the rabbit noticed alice as she went hunting about and called out to her in an angry tone why mary ann what are you doing out here run home this moment and fetch me a pair of gloves and a fan quick now and alice was so much frightened that she ran off at once in the direction it pointed to without trying to explain the mistake it had made', '  he took me for his housemaid she said to herself as she ran', ' how surprised hell be when he finds out who i am but id better take him his fan and gloves that is if i can find them', ' as she said this she came upon a neat little house on the door of which was a bright brass plate with the name w', ' rabbit engraved upon it', ' she went in without knocking and hurried upstairs in great fear lest she should meet the real mary ann and be turned out of the house before she had found the fan and gloves', '  how queer it seems alice said to herself to be going messages for a rabbit i suppose dinahll be sending me on messages next and she began fancying the sort of thing that would happen miss alice come here directly and get ready for your walk coming in a minute nurse but ive got to see that the mouse doesnt get out', ' only i dont think alice went on that theyd let dinah stop in the house if it began ordering people about like that  by this time she had found her way into a tidy little room with a table in the window and on it as she had hoped a fan and two or three pairs of tiny white kid gloves she took up the fan and a pair of the gloves and was just going to leave the room when her eye fell upon a little bottle that stood near the lookingglass', ' there was no label this time with the words drink me but nevertheless she uncorked it and put it to her lips', ' i know something interesting is sure to happen she said to herself whenever i eat or drink anything so ill just see what this bottle does', ' i do hope itll make me grow large again for really im quite tired of being such a tiny little thing  it did so indeed and much sooner than she had expected before she had drunk half the bottle she found her head pressing against the ceiling and had to stoop to save her neck from being broken', ' she hastily put down the bottle saying to herself thats quite enough i hope i shant grow any more as it is i cant get out at the door i do wish i hadnt drunk quite so much  alas it was too late to wish that she went on growing and growing and very soon had to kneel down on the floor in another minute there was not even room for this and she tried the effect of lying down with one elbow against the door and the other arm curled round her head', ' still she went on growing and as a last resource she put one arm out of the window and one foot up the chimney and said to herself now i can do no more whatever happens', ' what will become of me  luckily for alice the little magic bottle had now had its full effect and she grew no larger still it was very uncomfortable and as there seemed to be no sort of chance of her ever getting out of the room again no wonder she felt unhappy', '  it was much pleasanter at home thought poor alice when one wasnt always growing larger and smaller and being ordered about by mice and rabbits', ' i almost wish i hadnt gone down that rabbithole and yet and yet its rather curious you know this sort of life i do wonder what can have happened to me when i used to read fairytales i fancied that kind of thing never happened and now here i am in the middle of one there ought to be a book written about me that there ought and when i grow up ill write one but im grown up now she added in a sorrowful tone at least theres no room to grow up any more here', '  but then thought alice shall i never get any older than i am now thatll be a comfort one way never to be an old woman but then always to have lessons to learn oh i shouldnt like that  oh you foolish alice she answered herself', ' how can you learn lessons in here why theres hardly room for you and no room at all for any lessonbooks  and so she went on taking first one side and then the other and making quite a conversation of it altogether but after a few minutes she heard a voice outside and stopped to listen', '  mary ann mary ann said the voice', ' fetch me my gloves this moment then came a little pattering of feet on the stairs', ' alice knew it was the rabbit coming to look for her and she trembled till she shook the house quite forgetting that she was now about a thousand times as large as the rabbit and had no reason to be afraid of it', '  presently the rabbit came up to the door and tried to open it but as the door opened inwards and alices elbow was pressed hard against it that attempt proved a failure', ' alice heard it say to itself then ill go round and get in at the window', '  that you wont thought alice and after waiting till she fancied she heard the rabbit just under the window she suddenly spread out her hand and made a snatch in the air', ' she did not get hold of anything but she heard a little shriek and a fall and a crash of broken glass from which she concluded that it was just possible it had fallen into a cucumberframe or something of the sort', '  next came an angry voice the rabbits pat pat where are you and then a voice she had never heard before sure then im here digging for apples yer honour  digging for apples indeed said the rabbit angrily', ' here come and help me out of this sounds of more broken glass', '  now tell me pat whats that in the window  sure its an arm yer honour he pronounced it arrum', '  an arm you goose who ever saw one that size why it fills the whole window  sure it does yer honour but its an arm for all that', '  well its got no business there at any rate go and take it away  there was a long silence after this and alice could only hear whispers now and then such as sure i dont like it yer honour at all at all do as i tell you you coward and at last she spread out her hand again and made another snatch in the air', ' this time there were two little shrieks and more sounds of broken glass', ' what a number of cucumberframes there must be thought alice', ' i wonder what theyll do next as for pulling me out of the window i only wish they could im sure i dont want to stay in here any longer  she waited for some time without hearing anything more at last came a rumbling of little cartwheels and the sound of a good many voices all talking together she made out the words wheres the other ladder why i hadnt to bring but one bills got the other bill fetch it here lad here put em up at this corner no tie em together first they dont reach half high enough yet oh theyll do well enough dont be particular here bill catch hold of this rope will the roof bear mind that loose slate oh its coming down heads below a loud crash now who did that it was bill i fancy whos to go down the chimney nay i shant you do it that i wont then bills to go down here bill the master says youre to go down the chimney  oh so bills got to come down the chimney has he said alice to herself', ' shy they seem to put everything upon bill i wouldnt be in bills place for a good deal this fireplace is narrow to be sure but i think i can kick a little  she drew her foot as far down the chimney as she could and waited till she heard a little animal she couldnt guess of what sort it was scratching and scrambling about in the chimney close above her then saying to herself this is bill she gave one sharp kick and waited to see what would happen next', '  the first thing she heard was a general chorus of there goes bill then the rabbits voice along catch him you by the hedge then silence and then another confusion of voices hold up his head brandy now dont choke him how was it old fellow what happened to you tell us all about it  last came a little feeble squeaking voice thats bill thought alice well i hardly know no more thank ye im better now but im a deal too flustered to tell you all i know is something comes at me like a jackinthebox and up i goes like a skyrocket  so you did old fellow said the others', '  we must burn the house down said the rabbits voice and alice called out as loud as she could if you do', ' ill set dinah at you  there was a dead silence instantly and alice thought to herself i wonder what they will do next if they had any sense theyd take the roof off', ' after a minute or two they began moving about again and alice heard the rabbit say a barrowful will do to begin with', '  a barrowful of what thought alice but she had not long to doubt for the next moment a shower of little pebbles came rattling in at the window and some of them hit her in the face', ' ill put a stop to this she said to herself and shouted out youd better not do that again which produced another dead silence', '  alice noticed with some surprise that the pebbles were all turning into little cakes as they lay on the floor and a bright idea came into her head', ' if i eat one of these cakes she thought its sure to make some change in my size and as it cant possibly make me larger it must make me smaller i suppose', '  so she swallowed one of the cakes and was delighted to find that she began shrinking directly', ' as soon as she was small enough to get through the door she ran out of the house and found quite a crowd of little animals and birds waiting outside']\n",
            "Total Tokens: 991\n",
            "Unique Tokens: 990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h32uQEgeL8FA",
        "colab_type": "text"
      },
      "source": [
        "3. Train the model on padded sequences (Links to an external site.) rather than random sequences of characters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqGFfvKTqqNz",
        "colab_type": "code",
        "outputId": "710adfcd-21a5-463e-c1ad-47b711d5431b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#We would be taking a compromise so we save a lot of memory and have to truncate maybe 5% of all the sequences. \n",
        "#Sequences that are too long should be truncated at the end\n",
        "num_lines = [len(line) for line in tokens]\n",
        "std_line = np.std(num_lines)\n",
        "mean_line = np.mean(num_lines)\n",
        "seq_len= (int)((mean_line+2*std_line))\n",
        "print(mean_line, std_line, seq_len, np.median(num_lines))\n",
        "\n",
        "#also find the most frequent length\n",
        "(values,counts) = np.unique(num_lines,return_counts=True)\n",
        "ind=np.argmax(counts)\n",
        "print(values[ind])\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "\n",
        "def get_sequence_of_tokens(tokens):\n",
        "    ## tokenization\n",
        "    tokenizer.fit_on_texts(tokens)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    \n",
        "    ## convert data to sequence of tokens \n",
        "    input_sequences = []\n",
        "    for line in tokens:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    return input_sequences, vocab_size\n",
        "\n",
        "sequences, vocab_size = get_sequence_of_tokens(tokens)\n",
        "sequences[:1]\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "137.6024217961655 135.70159788002653 409 94.0\n",
            "30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[303, 467]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WcjZEFIzoxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_padded_sequences(input_sequences):\n",
        "    max_sequence_len = max([len(i) for i in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "    X, y = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "    y = to_categorical(y, num_classes=vocab_size)\n",
        "    return X, y, max_sequence_len\n",
        "\n",
        "X, y, seq_length = generate_padded_sequences(sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRqAm6ODL-2L",
        "colab_type": "text"
      },
      "source": [
        "5. Add dropout to the input layer, remove it from the layer before dense layer. Use Dropout value of 0.1 everywhere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZe4pxfo9Y5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 30, input_length=seq_length-1))\n",
        "#model.add(Dropout(0.1))\n",
        "#A dropout on the input means that for a given probability, the data on the input \n",
        "# connection to each LSTM block will be excluded from node activation and weight updates.\n",
        "model.add(LSTM(256, input_shape=(seq_length, vocab_size), return_sequences=True, dropout=0.1))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmI52Zhnfzrz",
        "colab_type": "code",
        "outputId": "dcad9aa6-bf30-4972-f8d3-9fad1fedf95c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "print(model.summary())\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 300, 30)           79440     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 300, 30)           0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 300, 256)          293888    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 300, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2648)              680536    \n",
            "=================================================================\n",
            "Total params: 1,579,176\n",
            "Trainable params: 1,579,176\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy4q8sUtihYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKdK6gQlMIn8",
        "colab_type": "text"
      },
      "source": [
        "4. Train the model for 100 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMBv3qmMimrY",
        "colab_type": "code",
        "outputId": "135c3057-6a39-4db9-dbf0-79e73cf10833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X, y, epochs=100, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0726 17:37:04.334365 140093314234240 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "25562/25562 [==============================] - 188s 7ms/step - loss: 6.4132\n",
            "\n",
            "Epoch 00001: loss improved from inf to 6.41316, saving model to weights-improvement-01-6.4132-bigger.hdf5\n",
            "Epoch 2/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1998\n",
            "\n",
            "Epoch 00002: loss improved from 6.41316 to 6.19985, saving model to weights-improvement-02-6.1998-bigger.hdf5\n",
            "Epoch 3/100\n",
            "25562/25562 [==============================] - 183s 7ms/step - loss: 6.1960\n",
            "\n",
            "Epoch 00003: loss improved from 6.19985 to 6.19595, saving model to weights-improvement-03-6.1960-bigger.hdf5\n",
            "Epoch 4/100\n",
            "25562/25562 [==============================] - 183s 7ms/step - loss: 6.1988\n",
            "\n",
            "Epoch 00004: loss did not improve from 6.19595\n",
            "Epoch 5/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.2020\n",
            "\n",
            "Epoch 00005: loss did not improve from 6.19595\n",
            "Epoch 6/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1969\n",
            "\n",
            "Epoch 00006: loss did not improve from 6.19595\n",
            "Epoch 7/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.2009\n",
            "\n",
            "Epoch 00007: loss did not improve from 6.19595\n",
            "Epoch 8/100\n",
            "25562/25562 [==============================] - 183s 7ms/step - loss: 6.1950\n",
            "\n",
            "Epoch 00008: loss improved from 6.19595 to 6.19496, saving model to weights-improvement-08-6.1950-bigger.hdf5\n",
            "Epoch 9/100\n",
            "25562/25562 [==============================] - 183s 7ms/step - loss: 6.1995\n",
            "\n",
            "Epoch 00009: loss did not improve from 6.19496\n",
            "Epoch 10/100\n",
            "25562/25562 [==============================] - 183s 7ms/step - loss: 6.1963\n",
            "\n",
            "Epoch 00010: loss did not improve from 6.19496\n",
            "Epoch 11/100\n",
            "25562/25562 [==============================] - 183s 7ms/step - loss: 6.1970\n",
            "\n",
            "Epoch 00011: loss did not improve from 6.19496\n",
            "Epoch 12/100\n",
            "25562/25562 [==============================] - 183s 7ms/step - loss: 6.1995\n",
            "\n",
            "Epoch 00012: loss did not improve from 6.19496\n",
            "Epoch 13/100\n",
            "25562/25562 [==============================] - 183s 7ms/step - loss: 6.1976\n",
            "\n",
            "Epoch 00013: loss did not improve from 6.19496\n",
            "Epoch 14/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1974\n",
            "\n",
            "Epoch 00014: loss did not improve from 6.19496\n",
            "Epoch 15/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1970\n",
            "\n",
            "Epoch 00015: loss did not improve from 6.19496\n",
            "Epoch 16/100\n",
            "25562/25562 [==============================] - 183s 7ms/step - loss: 6.2003\n",
            "\n",
            "Epoch 00016: loss did not improve from 6.19496\n",
            "Epoch 17/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1965\n",
            "\n",
            "Epoch 00017: loss did not improve from 6.19496\n",
            "Epoch 18/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1946\n",
            "\n",
            "Epoch 00018: loss improved from 6.19496 to 6.19459, saving model to weights-improvement-18-6.1946-bigger.hdf5\n",
            "Epoch 19/100\n",
            "25562/25562 [==============================] - 180s 7ms/step - loss: 6.1993\n",
            "\n",
            "Epoch 00019: loss did not improve from 6.19459\n",
            "Epoch 20/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1976\n",
            "\n",
            "Epoch 00020: loss did not improve from 6.19459\n",
            "Epoch 21/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.2004\n",
            "\n",
            "Epoch 00021: loss did not improve from 6.19459\n",
            "Epoch 22/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1973\n",
            "\n",
            "Epoch 00022: loss did not improve from 6.19459\n",
            "Epoch 23/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1958\n",
            "\n",
            "Epoch 00023: loss did not improve from 6.19459\n",
            "Epoch 24/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1952\n",
            "\n",
            "Epoch 00024: loss did not improve from 6.19459\n",
            "Epoch 25/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1979\n",
            "\n",
            "Epoch 00025: loss did not improve from 6.19459\n",
            "Epoch 26/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.2010\n",
            "\n",
            "Epoch 00026: loss did not improve from 6.19459\n",
            "Epoch 27/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1948\n",
            "\n",
            "Epoch 00027: loss did not improve from 6.19459\n",
            "Epoch 28/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1974\n",
            "\n",
            "Epoch 00028: loss did not improve from 6.19459\n",
            "Epoch 29/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.2001\n",
            "\n",
            "Epoch 00029: loss did not improve from 6.19459\n",
            "Epoch 30/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1990\n",
            "\n",
            "Epoch 00030: loss did not improve from 6.19459\n",
            "Epoch 31/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1960\n",
            "\n",
            "Epoch 00031: loss did not improve from 6.19459\n",
            "Epoch 32/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1984\n",
            "\n",
            "Epoch 00032: loss did not improve from 6.19459\n",
            "Epoch 33/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1987\n",
            "\n",
            "Epoch 00033: loss did not improve from 6.19459\n",
            "Epoch 34/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1981\n",
            "\n",
            "Epoch 00034: loss did not improve from 6.19459\n",
            "Epoch 35/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1979\n",
            "\n",
            "Epoch 00035: loss did not improve from 6.19459\n",
            "Epoch 36/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1983\n",
            "\n",
            "Epoch 00036: loss did not improve from 6.19459\n",
            "Epoch 37/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1936\n",
            "\n",
            "Epoch 00037: loss improved from 6.19459 to 6.19360, saving model to weights-improvement-37-6.1936-bigger.hdf5\n",
            "Epoch 38/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1951\n",
            "\n",
            "Epoch 00038: loss did not improve from 6.19360\n",
            "Epoch 39/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1959\n",
            "\n",
            "Epoch 00039: loss did not improve from 6.19360\n",
            "Epoch 40/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1930\n",
            "\n",
            "Epoch 00040: loss improved from 6.19360 to 6.19303, saving model to weights-improvement-40-6.1930-bigger.hdf5\n",
            "Epoch 41/100\n",
            "25562/25562 [==============================] - 179s 7ms/step - loss: 6.1926\n",
            "\n",
            "Epoch 00041: loss improved from 6.19303 to 6.19264, saving model to weights-improvement-41-6.1926-bigger.hdf5\n",
            "Epoch 42/100\n",
            "25562/25562 [==============================] - 179s 7ms/step - loss: 6.1922\n",
            "\n",
            "Epoch 00042: loss improved from 6.19264 to 6.19224, saving model to weights-improvement-42-6.1922-bigger.hdf5\n",
            "Epoch 43/100\n",
            "25562/25562 [==============================] - 180s 7ms/step - loss: 6.1939\n",
            "\n",
            "Epoch 00043: loss did not improve from 6.19224\n",
            "Epoch 44/100\n",
            "25562/25562 [==============================] - 180s 7ms/step - loss: 6.1943\n",
            "\n",
            "Epoch 00044: loss did not improve from 6.19224\n",
            "Epoch 45/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1928\n",
            "\n",
            "Epoch 00045: loss did not improve from 6.19224\n",
            "Epoch 46/100\n",
            "25562/25562 [==============================] - 180s 7ms/step - loss: 6.1926\n",
            "\n",
            "Epoch 00046: loss did not improve from 6.19224\n",
            "Epoch 47/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1929\n",
            "\n",
            "Epoch 00047: loss did not improve from 6.19224\n",
            "Epoch 48/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1896\n",
            "\n",
            "Epoch 00048: loss improved from 6.19224 to 6.18965, saving model to weights-improvement-48-6.1896-bigger.hdf5\n",
            "Epoch 49/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1937\n",
            "\n",
            "Epoch 00049: loss did not improve from 6.18965\n",
            "Epoch 50/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1937\n",
            "\n",
            "Epoch 00050: loss did not improve from 6.18965\n",
            "Epoch 51/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1906\n",
            "\n",
            "Epoch 00051: loss did not improve from 6.18965\n",
            "Epoch 52/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1920\n",
            "\n",
            "Epoch 00052: loss did not improve from 6.18965\n",
            "Epoch 53/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1930\n",
            "\n",
            "Epoch 00053: loss did not improve from 6.18965\n",
            "Epoch 54/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1941\n",
            "\n",
            "Epoch 00054: loss did not improve from 6.18965\n",
            "Epoch 55/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1936\n",
            "\n",
            "Epoch 00055: loss did not improve from 6.18965\n",
            "Epoch 56/100\n",
            "25562/25562 [==============================] - 180s 7ms/step - loss: 6.1918\n",
            "\n",
            "Epoch 00056: loss did not improve from 6.18965\n",
            "Epoch 57/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1953\n",
            "\n",
            "Epoch 00057: loss did not improve from 6.18965\n",
            "Epoch 58/100\n",
            "25562/25562 [==============================] - 180s 7ms/step - loss: 6.1948\n",
            "\n",
            "Epoch 00058: loss did not improve from 6.18965\n",
            "Epoch 59/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1953\n",
            "\n",
            "Epoch 00059: loss did not improve from 6.18965\n",
            "Epoch 60/100\n",
            "25562/25562 [==============================] - 180s 7ms/step - loss: 6.1921\n",
            "\n",
            "Epoch 00060: loss did not improve from 6.18965\n",
            "Epoch 61/100\n",
            "25562/25562 [==============================] - 180s 7ms/step - loss: 6.1956\n",
            "\n",
            "Epoch 00061: loss did not improve from 6.18965\n",
            "Epoch 62/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1927\n",
            "\n",
            "Epoch 00062: loss did not improve from 6.18965\n",
            "Epoch 63/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1939\n",
            "\n",
            "Epoch 00063: loss did not improve from 6.18965\n",
            "Epoch 64/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1902\n",
            "\n",
            "Epoch 00064: loss did not improve from 6.18965\n",
            "Epoch 65/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1917\n",
            "\n",
            "Epoch 00065: loss did not improve from 6.18965\n",
            "Epoch 66/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1929\n",
            "\n",
            "Epoch 00066: loss did not improve from 6.18965\n",
            "Epoch 67/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1938\n",
            "\n",
            "Epoch 00067: loss did not improve from 6.18965\n",
            "Epoch 68/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1917\n",
            "\n",
            "Epoch 00068: loss did not improve from 6.18965\n",
            "Epoch 69/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1913\n",
            "\n",
            "Epoch 00069: loss did not improve from 6.18965\n",
            "Epoch 70/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1917\n",
            "\n",
            "Epoch 00070: loss did not improve from 6.18965\n",
            "Epoch 71/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1929\n",
            "\n",
            "Epoch 00071: loss did not improve from 6.18965\n",
            "Epoch 72/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1926\n",
            "\n",
            "Epoch 00072: loss did not improve from 6.18965\n",
            "Epoch 73/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1913\n",
            "\n",
            "Epoch 00073: loss did not improve from 6.18965\n",
            "Epoch 74/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1926\n",
            "\n",
            "Epoch 00074: loss did not improve from 6.18965\n",
            "Epoch 75/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1933\n",
            "\n",
            "Epoch 00075: loss did not improve from 6.18965\n",
            "Epoch 76/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1930\n",
            "\n",
            "Epoch 00076: loss did not improve from 6.18965\n",
            "Epoch 77/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1931\n",
            "\n",
            "Epoch 00077: loss did not improve from 6.18965\n",
            "Epoch 78/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1940\n",
            "\n",
            "Epoch 00078: loss did not improve from 6.18965\n",
            "Epoch 79/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1905\n",
            "\n",
            "Epoch 00079: loss did not improve from 6.18965\n",
            "Epoch 80/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1951\n",
            "\n",
            "Epoch 00080: loss did not improve from 6.18965\n",
            "Epoch 81/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1930\n",
            "\n",
            "Epoch 00081: loss did not improve from 6.18965\n",
            "Epoch 82/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1906\n",
            "\n",
            "Epoch 00082: loss did not improve from 6.18965\n",
            "Epoch 83/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1948\n",
            "\n",
            "Epoch 00083: loss did not improve from 6.18965\n",
            "Epoch 84/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1917\n",
            "\n",
            "Epoch 00084: loss did not improve from 6.18965\n",
            "Epoch 85/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1943\n",
            "\n",
            "Epoch 00085: loss did not improve from 6.18965\n",
            "Epoch 86/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1916\n",
            "\n",
            "Epoch 00086: loss did not improve from 6.18965\n",
            "Epoch 87/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1938\n",
            "\n",
            "Epoch 00087: loss did not improve from 6.18965\n",
            "Epoch 88/100\n",
            "25562/25562 [==============================] - 181s 7ms/step - loss: 6.1921\n",
            "\n",
            "Epoch 00088: loss did not improve from 6.18965\n",
            "Epoch 89/100\n",
            "25562/25562 [==============================] - 180s 7ms/step - loss: 6.1943\n",
            "\n",
            "Epoch 00089: loss did not improve from 6.18965\n",
            "Epoch 90/100\n",
            "25562/25562 [==============================] - 180s 7ms/step - loss: 6.1927\n",
            "\n",
            "Epoch 00090: loss did not improve from 6.18965\n",
            "Epoch 91/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1910\n",
            "\n",
            "Epoch 00091: loss did not improve from 6.18965\n",
            "Epoch 92/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1944\n",
            "\n",
            "Epoch 00092: loss did not improve from 6.18965\n",
            "Epoch 93/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1915\n",
            "\n",
            "Epoch 00093: loss did not improve from 6.18965\n",
            "Epoch 94/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1943\n",
            "\n",
            "Epoch 00094: loss did not improve from 6.18965\n",
            "Epoch 95/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1927\n",
            "\n",
            "Epoch 00095: loss did not improve from 6.18965\n",
            "Epoch 96/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1922\n",
            "\n",
            "Epoch 00096: loss did not improve from 6.18965\n",
            "Epoch 97/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1924\n",
            "\n",
            "Epoch 00097: loss did not improve from 6.18965\n",
            "Epoch 98/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1923\n",
            "\n",
            "Epoch 00098: loss did not improve from 6.18965\n",
            "Epoch 99/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1951\n",
            "\n",
            "Epoch 00099: loss did not improve from 6.18965\n",
            "Epoch 100/100\n",
            "25562/25562 [==============================] - 182s 7ms/step - loss: 6.1939\n",
            "\n",
            "Epoch 00100: loss did not improve from 6.18965\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f69b99ba780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di0V070Ti0xr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "a76ab867-a259-4ba6-aa50-d9597f9b4213"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n",
            "weights-improvement-01-6.4132-bigger.hdf5\n",
            "weights-improvement-02-6.1998-bigger.hdf5\n",
            "weights-improvement-03-6.1960-bigger.hdf5\n",
            "weights-improvement-08-6.1950-bigger.hdf5\n",
            "weights-improvement-18-6.1946-bigger.hdf5\n",
            "weights-improvement-37-6.1936-bigger.hdf5\n",
            "weights-improvement-40-6.1930-bigger.hdf5\n",
            "weights-improvement-41-6.1926-bigger.hdf5\n",
            "weights-improvement-42-6.1922-bigger.hdf5\n",
            "weights-improvement-48-6.1896-bigger.hdf5\n",
            "wonderland.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1WwfOj9y6nA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_len=seq_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54ghUObbMSZp",
        "colab_type": "text"
      },
      "source": [
        "1. Predict 500 characters only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp4WytbYBTy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "\tresult = list()\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded],\n",
        "                            maxlen=seq_length,\n",
        "                            padding='pre',\n",
        "                            truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t#print(yhat.shape, yhat)\n",
        "    # map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\t#print(\"Index:\", index)\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\t\tif(len(in_text) >= 500):\n",
        "\t\t\t\tprint(\"completed 500 character\")\n",
        "\t\t\t\tbreak\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSUzckxcCGqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "e5187a10-a892-4489-cd9f-660929ed3887"
      },
      "source": [
        "print(tokens[0])\n",
        "# select a seed text\n",
        "seed_text = tokens[randint(0,len(tokens))]\n",
        "print(seed_text + '\\n')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           alices adventures in wonderland  lewis carroll  the millennium fulcrum edition 3\n",
            " now if you only kept on good terms with him hed do almost anything you liked with the clock\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9u2elsLBhbB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "00aa611a-f27b-48a8-9ffd-f98fe4c4e659"
      },
      "source": [
        "# generate new text\n",
        "generated = generate_seq(model, tokenizer, seq_length-1, seed_text, 500)\n",
        "print(generated)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed 500 character\n",
            "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}