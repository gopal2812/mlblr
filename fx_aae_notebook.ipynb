{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "fx_aae_notebook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopal2812/mlblr/blob/master/fx_aae_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HlGfk2GNUzye"
      },
      "source": [
        "# Data Exploration with Adversarial Autoencoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GWtC9DxiAfJY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "outputId": "d0aa4967-fefe-4f5f-aab1-9b293f968d60"
      },
      "source": [
        "#install additional dependencies if needed\n",
        "#I assume you have tensorflow, keras, numpy and pandas installed\n",
        "!pip install requests\n",
        "!pip install tables\n",
        "!pip install arrow\n",
        "!pip install ta\n",
        "!pip install tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.6/dist-packages (3.4.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from tables) (1.12.0)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.6/dist-packages (from tables) (2.6.9)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from tables) (1.16.4)\n",
            "Collecting arrow\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/c6/32df2c68e02e2d6b4457223fa499634edabb2d4ff74f00087ffff49b4be4/arrow-0.14.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from arrow) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->arrow) (1.12.0)\n",
            "Installing collected packages: arrow\n",
            "Successfully installed arrow-0.14.5\n",
            "Collecting ta\n",
            "  Downloading https://files.pythonhosted.org/packages/3e/d3/749d9c6975a699de8b661113cb835212176baf3b8714389e6f9f51a396da/ta-0.4.5.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ta) (1.16.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from ta) (0.24.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from ta) (0.21.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->ta) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->ta) (2.5.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->ta) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->ta) (0.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas->ta) (1.12.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.4.5-cp36-none-any.whl size=16698 sha256=fe213c99d9b412f13bc58b0d3a2c67e1b1341697e1b58a177184bbdc9916347d\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/ac/09/28855b628633d42061f670c59df6877a1dbdf70d04c985bc18\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.4.5\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YJDai8iT07ZZ",
        "outputId": "44d6e31f-aaf1-4829-8f5b-f54c2639b21c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import ta\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import arrow as time\n",
        "import requests as http\n",
        "from tqdm import tqdm\n",
        "\n",
        "from keras.layers.advanced_activations import PReLU\n",
        "from keras.optimizers import Nadam\n",
        "from keras.models import Model\n",
        "from keras.layers import *"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qd8ez9f-OiLN"
      },
      "source": [
        "## Loading the Data from Oanda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "syqusdQ_TZTu",
        "colab": {}
      },
      "source": [
        "class InstrumentLoader:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.store = pd.HDFStore('oanda_api_store.h5')\n",
        "        self.time_format = 'YYYY-MM-DDTHH:mm:ss.SSSSSSSSZ'\n",
        "        self.inst_base_url = 'https://api-fxpractice.oanda.com/v3/instruments/'\n",
        "\n",
        "    def load_period(self, instrument, granularity, start, end):\n",
        "        time_range = time.Arrow.range('day', start, end)\n",
        "\n",
        "        trading_days = [\n",
        "            day for day in time_range if day.format('d') not in ['6']\n",
        "        ]\n",
        "\n",
        "        signals = []\n",
        "        for i in tqdm(range(len(trading_days)), desc=\"downloading data\"):\n",
        "            day = trading_days[i]\n",
        "            values = self.load_into_df(day, instrument, granularity)\n",
        "            signals.append(values)\n",
        "        return signals\n",
        "\n",
        "    def load_into_df(self, day, instrument, granularity):\n",
        "        day_key = instrument + granularity + day.format('YYYYMMDD')\n",
        "        if day_key not in self.store:\n",
        "\n",
        "            values = self.load_day(day, instrument, granularity, \"M\")\n",
        "            \n",
        "            day_close = pd.Series([\n",
        "                float(candle['mid']['c']) for candle in values\n",
        "            ]).rename('close')\n",
        "            day_open = pd.Series([\n",
        "                float(candle['mid']['o']) for candle in values\n",
        "            ]).rename('open')\n",
        "            day_high = pd.Series([\n",
        "                float(candle['mid']['h']) for candle in values\n",
        "            ]).rename('high')\n",
        "            day_low = pd.Series([\n",
        "                float(candle['mid']['l']) for candle in values\n",
        "            ]).rename('low')\n",
        "            day_volume = pd.Series([\n",
        "                int(candle['volume']) for candle in values\n",
        "            ]).rename('volume')\n",
        "\n",
        "            time_of_day = pd.Series([\n",
        "                int(time.get(candle['time'], self.time_format).format('HHmmss'))\n",
        "                for candle in values\n",
        "            ]).rename('time_of_day')\n",
        "\n",
        "            signals = [\n",
        "                time_of_day, day_open, day_close, day_high, day_low, day_volume\n",
        "            ]\n",
        "\n",
        "            raw_day = pd.concat(signals, axis=1).set_index('time_of_day')\n",
        "\n",
        "            self.store[day_key] = raw_day\n",
        "            return raw_day\n",
        "        else:\n",
        "            raw_day = self.store[day_key]\n",
        "            return raw_day\n",
        "\n",
        "    def load_day(self, day, instrument, granularity, price):\n",
        "        time_format = 'YYYY-MM-DDTHH:mm:ssZ'\n",
        "        base_uri = self.inst_base_url + instrument + \"/candles\"\n",
        "        headers = {\"Authorization\": self.config['token']}\n",
        "        parameters = {\n",
        "            \"from\": day.format(time_format),\n",
        "            \"to\": day.shift(days=1).format(time_format),\n",
        "            \"price\": price,\n",
        "            \"granularity\": granularity,\n",
        "            \"includeFirst\": \"True\",\n",
        "        }\n",
        "        response = http.get(\n",
        "            base_uri, params=parameters, headers=headers).json()\n",
        "        print(response)\n",
        "        return response.get(\"candles\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W3UfSM4vJ9DL",
        "outputId": "916eae39-c466-43f4-c136-d4e5eb746978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        }
      },
      "source": [
        "config = {\n",
        "      'token': \"Bearer <YOUR API TOKEN HERE!>\",\n",
        "    }\n",
        "\n",
        "start = time.get(\"2018-01-02T00:00:00Z\", 'YYYY-MM-DDTHH:mm:ss')\n",
        "end = time.get(\"2018-12-30T00:00:00Z\", 'YYYY-MM-DDTHH:mm:ss')\n",
        "api = InstrumentLoader(config)\n",
        "\n",
        "days = api.load_period('EUR_USD', 'M5', start, end)\n",
        "\n",
        "test = days[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/arrow/factory.py:249: ArrowParseWarning: The .get() parsing method with a format string will parse more strictly in version 0.15.0.See https://github.com/crsmithdev/arrow/issues/612 for more details.\n",
            "  ArrowParseWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/arrow/factory.py:249: ArrowParseWarning: The .get() parsing method with a format string will parse more strictly in version 0.15.0.See https://github.com/crsmithdev/arrow/issues/612 for more details.\n",
            "  ArrowParseWarning,\n",
            "\rdownloading data:   0%|          | 0/311 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'errorMessage': 'Insufficient authorization to perform request.'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-dd0388ffc776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInstrumentLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_period\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EUR_USD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'M5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-bd81ac8f4d01>\u001b[0m in \u001b[0;36mload_period\u001b[0;34m(self, instrument, granularity, start, end)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrading_days\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"downloading data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mday\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_days\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_into_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstrument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0msignals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msignals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-bd81ac8f4d01>\u001b[0m in \u001b[0;36mload_into_df\u001b[0;34m(self, day, instrument, granularity)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             day_close = pd.Series([\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcandle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             ]).rename('close')\n\u001b[1;32m     31\u001b[0m             day_open = pd.Series([\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u3yI_Ul9FTp2"
      },
      "source": [
        "## Enhancing and preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pIiz7eUgY9o9",
        "colab": {}
      },
      "source": [
        "def roll_stats(data, window=32):\n",
        "    name = data.name\n",
        "    data_mean = data.rolling(window).mean().rename(name + '_mean')\n",
        "    data_var = data.rolling(window).var().rename(name + '_var')\n",
        "    data_skew = data.rolling(window).skew().rename(name + '_skew')\n",
        "    return pd.concat([data_mean, data_var, data_skew], axis=1)\n",
        "\n",
        "def log_returns(frame):\n",
        "    return np.log(frame) - np.log(frame.shift(1))\n",
        "\n",
        "enhanced_days = []\n",
        "\n",
        "for i in range(len(days)):\n",
        "    day = days[i]\n",
        "\n",
        "    high = day['high']\n",
        "    low = day['low']\n",
        "    close = day['close']\n",
        "    volume = day['volume']\n",
        "\n",
        "    close_returns = log_returns(close)\n",
        "    close_stats = roll_stats(close_returns)\n",
        "    rsi = ta.momentum.rsi(close, n=13, fillna=False).rename('rsi')\n",
        "    atr = ta.volatility.average_true_range(high, low, close, n=13, fillna=False).rename('atr')\n",
        "    macd_signal = ta.trend.macd_signal(close, n_fast=13, n_slow=35, n_sign=8, fillna=False).rename('macd_signal')\n",
        "\n",
        "    all_data = pd.concat([close_returns, log_returns(high), log_returns(low), close_stats, volume, rsi, atr, macd_signal], axis=1)\n",
        "    aligned_data = all_data.dropna()\n",
        "    enhanced_days.append(aligned_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rJNsTW7RZJIj",
        "colab": {}
      },
      "source": [
        "data_x = []\n",
        "\n",
        "window_size = 32\n",
        "step_size = 16\n",
        "\n",
        "all_days = pd.concat(enhanced_days)\n",
        "all_mean = all_days.mean()\n",
        "all_std = all_days.std()\n",
        "\n",
        "windowed_signals = []\n",
        "\n",
        "for i in range(len(enhanced_days)):\n",
        "    signal = enhanced_days[i]\n",
        "\n",
        "    if len(signal) >= window_size:\n",
        "        for j in range(0, len(signal) - window_size, step_size):\n",
        "            window_x = signal.iloc[j:j + window_size]\n",
        "            windowed_signals.append(window_x)\n",
        "            window_x = (window_x - window_x.mean()) / window_x.std()\n",
        "            data_x.append(window_x.values)\n",
        "    \n",
        "\n",
        "data_x = np.array(data_x)\n",
        "\n",
        "split_train = int(len(data_x) * 0.7)\n",
        "\n",
        "train_x = data_x[:split_train]\n",
        "originals_x = windowed_signals[:split_train]\n",
        "\n",
        "test_x = data_x[split_train:]\n",
        "\n",
        "print(train_x.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GtLo3FmgeYGf",
        "colab": {}
      },
      "source": [
        "def sample_normal(latent_dim, batch_size, window_size=None):\n",
        "    shape = (batch_size, latent_dim) if window_size is None else (batch_size, window_size, latent_dim)\n",
        "    return np.random.normal(size=shape)\n",
        "  \n",
        "def sample_categories(cat_dim, batch_size):\n",
        "    cats = np.zeros((batch_size, cat_dim))\n",
        "    for i in range(batch_size):\n",
        "        one = np.random.randint(0, cat_dim)\n",
        "        cats[i][one] = 1\n",
        "    return cats\n",
        "  \n",
        "def create_encoder(latent_dim, cat_dim, window_size, input_dim):\n",
        "    input_layer = Input(shape=(window_size, input_dim))\n",
        "    \n",
        "    code = TimeDistributed(Dense(64, activation='linear'))(input_layer)\n",
        "    code = Bidirectional(LSTM(128, return_sequences=True))(code)\n",
        "    code = BatchNormalization()(code)\n",
        "    code = ELU()(code)\n",
        "    code = Bidirectional(LSTM(64))(code)\n",
        "    code = BatchNormalization()(code)\n",
        "    code = ELU()(code)\n",
        "    \n",
        "    cat = Dense(64)(code)\n",
        "    cat = BatchNormalization()(cat)\n",
        "    cat = PReLU()(cat)\n",
        "    cat = Dense(cat_dim, activation='softmax')(cat)\n",
        "    \n",
        "    latent_repr = Dense(64)(code)\n",
        "    latent_repr = BatchNormalization()(latent_repr)\n",
        "    latent_repr = PReLU()(latent_repr)\n",
        "    latent_repr = Dense(latent_dim, activation='linear')(latent_repr)\n",
        "    \n",
        "    decode = Concatenate()([latent_repr, cat])\n",
        "    decode = RepeatVector(window_size)(decode)\n",
        "    decode = Bidirectional(LSTM(64, return_sequences=True))(decode)\n",
        "    decode = ELU()(decode)\n",
        "    decode = Bidirectional(LSTM(128, return_sequences=True))(decode)\n",
        "    decode = ELU()(decode)\n",
        "    decode = TimeDistributed(Dense(64))(decode)\n",
        "    decode = ELU()(decode)\n",
        "    decode = TimeDistributed(Dense(input_dim, activation='linear'))(decode)\n",
        "    \n",
        "    error = Subtract()([input_layer, decode])\n",
        "        \n",
        "    return Model(input_layer, [decode, latent_repr, cat, error])\n",
        "\n",
        "  \n",
        "def create_discriminator(latent_dim):\n",
        "    input_layer = Input(shape=(latent_dim,))\n",
        "    disc = Dense(128)(input_layer)\n",
        "    disc = ELU()(disc) #LeakyReLU(alpha=0.2)(disc)\n",
        "    disc = Dense(64)(disc)\n",
        "    disc = ELU()(disc) #LeakyReLU(alpha=0.2)(disc)\n",
        "    disc = Dense(1, activation=\"sigmoid\")(disc)\n",
        "    \n",
        "    model = Model(input_layer, disc)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xYdQKItBhcbG"
      },
      "source": [
        "## Encoding Data to find Clusters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bbD3wNhkhYaU",
        "colab": {}
      },
      "source": [
        "window_size = train_x.shape[1]\n",
        "input_dim = train_x.shape[2]\n",
        "latent_dim = 32\n",
        "cat_dim = 8\n",
        "\n",
        "prior_discriminator = create_discriminator(latent_dim)\n",
        "prior_discriminator.compile(loss='binary_crossentropy', \n",
        "                            optimizer=Nadam(0.0002, 0.5), \n",
        "                            metrics=['accuracy'])\n",
        "\n",
        "prior_discriminator.trainable = False\n",
        "\n",
        "cat_discriminator = create_discriminator(cat_dim)\n",
        "cat_discriminator.compile(loss='binary_crossentropy', \n",
        "                          optimizer=Nadam(0.0002, 0.5), \n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "cat_discriminator.trainable = False\n",
        "\n",
        "encoder = create_encoder(latent_dim, cat_dim, window_size, input_dim)\n",
        "\n",
        "signal_in = Input(shape=(window_size, input_dim))\n",
        "reconstructed_signal, encoded_repr, category, _ = encoder(signal_in)\n",
        "\n",
        "is_real_prior = prior_discriminator(encoded_repr)\n",
        "is_real_cat = cat_discriminator(category)\n",
        "\n",
        "autoencoder = Model(signal_in, [reconstructed_signal, is_real_prior, is_real_cat])\n",
        "autoencoder.compile(loss=['mse', 'binary_crossentropy', 'binary_crossentropy'],\n",
        "                                loss_weights=[0.99, 0.005, 0.005],\n",
        "                                optimizer=Nadam(0.0002, 0.5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ljwfe7b6TTig",
        "colab": {}
      },
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O-qaYIKahpxU",
        "colab": {}
      },
      "source": [
        "batches = 10000\n",
        "batch_size=64\n",
        "\n",
        "losses_disc = []\n",
        "losses_disc_cat = []\n",
        "losses_ae = []\n",
        "losses_val = []\n",
        "\n",
        "real = np.ones((batch_size, 1))\n",
        "fake = np.zeros((batch_size, 1))\n",
        "\n",
        "def discriminator_training(discriminator, real, fake):\n",
        "    def train(real_samples, fake_samples):\n",
        "        discriminator.trainable = True\n",
        "\n",
        "        loss_real = discriminator.train_on_batch(real_samples, real)\n",
        "        loss_fake = discriminator.train_on_batch(fake_samples, fake)\n",
        "        loss = np.add(loss_real, loss_fake) * 0.5\n",
        "\n",
        "        discriminator.trainable = False\n",
        "\n",
        "        return loss\n",
        "    return train\n",
        "\n",
        "train_prior_discriminator = discriminator_training(prior_discriminator, real, fake)\n",
        "train_cat_discriminator = discriminator_training(cat_discriminator, real, fake)\n",
        "\n",
        "pbar = tqdm(range(batches))\n",
        "\n",
        "for _ in pbar:\n",
        "  \n",
        "    ids = np.random.randint(0, train_x.shape[0], batch_size)\n",
        "    signals = train_x[ids]\n",
        "\n",
        "    _, latent_fake, category_fake, _ = encoder.predict(signals)\n",
        "\n",
        "    latent_real = sample_normal(latent_dim, batch_size)\n",
        "    category_real = sample_categories(cat_dim, batch_size)\n",
        "\n",
        "    prior_loss = train_prior_discriminator(latent_real, latent_fake)\n",
        "    cat_loss = train_cat_discriminator(category_real, category_fake)\n",
        "\n",
        "    losses_disc.append(prior_loss)\n",
        "    losses_disc_cat.append(cat_loss)\n",
        "\n",
        "    encoder_loss = autoencoder.train_on_batch(signals, [signals, real, real])\n",
        "    losses_ae.append(encoder_loss)\n",
        "\n",
        "    val_loss = autoencoder.test_on_batch(signals, [signals, real, real])\n",
        "    losses_val.append(val_loss)\n",
        "\n",
        "    pbar.set_description(\"[Acc. Prior/Cat: %.2f%% / %.2f%%] [MSE train/val: %f / %f]\" \n",
        "            % (100*prior_loss[1], 100*cat_loss[1], encoder_loss[1], val_loss[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FTc-3lrmWY3_",
        "colab": {}
      },
      "source": [
        "autoencoder.save_weights('aae.hdf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XAbmtQ5JlTXc"
      },
      "source": [
        "## Show Loss and Result Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vgD0vTjqhwi9",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=3)\n",
        "fig.set_size_inches(30, 6)\n",
        "\n",
        "axes[0].plot([loss[1] for loss in losses_disc])\n",
        "axes[1].plot([loss[1] for loss in losses_disc_cat])\n",
        "axes[2].plot([loss[1] for loss in losses_ae])\n",
        "axes[2].plot([loss[1] for loss in losses_val])\n",
        "\n",
        "fig.show()\n",
        "\n",
        "size = 5\n",
        "offset = 5\n",
        "\n",
        "fig, axes = plt.subplots(nrows=size, ncols=5)\n",
        "fig.set_size_inches(20, 3 * size)\n",
        "\n",
        "(dec, rep, cat, error) = encoder.predict(test_x[offset:size+offset])\n",
        "\n",
        "for i in range(size):  \n",
        "    axes[i,0].plot(test_x[i+offset])\n",
        "    axes[i,1].imshow(rep[i].reshape(8,4))\n",
        "    axes[i,2].imshow(cat[i].reshape(cat_dim, 1))\n",
        "    axes[i,3].plot(dec[i])\n",
        "    axes[i,4].plot(error[i])\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "67J6j5B9_C0f"
      },
      "source": [
        "## Inspect Categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w1bQ9LiwozJp",
        "colab": {}
      },
      "source": [
        "(dec, rep, cat, error) = encoder.predict(data_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vD4v_pDlo1dy",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "categories = [np.argmax(item) for item in cat]\n",
        "counts = Counter(categories)\n",
        "print(counts)\n",
        "\n",
        "labels, count = zip(*counts.items())\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.bar(labels, count)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uBYzN5-Xo5ar"
      },
      "source": [
        "## Latent Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1zdvcq-fo48T",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3)\n",
        "fig.set_size_inches(30,7)\n",
        "\n",
        "X = TSNE(n_components=2, perplexity=10).fit_transform(rep)\n",
        "\n",
        "axes[0].scatter([x[0] for x in X], [x[1] for x in X], c=categories, cmap='viridis')\n",
        "\n",
        "X = TSNE(n_components=2, perplexity=30).fit_transform(rep)\n",
        "\n",
        "axes[1].scatter([x[0] for x in X], [x[1] for x in X], c=categories, cmap='viridis')\n",
        "\n",
        "X = TSNE(n_components=2, perplexity=50).fit_transform(rep)\n",
        "\n",
        "axes[2].scatter([x[0] for x in X], [x[1] for x in X], c=categories, cmap='viridis')\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JC2LSJbzpCrT"
      },
      "source": [
        "## Categories in the Input Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fuj8zjvxpGIN",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3)\n",
        "fig.set_size_inches(30,7)\n",
        "\n",
        "input_data = data_x.reshape(data_x.shape[0], data_x.shape[1] * data_x.shape[2])\n",
        "\n",
        "X = TSNE(n_components=2, perplexity=10).fit_transform(input_data)\n",
        "\n",
        "axes[0].scatter([x[0] for x in X], [x[1] for x in X], c=categories, cmap='viridis')\n",
        "\n",
        "X = TSNE(n_components=2, perplexity=30).fit_transform(input_data)\n",
        "\n",
        "axes[1].scatter([x[0] for x in X], [x[1] for x in X], c=categories, cmap='viridis')\n",
        "\n",
        "X = TSNE(n_components=2, perplexity=50).fit_transform(input_data)\n",
        "\n",
        "axes[2].scatter([x[0] for x in X], [x[1] for x in X], c=categories, cmap='viridis')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1UQkYAmWLNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}