{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled34.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopal2812/mlblr/blob/master/lstmassignment6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk3hWKZrWyID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff19df1b-a99d-41f6-b581-4709935338a5"
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import string\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from random import randint\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTHrbpjl58m6",
        "colab_type": "code",
        "outputId": "01869510-4745-4e50-9c4a-4bf3c7958c7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHUSrF01aABw",
        "colab_type": "code",
        "outputId": "f6271c5c-d71c-43ea-dfeb-398fcd981deb",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-932d323b-dbac-4053-b1ee-63327f3b28b6\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-932d323b-dbac-4053-b1ee-63327f3b28b6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving wonderland.txt to wonderland.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iRaOkbjd6KY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkgMn6T4LvyK",
        "colab_type": "text"
      },
      "source": [
        "2. Remove all the punctuation from the source text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgDjjWb7c1PX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# replace '--' with a space ' '\n",
        "\tdoc = doc.replace('--', ' ')\n",
        "  \n",
        " \t# replace '\\n' with a space ' '\n",
        "\t#doc = doc.replace('\\n', ' ')\n",
        "  \n",
        "\t# split into tokens by full stop\n",
        "\ttokens = doc.split('\\n')\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# make lower case\n",
        "\ttokens = [word.lower() for word in tokens]\n",
        "\t# using join() +  split() to \n",
        "\t# perform removal of empty list\n",
        "\tfor word in tokens:\n",
        "\t\tword = ' '.join(word).split()\n",
        "\ttokens = list(filter(None, tokens))\n",
        "\tprint(type(tokens))\n",
        "\treturn tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yaw3vLRqdRgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWTUX4yHdTFW",
        "colab_type": "code",
        "outputId": "414394fa-6578-4fbf-b7f5-bbd5773d0608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "in_filename = \"wonderland.txt\"\n",
        "doc = load_doc(in_filename)\n",
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "['alices adventures in wonderland', 'lewis carroll', 'the millennium fulcrum edition 30', 'chapter i down the rabbithole', 'alice was beginning to get very tired of sitting by her sister on the', 'bank and of having nothing to do once or twice she had peeped into the', 'book her sister was reading but it had no pictures or conversations in', 'it and what is the use of a book thought alice without pictures or', 'conversations', 'so she was considering in her own mind as well as she could for the', 'hot day made her feel very sleepy and stupid whether the pleasure', 'of making a daisychain would be worth the trouble of getting up and', 'picking the daisies when suddenly a white rabbit with pink eyes ran', 'close by her', 'there was nothing so very remarkable in that nor did alice think it so', 'very much out of the way to hear the rabbit say to itself oh dear', 'oh dear i shall be late when she thought it over afterwards it', 'occurred to her that she ought to have wondered at this but at the time', 'it all seemed quite natural but when the rabbit actually took a watch', 'out of its waistcoatpocket and looked at it and then hurried on', 'alice started to her feet for it flashed across her mind that she had', 'never before seen a rabbit with either a waistcoatpocket or a watch', 'to take out of it and burning with curiosity she ran across the field', 'after it and fortunately was just in time to see it pop down a large', 'rabbithole under the hedge', 'in another moment down went alice after it never once considering how', 'in the world she was to get out again', 'the rabbithole went straight on like a tunnel for some way and then', 'dipped suddenly down so suddenly that alice had not a moment to think', 'about stopping herself before she found herself falling down a very deep', 'well', 'either the well was very deep or she fell very slowly for she had', 'plenty of time as she went down to look about her and to wonder what was', 'going to happen next first she tried to look down and make out what', 'she was coming to but it was too dark to see anything then she', 'looked at the sides of the well and noticed that they were filled with', 'cupboards and bookshelves here and there she saw maps and pictures', 'hung upon pegs she took down a jar from one of the shelves as', 'she passed it was labelled orange marmalade but to her great', 'disappointment it was empty she did not like to drop the jar for fear', 'of killing somebody so managed to put it into one of the cupboards as', 'she fell past it', 'well thought alice to herself after such a fall as this i shall', 'think nothing of tumbling down stairs how brave theyll all think me at', 'home why i wouldnt say anything about it even if i fell off the top', 'of the house which was very likely true', 'down down down would the fall never come to an end i wonder how', 'many miles ive fallen by this time she said aloud i must be getting', 'somewhere near the centre of the earth let me see that would be four', 'thousand miles down i think  for you see alice had learnt several', 'things of this sort in her lessons in the schoolroom and though this', 'was not a very good opportunity for showing off her knowledge as there', 'was no one to listen to her still it was good practice to say it over', ' yes thats about the right distance but then i wonder what latitude', 'or longitude ive got to alice had no idea what latitude was or', 'longitude either but thought they were nice grand words to say', 'presently she began again i wonder if i shall fall right through the', 'earth how funny itll seem to come out among the people that walk with', 'their heads downward the antipathies i think  she was rather glad', 'there was no one listening this time as it didnt sound at all the', 'right word  but i shall have to ask them what the name of the country', 'is you know please maam is this new zealand or australia and', 'she tried to curtsey as she spoke fancy curtseying as youre falling', 'through the air do you think you could manage it and what an', 'ignorant little girl shell think me for asking no itll never do to', 'ask perhaps i shall see it written up somewhere', 'down down down there was nothing else to do so alice soon began', 'talking again dinahll miss me very much tonight i should think', 'dinah was the cat i hope theyll remember her saucer of milk at', 'teatime dinah my dear i wish you were down here with me there are no', 'mice in the air im afraid but you might catch a bat and thats very', 'like a mouse you know but do cats eat bats i wonder and here alice', 'began to get rather sleepy and went on saying to herself in a dreamy', 'sort of way do cats eat bats do cats eat bats and sometimes do', 'bats eat cats for you see as she couldnt answer either question', 'it didnt much matter which way she put it she felt that she was dozing', 'off and had just begun to dream that she was walking hand in hand with', 'dinah and saying to her very earnestly now dinah tell me the truth', 'did you ever eat a bat when suddenly thump thump down she came upon', 'a heap of sticks and dry leaves and the fall was over', 'alice was not a bit hurt and she jumped up on to her feet in a moment', 'she looked up but it was all dark overhead before her was another', 'long passage and the white rabbit was still in sight hurrying down it', 'there was not a moment to be lost away went alice like the wind and', 'was just in time to hear it say as it turned a corner oh my ears', 'and whiskers how late its getting she was close behind it when she', 'turned the corner but the rabbit was no longer to be seen she found', 'herself in a long low hall which was lit up by a row of lamps hanging', 'from the roof', 'there were doors all round the hall but they were all locked and when', 'alice had been all the way down one side and up the other trying every', 'door she walked sadly down the middle wondering how she was ever to', 'get out again', 'suddenly she came upon a little threelegged table all made of solid', 'glass there was nothing on it except a tiny golden key and alices', 'first thought was that it might belong to one of the doors of the hall', 'but alas either the locks were too large or the key was too small', 'but at any rate it would not open any of them however on the second', 'time round she came upon a low curtain she had not noticed before and', 'behind it was a little door about fifteen inches high she tried the', 'little golden key in the lock and to her great delight it fitted', 'alice opened the door and found that it led into a small passage not', 'much larger than a rathole she knelt down and looked along the passage', 'into the loveliest garden you ever saw how she longed to get out of', 'that dark hall and wander about among those beds of bright flowers and', 'those cool fountains but she could not even get her head through the', 'doorway and even if my head would go through thought poor alice it', 'would be of very little use without my shoulders oh how i wish i could', 'shut up like a telescope i think i could if i only knew how to begin', 'for you see so many outoftheway things had happened lately', 'that alice had begun to think that very few things indeed were really', 'impossible', 'there seemed to be no use in waiting by the little door so she went', 'back to the table half hoping she might find another key on it or at', 'any rate a book of rules for shutting people up like telescopes this', 'time she found a little bottle on it which certainly was not here', 'before said alice and round the neck of the bottle was a paper', 'label with the words drink me beautifully printed on it in large', 'letters', 'it was all very well to say drink me but the wise little alice was', 'not going to do that in a hurry no ill look first she said and', 'see whether its marked poison or not for she had read several nice', 'little histories about children who had got burnt and eaten up by wild', 'beasts and other unpleasant things all because they would not remember', 'the simple rules their friends had taught them such as that a redhot', 'poker will burn you if you hold it too long and that if you cut your', 'finger very deeply with a knife it usually bleeds and she had never', 'forgotten that if you drink much from a bottle marked poison it is', 'almost certain to disagree with you sooner or later', 'however this bottle was not marked poison so alice ventured to taste', 'it and finding it very nice it had in fact a sort of mixed flavour', 'of cherrytart custard pineapple roast turkey toffee and hot', 'buttered toast she very soon finished it off', '                          ', '                        ', '                          ', 'what a curious feeling said alice i must be shutting up like a', 'telescope', 'and so it was indeed she was now only ten inches high and her face', 'brightened up at the thought that she was now the right size for going', 'through the little door into that lovely garden first however she', 'waited for a few minutes to see if she was going to shrink any further', 'she felt a little nervous about this for it might end you know said', 'alice to herself in my going out altogether like a candle i wonder', 'what i should be like then and she tried to fancy what the flame of a', 'candle is like after the candle is blown out for she could not remember', 'ever having seen such a thing', 'after a while finding that nothing more happened she decided on going', 'into the garden at once but alas for poor alice when she got to the', 'door she found she had forgotten the little golden key and when she', 'went back to the table for it she found she could not possibly reach', 'it she could see it quite plainly through the glass and she tried her', 'best to climb up one of the legs of the table but it was too slippery', 'and when she had tired herself out with trying the poor little thing', 'sat down and cried', 'come theres no use in crying like that said alice to herself', 'rather sharply i advise you to leave off this minute she generally', 'gave herself very good advice though she very seldom followed it', 'and sometimes she scolded herself so severely as to bring tears into', 'her eyes and once she remembered trying to box her own ears for having', 'cheated herself in a game of croquet she was playing against herself', 'for this curious child was very fond of pretending to be two people', 'but its no use now thought poor alice to pretend to be two people', 'why theres hardly enough of me left to make one respectable person', 'soon her eye fell on a little glass box that was lying under the table', 'she opened it and found in it a very small cake on which the words', 'eat me were beautifully marked in currants well ill eat it said', 'alice and if it makes me grow larger i can reach the key and if it', 'makes me grow smaller i can creep under the door so either way ill', 'get into the garden and i dont care which happens', 'she ate a little bit and said anxiously to herself which way which', 'way holding her hand on the top of her head to feel which way it was', 'growing and she was quite surprised to find that she remained the same', 'size to be sure this generally happens when one eats cake but alice', 'had got so much into the way of expecting nothing but outoftheway', 'things to happen that it seemed quite dull and stupid for life to go on', 'in the common way', 'so she set to work and very soon finished off the cake', '                          ', '                        ', '                          ', 'chapter ii the pool of tears', 'curiouser and curiouser cried alice she was so much surprised that', 'for the moment she quite forgot how to speak good english now im', 'opening out like the largest telescope that ever was goodbye feet', 'for when she looked down at her feet they seemed to be almost out of', 'sight they were getting so far off oh my poor little feet i wonder', 'who will put on your shoes and stockings for you now dears im sure', 'i shant be able i shall be a great deal too far off to trouble', 'myself about you you must manage the best way you can but i must be', 'kind to them thought alice or perhaps they wont walk the way i want', 'to go let me see ill give them a new pair of boots every christmas', 'and she went on planning to herself how she would manage it they must', 'go by the carrier she thought and how funny itll seem sending', 'presents to ones own feet and how odd the directions will look', '     alices right foot esq', '       hearthrug', '         near the fender', '           with alices love', 'oh dear what nonsense im talking']\n",
            "Total Tokens: 2480\n",
            "Unique Tokens: 2454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h32uQEgeL8FA",
        "colab_type": "text"
      },
      "source": [
        "3. Train the model on padded sequences (Links to an external site.) rather than random sequences of characters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqGFfvKTqqNz",
        "colab_type": "code",
        "outputId": "33e03804-164e-469a-c35f-f5c9fe7822ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "#We would be taking a compromise so we save a lot of memory and have to truncate maybe 5% of all the sequences. \n",
        "#Sequences that are too long should be truncated at the end\n",
        "num_lines = [len(line) for line in tokens]\n",
        "std_line = np.std(num_lines)\n",
        "mean_line = np.mean(num_lines)\n",
        "seq_len= (int)((mean_line+2*std_line))\n",
        "\n",
        "print(mean_line, std_line, seq_len, np.median(num_lines))\n",
        "\n",
        "#also find the most frequent length\n",
        "(values,counts) = np.unique(num_lines,return_counts=True)\n",
        "ind=np.argmax(counts)\n",
        "print(values[ind])\n",
        "\n",
        "seq_length=100\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "\n",
        "def get_sequence_of_tokens(tokens):\n",
        "    ## tokenization\n",
        "    tokenizer.fit_on_texts(tokens)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    \n",
        "    ## convert data to sequence of tokens \n",
        "    input_sequences = []\n",
        "    for line in tokens:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    return input_sequences, vocab_size\n",
        "\n",
        "sequences, vocab_size = get_sequence_of_tokens(tokens)\n",
        "print(sequences[:200])\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "53.63508064516129 18.977711332319586 91 64.0\n",
            "67\n",
            "[[303, 467], [303, 467, 11], [303, 467, 11, 843], [1481, 1482], [1, 1483], [1, 1483, 1484], [1, 1483, 1484, 1485], [1, 1483, 1484, 1485, 1486], [304, 9], [304, 9, 37], [304, 9, 37, 1], [304, 9, 37, 1, 689], [10, 13], [10, 13, 251], [10, 13, 251, 3], [10, 13, 251, 3, 101], [10, 13, 251, 3, 101, 27], [10, 13, 251, 3, 101, 27, 468], [10, 13, 251, 3, 101, 27, 468, 7], [10, 13, 251, 3, 101, 27, 468, 7, 346], [10, 13, 251, 3, 101, 27, 468, 7, 346, 76], [10, 13, 251, 3, 101, 27, 468, 7, 346, 76, 16], [10, 13, 251, 3, 101, 27, 468, 7, 346, 76, 16, 415], [10, 13, 251, 3, 101, 27, 468, 7, 346, 76, 16, 415, 18], [10, 13, 251, 3, 101, 27, 468, 7, 346, 76, 16, 415, 18, 1], [844, 2], [844, 2, 7], [844, 2, 7, 347], [844, 2, 7, 347, 129], [844, 2, 7, 347, 129, 3], [844, 2, 7, 347, 129, 3, 52], [844, 2, 7, 347, 129, 3, 52, 130], [844, 2, 7, 347, 129, 3, 52, 130, 55], [844, 2, 7, 347, 129, 3, 52, 130, 55, 589], [844, 2, 7, 347, 129, 3, 52, 130, 55, 589, 5], [844, 2, 7, 347, 129, 3, 52, 130, 55, 589, 5, 21], [844, 2, 7, 347, 129, 3, 52, 130, 55, 589, 5, 21, 845], [844, 2, 7, 347, 129, 3, 52, 130, 55, 589, 5, 21, 845, 64], [844, 2, 7, 347, 129, 3, 52, 130, 55, 589, 5, 21, 845, 64, 1], [469, 16], [469, 16, 415], [469, 16, 415, 13], [469, 16, 415, 13, 846], [469, 16, 415, 13, 846, 22], [469, 16, 415, 13, 846, 22, 6], [469, 16, 415, 13, 846, 22, 6, 21], [469, 16, 415, 13, 846, 22, 6, 21, 43], [469, 16, 415, 13, 846, 22, 6, 21, 43, 690], [469, 16, 415, 13, 846, 22, 6, 21, 43, 690, 55], [469, 16, 415, 13, 846, 22, 6, 21, 43, 690, 55, 1073], [469, 16, 415, 13, 846, 22, 6, 21, 43, 690, 55, 1073, 11], [6, 2], [6, 2, 28], [6, 2, 28, 35], [6, 2, 28, 35, 1], [6, 2, 28, 35, 1, 209], [6, 2, 28, 35, 1, 209, 7], [6, 2, 28, 35, 1, 209, 7, 4], [6, 2, 28, 35, 1, 209, 7, 4, 469], [6, 2, 28, 35, 1, 209, 7, 4, 469, 58], [6, 2, 28, 35, 1, 209, 7, 4, 469, 58, 10], [6, 2, 28, 35, 1, 209, 7, 4, 469, 58, 10, 165], [6, 2, 28, 35, 1, 209, 7, 4, 469, 58, 10, 165, 690], [6, 2, 28, 35, 1, 209, 7, 4, 469, 58, 10, 165, 690, 55], [24, 5], [24, 5, 13], [24, 5, 13, 847], [24, 5, 13, 847, 11], [24, 5, 13, 847, 11, 16], [24, 5, 13, 847, 11, 16, 348], [24, 5, 13, 847, 11, 16, 348, 322], [24, 5, 13, 847, 11, 16, 348, 322, 15], [24, 5, 13, 847, 11, 16, 348, 322, 15, 71], [24, 5, 13, 847, 11, 16, 348, 322, 15, 71, 15], [24, 5, 13, 847, 11, 16, 348, 322, 15, 71, 15, 5], [24, 5, 13, 847, 11, 16, 348, 322, 15, 71, 15, 5, 56], [24, 5, 13, 847, 11, 16, 348, 322, 15, 71, 15, 5, 56, 23], [24, 5, 13, 847, 11, 16, 348, 322, 15, 71, 15, 5, 56, 23, 1], [590, 169], [590, 169, 144], [590, 169, 144, 16], [590, 169, 144, 16, 416], [590, 169, 144, 16, 416, 27], [590, 169, 144, 16, 416, 27, 591], [590, 169, 144, 16, 416, 27, 591, 2], [590, 169, 144, 16, 416, 27, 591, 2, 532], [590, 169, 144, 16, 416, 27, 591, 2, 532, 323], [590, 169, 144, 16, 416, 27, 591, 2, 532, 323, 1], [590, 169, 144, 16, 416, 27, 591, 2, 532, 323, 1, 1074], [7, 417], [7, 417, 4], [7, 417, 4, 1487], [7, 417, 4, 1487, 48], [7, 417, 4, 1487, 48, 25], [7, 417, 4, 1487, 48, 25, 691], [7, 417, 4, 1487, 48, 25, 691, 1], [7, 417, 4, 1487, 48, 25, 691, 1, 533], [7, 417, 4, 1487, 48, 25, 691, 1, 533, 7], [7, 417, 4, 1487, 48, 25, 691, 1, 533, 7, 187], [7, 417, 4, 1487, 48, 25, 691, 1, 533, 7, 187, 38], [7, 417, 4, 1487, 48, 25, 691, 1, 533, 7, 187, 38, 2], [1075, 1], [1075, 1, 1488], [1075, 1, 1488, 54], [1075, 1, 1488, 54, 274], [1075, 1, 1488, 54, 274, 4], [1075, 1, 1488, 54, 274, 4, 145], [1075, 1, 1488, 54, 274, 4, 145, 107], [1075, 1, 1488, 54, 274, 4, 145, 107, 20], [1075, 1, 1488, 54, 274, 4, 145, 107, 20, 1489], [1075, 1, 1488, 54, 274, 4, 145, 107, 20, 1489, 152], [1075, 1, 1488, 54, 274, 4, 145, 107, 20, 1489, 152, 228], [275, 76], [275, 76, 16], [57, 13], [57, 13, 129], [57, 13, 129, 24], [57, 13, 129, 24, 27], [57, 13, 129, 24, 27, 1076], [57, 13, 129, 24, 27, 1076, 11], [57, 13, 129, 24, 27, 1076, 11, 14], [57, 13, 129, 24, 27, 1076, 11, 14, 848], [57, 13, 129, 24, 27, 1076, 11, 14, 848, 66], [57, 13, 129, 24, 27, 1076, 11, 14, 848, 66, 10], [57, 13, 129, 24, 27, 1076, 11, 14, 848, 66, 10, 84], [57, 13, 129, 24, 27, 1076, 11, 14, 848, 66, 10, 84, 6], [57, 13, 129, 24, 27, 1076, 11, 14, 848, 66, 10, 84, 6, 24], [27, 88], [27, 88, 34], [27, 88, 34, 7], [27, 88, 34, 7, 1], [27, 88, 34, 7, 1, 85], [27, 88, 34, 7, 1, 85, 3], [27, 88, 34, 7, 1, 85, 3, 252], [27, 88, 34, 7, 1, 85, 3, 252, 1], [27, 88, 34, 7, 1, 85, 3, 252, 1, 107], [27, 88, 34, 7, 1, 85, 3, 252, 1, 107, 89], [27, 88, 34, 7, 1, 85, 3, 252, 1, 107, 89, 3], [27, 88, 34, 7, 1, 85, 3, 252, 1, 107, 89, 3, 253], [27, 88, 34, 7, 1, 85, 3, 252, 1, 107, 89, 3, 253, 103], [27, 88, 34, 7, 1, 85, 3, 252, 1, 107, 89, 3, 253, 103, 153], [103, 153], [103, 153, 9], [103, 153, 9, 170], [103, 153, 9, 170, 25], [103, 153, 9, 170, 25, 534], [103, 153, 9, 170, 25, 534, 54], [103, 153, 9, 170, 25, 534, 54, 5], [103, 153, 9, 170, 25, 534, 54, 5, 58], [103, 153, 9, 170, 25, 534, 54, 5, 58, 6], [103, 153, 9, 170, 25, 534, 54, 5, 58, 6, 113], [103, 153, 9, 170, 25, 534, 54, 5, 58, 6, 113, 1077], [103, 153, 9, 170, 25, 534, 54, 5, 58, 6, 113, 1077, 6], [1078, 3], [1078, 3, 16], [1078, 3, 16, 14], [1078, 3, 16, 14, 5], [1078, 3, 16, 14, 5, 254], [1078, 3, 16, 14, 5, 254, 3], [1078, 3, 16, 14, 5, 254, 3, 53], [1078, 3, 16, 14, 5, 254, 3, 53, 1490], [1078, 3, 16, 14, 5, 254, 3, 53, 1490, 17], [1078, 3, 16, 14, 5, 254, 3, 53, 1490, 17, 29], [1078, 3, 16, 14, 5, 254, 3, 53, 1490, 17, 29, 22], [1078, 3, 16, 14, 5, 254, 3, 53, 1490, 17, 29, 22, 17], [1078, 3, 16, 14, 5, 254, 3, 53, 1490, 17, 29, 22, 17, 1], [1078, 3, 16, 14, 5, 254, 3, 53, 1490, 17, 29, 22, 17, 1, 60], [6, 19], [6, 19, 159], [6, 19, 159, 80], [6, 19, 159, 80, 692], [6, 19, 159, 80, 692, 22], [6, 19, 159, 80, 692, 22, 54], [6, 19, 159, 80, 692, 22, 54, 1], [6, 19, 159, 80, 692, 22, 54, 1, 107], [6, 19, 159, 80, 692, 22, 54, 1, 107, 1491], [6, 19, 159, 80, 692, 22, 54, 1, 107, 1491, 177], [6, 19, 159, 80, 692, 22, 54, 1, 107, 1491, 177, 4], [6, 19, 159, 80, 692, 22, 54, 1, 107, 1491, 177, 4, 418], [34, 7], [34, 7, 33], [34, 7, 33, 1079], [34, 7, 33, 1079, 2], [34, 7, 33, 1079, 2, 104], [34, 7, 33, 1079, 2, 104, 17], [34, 7, 33, 1079, 2, 104, 17, 6], [34, 7, 33, 1079, 2, 104, 17, 6, 2], [34, 7, 33, 1079, 2, 104, 17, 6, 2, 41], [34, 7, 33, 1079, 2, 104, 17, 6, 2, 41, 324], [34, 7, 33, 1079, 2, 104, 17, 6, 2, 41, 324, 18], [10, 1080], [10, 1080, 3], [10, 1080, 3, 16], [10, 1080, 3, 16, 202], [10, 1080, 3, 16, 202, 23], [10, 1080, 3, 16, 202, 23, 6], [10, 1080, 3, 16, 202, 23, 6, 1492], [10, 1080, 3, 16, 202, 23, 6, 1492, 592], [10, 1080, 3, 16, 202, 23, 6, 1492, 592, 16], [10, 1080, 3, 16, 202, 23, 6, 1492, 592, 16, 322]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WcjZEFIzoxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_padded_sequences(input_sequences, seq_length):\n",
        "    #max_sequence_len = max([len(i) for i in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=seq_length, padding='pre',truncating='pre'))\n",
        "    X, y = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "    y = to_categorical(y, num_classes=vocab_size)\n",
        "    return X, y\n",
        "\n",
        "X, y = generate_padded_sequences(sequences, seq_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRqAm6ODL-2L",
        "colab_type": "text"
      },
      "source": [
        "5. Add dropout to the input layer, remove it from the layer before dense layer. Use Dropout value of 0.1 everywhere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZe4pxfo9Y5T",
        "colab_type": "code",
        "outputId": "4edea314-40a5-4436-bc41-72329e0175d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length-1, mask_zero=True))\n",
        "model.add(Dropout(0.1))\n",
        "#A dropout on the input means that for a given probability, the data on the input \n",
        "# connection to each LSTM block will be excluded from node activation and weight updates.\n",
        "model.add(LSTM(256, input_shape=(seq_length, vocab_size), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0727 10:08:04.228968 140038774474624 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0727 10:08:04.266468 140038774474624 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0727 10:08:04.273483 140038774474624 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0727 10:08:04.385874 140038774474624 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0727 10:08:04.393807 140038774474624 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0727 10:08:04.778551 140038774474624 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0727 10:08:05.190787 140038774474624 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0727 10:08:05.211223 140038774474624 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmI52Zhnfzrz",
        "colab_type": "code",
        "outputId": "bf192190-8cdf-4af1-9e01-bacd5b240cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "print(model.summary())\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 99, 50)            132350    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 99, 50)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 99, 256)           314368    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 99, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2647)              680279    \n",
            "=================================================================\n",
            "Total params: 1,652,309\n",
            "Trainable params: 1,652,309\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy4q8sUtihYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "#checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [earlystop]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKdK6gQlMIn8",
        "colab_type": "text"
      },
      "source": [
        "4. Train the model for 100 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMBv3qmMimrY",
        "colab_type": "code",
        "outputId": "038777bf-8339-492e-a948-4cf0fe7f0332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X, y, epochs=100, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "24080/24080 [==============================] - 74s 3ms/step - loss: 6.4714\n",
            "Epoch 2/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "24080/24080 [==============================] - 69s 3ms/step - loss: 6.0424\n",
            "Epoch 3/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 5.9621\n",
            "Epoch 4/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 5.8468\n",
            "Epoch 5/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 5.7501\n",
            "Epoch 6/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 5.6413\n",
            "Epoch 7/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 5.5336\n",
            "Epoch 8/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 5.4324\n",
            "Epoch 9/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 5.3429\n",
            "Epoch 10/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 5.2671\n",
            "Epoch 11/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 5.1995\n",
            "Epoch 12/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 5.1361\n",
            "Epoch 13/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 5.0716\n",
            "Epoch 14/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 5.0120\n",
            "Epoch 15/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 4.9496\n",
            "Epoch 16/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 4.8857\n",
            "Epoch 17/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 4.8198\n",
            "Epoch 18/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 4.7538\n",
            "Epoch 19/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 4.6832\n",
            "Epoch 20/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 4.6061\n",
            "Epoch 21/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 4.5320\n",
            "Epoch 22/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 4.4532\n",
            "Epoch 23/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 4.3729\n",
            "Epoch 24/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 4.2989\n",
            "Epoch 25/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 4.2218\n",
            "Epoch 26/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 4.1474\n",
            "Epoch 27/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 4.0704\n",
            "Epoch 28/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 4.0003\n",
            "Epoch 29/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 3.9311\n",
            "Epoch 30/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 3.8607\n",
            "Epoch 31/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 3.7905\n",
            "Epoch 32/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 3.7201\n",
            "Epoch 33/100\n",
            "24080/24080 [==============================] - 67s 3ms/step - loss: 3.6544\n",
            "Epoch 34/100\n",
            "24080/24080 [==============================] - 67s 3ms/step - loss: 3.5895\n",
            "Epoch 35/100\n",
            "24080/24080 [==============================] - 67s 3ms/step - loss: 3.5273\n",
            "Epoch 36/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 3.4589\n",
            "Epoch 37/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 3.4035\n",
            "Epoch 38/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 3.3378\n",
            "Epoch 39/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 3.2811\n",
            "Epoch 40/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 3.2263\n",
            "Epoch 41/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 3.1723\n",
            "Epoch 42/100\n",
            "24080/24080 [==============================] - 67s 3ms/step - loss: 3.1128\n",
            "Epoch 43/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 3.0592\n",
            "Epoch 44/100\n",
            "24080/24080 [==============================] - 67s 3ms/step - loss: 3.0041\n",
            "Epoch 45/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 2.9528\n",
            "Epoch 46/100\n",
            "24080/24080 [==============================] - 67s 3ms/step - loss: 2.8985\n",
            "Epoch 47/100\n",
            "24080/24080 [==============================] - 67s 3ms/step - loss: 2.8573\n",
            "Epoch 48/100\n",
            "24080/24080 [==============================] - 67s 3ms/step - loss: 2.8033\n",
            "Epoch 49/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 2.7554\n",
            "Epoch 50/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 2.7060\n",
            "Epoch 51/100\n",
            "24080/24080 [==============================] - 67s 3ms/step - loss: 2.6623\n",
            "Epoch 52/100\n",
            "24080/24080 [==============================] - 67s 3ms/step - loss: 2.6189\n",
            "Epoch 53/100\n",
            "24080/24080 [==============================] - 67s 3ms/step - loss: 2.5714\n",
            "Epoch 54/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 2.5308\n",
            "Epoch 55/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 2.4828\n",
            "Epoch 56/100\n",
            "24080/24080 [==============================] - 67s 3ms/step - loss: 2.4408\n",
            "Epoch 57/100\n",
            "24080/24080 [==============================] - 67s 3ms/step - loss: 2.4026\n",
            "Epoch 58/100\n",
            "24080/24080 [==============================] - 67s 3ms/step - loss: 2.3615\n",
            "Epoch 59/100\n",
            "24080/24080 [==============================] - 67s 3ms/step - loss: 2.3199\n",
            "Epoch 60/100\n",
            "24080/24080 [==============================] - 66s 3ms/step - loss: 2.2764\n",
            "Epoch 61/100\n",
            "24080/24080 [==============================] - 66s 3ms/step - loss: 2.2367\n",
            "Epoch 62/100\n",
            "24080/24080 [==============================] - 65s 3ms/step - loss: 2.2004\n",
            "Epoch 63/100\n",
            "24080/24080 [==============================] - 65s 3ms/step - loss: 2.1596\n",
            "Epoch 64/100\n",
            "24080/24080 [==============================] - 66s 3ms/step - loss: 2.1235\n",
            "Epoch 65/100\n",
            "24080/24080 [==============================] - 65s 3ms/step - loss: 2.0857\n",
            "Epoch 66/100\n",
            "24080/24080 [==============================] - 65s 3ms/step - loss: 2.0509\n",
            "Epoch 67/100\n",
            "24080/24080 [==============================] - 65s 3ms/step - loss: 2.0152\n",
            "Epoch 68/100\n",
            "24080/24080 [==============================] - 65s 3ms/step - loss: 1.9774\n",
            "Epoch 69/100\n",
            "24080/24080 [==============================] - 66s 3ms/step - loss: 1.9480\n",
            "Epoch 70/100\n",
            "24080/24080 [==============================] - 67s 3ms/step - loss: 1.9088\n",
            "Epoch 71/100\n",
            "24080/24080 [==============================] - 66s 3ms/step - loss: 1.8764\n",
            "Epoch 72/100\n",
            "24080/24080 [==============================] - 68s 3ms/step - loss: 1.8386\n",
            "Epoch 73/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 1.8128\n",
            "Epoch 74/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 1.7803\n",
            "Epoch 75/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 1.7434\n",
            "Epoch 76/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 1.7123\n",
            "Epoch 77/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 1.6825\n",
            "Epoch 78/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 1.6538\n",
            "Epoch 79/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 1.6298\n",
            "Epoch 80/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 1.5919\n",
            "Epoch 81/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 1.5626\n",
            "Epoch 82/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 1.5320\n",
            "Epoch 83/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 1.5109\n",
            "Epoch 84/100\n",
            "24080/24080 [==============================] - 72s 3ms/step - loss: 1.4817\n",
            "Epoch 85/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 1.4568\n",
            "Epoch 86/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 1.4224\n",
            "Epoch 87/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 1.4000\n",
            "Epoch 88/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 1.3738\n",
            "Epoch 89/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 1.3490\n",
            "Epoch 90/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 1.3215\n",
            "Epoch 91/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 1.2966\n",
            "Epoch 92/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 1.2695\n",
            "Epoch 93/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 1.2418\n",
            "Epoch 94/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 1.2340\n",
            "Epoch 95/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 1.1985\n",
            "Epoch 96/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 1.1770\n",
            "Epoch 97/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 1.1574\n",
            "Epoch 98/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 1.1316\n",
            "Epoch 99/100\n",
            "24080/24080 [==============================] - 69s 3ms/step - loss: 1.1085\n",
            "Epoch 100/100\n",
            "24080/24080 [==============================] - 70s 3ms/step - loss: 1.0893\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5d2e41c9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di0V070Ti0xr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a1a536f-dbed-4f54-ef0a-edab8ef0a423"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  wonderland.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp4WytbYBTy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = load_model('weights-improvement-78-6.2132-bigger.hdf5')\n",
        "\n",
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "\tresult = list()\n",
        "\tin_text = seed_text\n",
        "\tchar_gen=0\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded],\n",
        "                            maxlen=seq_length-1,\n",
        "                            padding='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t#print(yhat.shape, yhat)\n",
        "    # map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\t#print(\"Index:\", index)\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\t#print(out_word)\n",
        "\t\tin_text += ' ' + out_word\n",
        "\t\tchar_gen+=len(out_word)\n",
        "\t\tif(char_gen >= 500):\n",
        "\t\t\t\tprint(\"completed 500 character\")\n",
        "\t\t\t\tbreak\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSUzckxcCGqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7a707170-e3a2-42c0-91c7-384e1df7b87a"
      },
      "source": [
        "print(tokens[0])\n",
        "# select a seed text\n",
        "seed_text = tokens[randint(0,len(tokens))]\n",
        "print(seed_text + '\\n')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alices adventures in wonderland\n",
            "out of its waistcoatpocket and looked at it and then hurried on\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9u2elsLBhbB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c3573483-5056-464e-8069-3514cf7bacd4"
      },
      "source": [
        "# generate new text\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 500)\n",
        "print(generated)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed 500 character\n",
            "it and the white rabbit had had finished this young lady is out of sight and she was a very truthful cake on among the distant little thing howled so at last and looked at her for one and she had a little way with it and the little little thing howled so alice did what was no use in spite of sight of the same age the queen was sitting on the fan and she went on what she found the same age as she was a little small terrier which had no idea what that she had wept anything to see the same little thing was thinking and the door was the door was to get out what it was no label this time and the door was to sing a morsel of the song\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}