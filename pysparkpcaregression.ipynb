{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled46.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOZWzqXIF4xop/2UjtPkUCY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopal2812/mlblr/blob/master/pysparkpcaregression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS9pAwoU9RRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar -xvf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvL9KWmb97xX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "try:\n",
        "    from pyspark import SparkContext, SparkConf\n",
        "    from pyspark.sql import SparkSession\n",
        "except ImportError as e:\n",
        "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')\n",
        "\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HhG_4Hs-o5x",
        "colab_type": "text"
      },
      "source": [
        "Welcome to the last exercise of this course. This is also the most advanced one because it somehow glues everything together you've learned.\n",
        "\n",
        "These are the steps you will do:\n",
        "\n",
        "load a data frame from cloudant/ApacheCouchDB\n",
        "perform feature transformation by calculating minimal and maximal values of different properties on time windows (we'll explain what a time windows is later in here)\n",
        "reduce these now twelve dimensions to three using the PCA (Principal Component Analysis) algorithm of SparkML (Spark Machine Learning) => We'll actually make use of SparkML a lot more in the next course\n",
        "plot the dimensionality reduced data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh-Yok7G-qdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/IBM/coursera/blob/master/coursera_ds/washing.parquet?raw=true\n",
        "!mv washing.parquet?raw=true washing.parquet\n",
        "\n",
        "df = spark.read.parquet('washing.parquet')\n",
        "df.createOrReplaceTempView('washing')\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFIZWP8s_GC9",
        "colab_type": "text"
      },
      "source": [
        "This is the feature transformation part of this exercise. Since our table is mixing schemas from different sensor data sources we are creating new features. In other word we use existing columns to calculate new ones. We only use min and max for now, but using more advanced aggregations as we've learned in week three may improve the results. We are calculating those aggregations over a sliding window \"w\". This window is defined in the SQL statement and basically reads the table by a one by one stride in direction of increasing timestamp. Whenever a row leaves the window a new one is included. Therefore this window is called sliding window (in contrast to tubling, time or count windows). More on this can be found here: https://flink.apache.org/news/2015/12/04/Introducing-windows.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONhW4tyz_PGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = spark.sql(\"\"\"\n",
        "select * from (\n",
        "  SELECT \n",
        "  min(temperature) over w as min_temperature,\n",
        "  max(temperature) over w as max_temperature,\n",
        "  min(voltage) over w as min_voltage,\n",
        "  max(voltage) over w as max_voltage,\n",
        "  min(flowrate) over w as min_flowrate,\n",
        "  max(flowrate) over w as max_flowrate,\n",
        "  min(frequency) over w as min_frequency,\n",
        "  max(frequency) over w as max_frequency,\n",
        "  min(hardness) over w as min_hardness,\n",
        "  max(hardness) over w as max_hardness,\n",
        "  min(speed) over w as min_speed,\n",
        "  max(speed) over w as max_speed\n",
        "  from Washing\n",
        "  WINDOW w AS (ORDER BY ts ROWS BETWEEN CURRENT ROW AND 10 FOLLOWING)\n",
        ")\n",
        "WHERE min_temperature is not null \n",
        "AND max_temperature is not null\n",
        "AND min_voltage is not null\n",
        "AND max_voltage is not null\n",
        "AND min_flowrate is not null\n",
        "AND max_flowrate is not null\n",
        "AND min_frequency is not null\n",
        "AND max_frequency is not null\n",
        "AND min_hardness is not null\n",
        "AND min_speed is not null\n",
        "AND max_speed is not null\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz8rXOs4CNlC",
        "colab_type": "text"
      },
      "source": [
        "Since this table contains null values also our window might contain them. In case for a certain feature all values in that window are null we obtain also null. As we can see here (in my dataset) this is the case for 9 rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOBJSFKW-3qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.count()-result.count()\n",
        "result.show(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-8EKqqxCXwB",
        "colab_type": "text"
      },
      "source": [
        "Now we import some classes from SparkML. PCA for the actual algorithm. Vectors for the data structure expected by PCA and VectorAssembler to transform data into these vector structures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saqB-H3SCcXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOwWYqqACpRt",
        "colab_type": "text"
      },
      "source": [
        "Let's define a vector transformation helper class which takes all our input features (result.columns) and created one additional column called \"features\" which contains all our input features as one single column wrapped in \"DenseVector\" objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C64_k9kgCiD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assembler = VectorAssembler(inputCols=result.columns, outputCol=\"features\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-HCNji3EKQS",
        "colab_type": "text"
      },
      "source": [
        "Now we actually transform the data, note that this is highly optimized code and runs really fast in contrast if we had implemented it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRnxp-3u-grH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = assembler.transform(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkl_9qakUVJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's have a look at how this new additional column \"features\" looks like:\n",
        "features.rdd.map(lambda r : r.features).take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxGzQz0sU1fF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
        "model = pca.fit(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iaACo1UVC_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_pca = model.transform(features).select('pcaFeatures')\n",
        "result_pca.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teWVi08SVXrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_pca.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9iyZCyKVlmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rdd= result_pca.rdd.sample(False,0.8)\n",
        "x = rdd.map(lambda a : a.pcaFeatures).map(lambda a: a[0]).collect()\n",
        "y = rdd.map(lambda a: a.pcaFeatures).map(lambda a: a[1]).collect()\n",
        "z = rdd.map(lambda a: a.pcaFeatures).map(lambda a: a[2]).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGldz72TXvOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ax.scatter(x,y,z, c='r', marker='o')\n",
        "\n",
        "ax.set_xlabel('dimension1')\n",
        "ax.set_ylabel('dimension2')\n",
        "ax.set_zlabel('dimension3')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v6Gj51DX7w0",
        "colab_type": "text"
      },
      "source": [
        "We can see two clusters in the data set. We can also see a third cluster which either can be outliers or a real cluster. In the next course we will actually learn how to compute clusters automatically. For now we know that the data indicates that there are two semi-stable states of the machine and sometime we see some anomalies since those data points don't fit into one of the two clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKibifsXN8ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# delete files from previous runs\n",
        "!rm -f hmp.parquet*\n",
        "\n",
        "# download the file containing the data in PARQUET format\n",
        "!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
        "    \n",
        "# create a dataframe out of it\n",
        "df = spark.read.parquet('hmp.parquet')\n",
        "\n",
        "# register a corresponding query table\n",
        "df.createOrReplaceTempView('df')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAUdp-oBOsfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Normalizer\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "#create the string index\n",
        "indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
        "encoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n",
        "                                  outputCol=\"features\")\n",
        "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n",
        "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer])\n",
        "model = pipeline.fit(df)\n",
        "prediction = model.transform(df)\n",
        "prediction.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkpbZ_VmPbOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now let’s create a new pipeline for kmeans.\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "kmeans = KMeans(featuresCol=\"features\").setK(14).setSeed(1)\n",
        "pipeline = Pipeline(stages=[vectorAssembler, kmeans])\n",
        "model = pipeline.fit(df)\n",
        "predictions = model.transform(df)\n",
        "\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih56pE2oP3e-",
        "colab_type": "text"
      },
      "source": [
        "We have 14 different movement patterns in the dataset, so setting K of KMeans to 14 is a good idea. But please experiment with different values for K, do you find a sweet spot? The closer Silhouette gets to 1, the better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwOxgtskP2Im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(2,14):\n",
        "  kmeans = KMeans(featuresCol=\"features\").setK(i).setSeed(1)\n",
        "  pipeline = Pipeline(stages=[vectorAssembler, kmeans])\n",
        "  model = pipeline.fit(df)\n",
        "  predictions = model.transform(df)\n",
        "  evaluator = ClusteringEvaluator()\n",
        "  silhouette = evaluator.evaluate(predictions)\n",
        "  print(\"Silhouette with squared euclidean distance {} {}\".format(str(silhouette), i))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgytdlG9aajA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#In this exercise we’ll use the HMP dataset again and perform some basic operations using Apache SparkML Pipeline components.\n",
        "\n",
        "!rm -f hmp.parquet*\n",
        "\n",
        "# download the file containing the data in PARQUET format\n",
        "!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
        "    \n",
        "# create a dataframe out of it\n",
        "df = spark.read.parquet('hmp.parquet')\n",
        "\n",
        "# register a corresponding query table\n",
        "df.createOrReplaceTempView('df')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYNm_FweaqaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Normalizer, MinMaxScaler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
        "encoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n",
        "                                  outputCol=\"features\")\n",
        "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n",
        "\n",
        "minmaxscaler = MinMaxScaler(inputCol=\"features_norm\", outputCol=\"features_minmaxscaler\")\n",
        "\n",
        "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer,minmaxscaler])\n",
        "model = pipeline.fit(df)\n",
        "prediction = model.transform(df)\n",
        "prediction.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5021Z6Dbpnb",
        "colab_type": "text"
      },
      "source": [
        "The difference between a transformer and an estimator is state. A transformer is stateless whereas an estimator keeps state. Therefore “VectorAsselmbler” is a transformer since it only need to read row by row. Normalizer, on the other hand need to compute statistics on the dataset before, therefore it is an estimator. An estimator has an additional “fit” function. “OneHotEncoder” has been deprecated in Spark 2.3, therefore please change the code below to use the OneHotEstimator instead of the “OneHotEncoder”.\n",
        "\n",
        "More information can be found here: http://spark.apache.org/docs/latest/ml-features.html#onehotencoderestimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FuZaUn_Pjbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "splits = df.randomSplit([0.8, 0.2])\n",
        "df_train = splits[0]\n",
        "df_test = splits[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXUnen3ke7TF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import Normalizer\n",
        "\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n",
        "                                  outputCol=\"features\")\n",
        "\n",
        "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYMMuwSOfe71",
        "colab_type": "text"
      },
      "source": [
        "Now we use LogisticRegression, a simple and basic linear classifier to obtain a classification performance baseline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W6mKDbLfFua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "pipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer,lr])\n",
        "model = pipeline.fit(df_train)\n",
        "prediction = model.transform(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzlJ0ymBfsvP",
        "colab_type": "text"
      },
      "source": [
        "If we look at the schema of the prediction dataframe we see that there is an additional column called prediction which contains the best guess for the class our model predicts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUivdhwvfo6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzzWMDQIgJKc",
        "colab_type": "text"
      },
      "source": [
        "If we look at the schema of the prediction dataframe we see that there is an additional column called prediction which contains the best guess for the class our model predicts.Let’s evaluate performance by using a build-in functionality of Apache SparkML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoAmLCUgIaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "MulticlassClassificationEvaluator().setMetricName(\"accuracy\").evaluate(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxcN3_kAggT5",
        "colab_type": "text"
      },
      "source": [
        "So we get 20% right. This is not bad for a baseline. Note that random guessing would give us only 7%. Of course we need to improve. You might have notices that we’re dealing with a time series here. And we’re not making use of that fact right now as we look at each training example only individually. But this is ok for now. More advanced courses like “Advanced Machine Learning and Signal Processing” (https://www.coursera.org/learn/advanced-machine-learning-signal-processing/) will teach you how to improve accuracy to the nearly 100% by using algorithms like Fourier transformation or wavelet transformation. But let’s skip this for now. In the following cell, please use the RandomForest classifier (you might need to play with the “numTrees” parameter) in the code cell below. You should get an accuracy of around 44%. More on RandomForest can be found here:\n",
        "\n",
        "https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGgKEiYVgfaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "rf = RandomForestClassifier(numTrees=10)\n",
        "pipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer,rf])\n",
        "model = pipeline.fit(df_train)\n",
        "prediction = model.transform(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVvyDQXnf1nW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXm35m-riiem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "MulticlassClassificationEvaluator().setMetricName(\"accuracy\").evaluate(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVAeSbc8iz0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# delete files from previous runs\n",
        "!rm -f jfk_weather*\n",
        "\n",
        "# download the file containing the data in CSV format\n",
        "!wget http://max-training-data.s3-api.us-geo.objectstorage.softlayer.net/noaa-weather/jfk_weather.tar.gz\n",
        "\n",
        "# extract the data\n",
        "!tar xvfz jfk_weather.tar.gz\n",
        "    \n",
        "# create a dataframe out of it by using the first row as field names and trying to infer a schema based on contents\n",
        "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv('jfk_weather.csv')\n",
        "\n",
        "# register a corresponding query table\n",
        "df.createOrReplaceTempView('df')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFkuWTcjjMM0",
        "colab_type": "text"
      },
      "source": [
        "The dataset contains some null values, therefore schema inference didn’t work properly for all columns, in addition, a column contained trailing characters, so we need to clean up the data set first. This is a normal task in any data science project since your data is never clean, don’t worry if you don’t understand all code, you won’t be asked about it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMS-YCAIjNfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H1urHECjjge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "from pyspark.sql.functions import translate, col\n",
        "\n",
        "df_cleaned = df \\\n",
        "    .withColumn(\"HOURLYWindSpeed\", df.HOURLYWindSpeed.cast('double')) \\\n",
        "    .withColumn(\"HOURLYWindDirection\", df.HOURLYWindDirection.cast('double')) \\\n",
        "    .withColumn(\"HOURLYStationPressure\", translate(col(\"HOURLYStationPressure\"), \"s,\", \"\")) \\\n",
        "    .withColumn(\"HOURLYPrecip\", translate(col(\"HOURLYPrecip\"), \"s,\", \"\")) \\\n",
        "    .withColumn(\"HOURLYRelativeHumidity\", translate(col(\"HOURLYRelativeHumidity\"), \"*\", \"\")) \\\n",
        "    .withColumn(\"HOURLYDRYBULBTEMPC\", translate(col(\"HOURLYDRYBULBTEMPC\"), \"*\", \"\")) \\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO3-DOJ2j5Nm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cleaned =   df_cleaned \\\n",
        "                    .withColumn(\"HOURLYStationPressure\", df_cleaned.HOURLYStationPressure.cast('double')) \\\n",
        "                    .withColumn(\"HOURLYPrecip\", df_cleaned.HOURLYPrecip.cast('double')) \\\n",
        "                    .withColumn(\"HOURLYRelativeHumidity\", df_cleaned.HOURLYRelativeHumidity.cast('double')) \\\n",
        "                    .withColumn(\"HOURLYDRYBULBTEMPC\", df_cleaned.HOURLYDRYBULBTEMPC.cast('double')) \\\n",
        "\n",
        "df_cleaned.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YSIsleJjGpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_filtered = df_cleaned.filter(\"\"\"\n",
        "    HOURLYWindSpeed <> 0\n",
        "    and HOURLYWindSpeed IS NOT NULL\n",
        "    and HOURLYWindDirection IS NOT NULL\n",
        "    and HOURLYStationPressure IS NOT NULL\n",
        "    and HOURLYPressureTendency IS NOT NULL\n",
        "    and HOURLYPrecip IS NOT NULL\n",
        "    and HOURLYRelativeHumidity IS NOT NULL\n",
        "    and HOURLYDRYBULBTEMPC IS NOT NULL\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwqMJgzpkoEv",
        "colab_type": "text"
      },
      "source": [
        "We want to predict the value of one column based of some others. It is sometimes helpful to print a correlation matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZwNU77HkjJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYWindDirection\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\"],\n",
        "                                  outputCol=\"features\")\n",
        "df_pipeline = vectorAssembler.transform(df_filtered)\n",
        "df_pipeline.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JREp0EzkzP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.stat import Correlation\n",
        "Correlation.corr(df_pipeline,\"features\").head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t8yD6Xvl-zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Correlation.corr(df_pipeline,\"features\").head()[0].toArray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPTCBm3EmbVx",
        "colab_type": "text"
      },
      "source": [
        "As we can see, HOURLYWindSpeed and HOURLYWindDirection correlate with 0.25478863 whereas HOURLYWindSpeed and HOURLYStationPressure correlate with -0.26171147, this is a good sign if we want to predict HOURLYWindSpeed from HOURLYWindDirection and HOURLYStationPressure. Note that the numbers can change over time as we are always working with the latest data. Since this is supervised learning, let’s split our data into train (80%) and test (20%) set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbwoXNjZmcik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "splits = df_filtered.randomSplit([0.8, 0.2])\n",
        "df_train = splits[0]\n",
        "df_test = splits[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHBfhY_9mlWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import Normalizer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols=[\n",
        "                                    \"HOURLYWindDirection\",\n",
        "                                    \"ELEVATION\",\n",
        "                                    \"HOURLYStationPressure\"],\n",
        "                                  outputCol=\"features\")\n",
        "\n",
        "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtO0qYMxm3fv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Now we define a function for evaluating our regression prediction performance. We’re using RMSE (Root Mean Squared Error) here , the smaller the better…"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfXQqby4mzPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regression_metrics(prediction):\n",
        "    from pyspark.ml.evaluation import RegressionEvaluator\n",
        "    evaluator = RegressionEvaluator(\n",
        "    labelCol=\"HOURLYWindSpeed\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "    rmse = evaluator.evaluate(prediction)\n",
        "    print(\"RMSE on test data = %g\" % rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJXU7k64nEps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LR1\n",
        "\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "\n",
        "lr = LinearRegression(labelCol=\"HOURLYWindSpeed\", featuresCol='features_norm', maxIter=100, regParam=0.0, elasticNetParam=0.0)\n",
        "pipeline = Pipeline(stages=[vectorAssembler, normalizer,lr])\n",
        "model = pipeline.fit(df_train)\n",
        "prediction = model.transform(df_test)\n",
        "regression_metrics(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aNOOqjmnVUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now we’ll try a Gradient Boosted Tree Regressor\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "gbt = GBTRegressor(labelCol=\"HOURLYWindSpeed\", maxIter=100)\n",
        "pipeline = Pipeline(stages=[vectorAssembler, normalizer,gbt])\n",
        "model = pipeline.fit(df_train)\n",
        "prediction = model.transform(df_test)\n",
        "regression_metrics(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEWXraiEoxYs",
        "colab_type": "text"
      },
      "source": [
        "## #Now let’s switch gears. Previously, we tried to predict HOURLYWindSpeed, but now we predict HOURLYWindDirection. In order to turn this into a classification problem we discretize the value using the Bucketizer. The new feature is called HOURLYWindDirectionBucketized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCTzauhYnz_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import Bucketizer, OneHotEncoder\n",
        "bucketizer = Bucketizer(splits=[ 0, 180, float('Inf') ],inputCol=\"HOURLYWindDirection\", outputCol=\"HOURLYWindDirectionBucketized\")\n",
        "encoder = OneHotEncoder(inputCol=\"HOURLYWindDirectionBucketized\", outputCol=\"HOURLYWindDirectionOHE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBwiB2gwoEye",
        "colab_type": "text"
      },
      "source": [
        "Again, we define a function in order to assess how we perform. Here we just use the accuracy measure which gives us the fraction of correctly classified examples. Again, 0 is bad, 1 is good."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rfmcU8KoF8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classification_metrics(prediction):\n",
        "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "    mcEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"HOURLYWindDirectionBucketized\")\n",
        "    accuracy = mcEval.evaluate(prediction)\n",
        "    print(\"Accuracy on test data = %g\" % accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz27XcaEoMHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#LGReg1\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=10)\n",
        "#,\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\"],\n",
        "                                  outputCol=\"features\")\n",
        "\n",
        "pipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,lr])\n",
        "model = pipeline.fit(df_train)\n",
        "prediction = model.transform(df_test)\n",
        "classification_metrics(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ89EtSpoTPz",
        "colab_type": "text"
      },
      "source": [
        "Let’s try some other Algorithms and see if model performance increases. It’s also important to tweak other parameters like parameters of individual algorithms (e.g. number of trees for RandomForest) or parameters in the feature engineering pipeline, e.g. train/test split ratio, normalization, bucketing, …"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyyZJ8DWoZNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RF1\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rf = RandomForestClassifier(labelCol=\"HOURLYWindDirectionBucketized\", numTrees=30)\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n",
        "                                  outputCol=\"features\")\n",
        "\n",
        "pipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,rf])\n",
        "model = pipeline.fit(df_train)\n",
        "prediction = model.transform(df_test)\n",
        "classification_metrics(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_wU7vp-vq1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RF1\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rf = RandomForestClassifier(labelCol=\"HOURLYWindDirectionBucketized\", numTrees=10)\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n",
        "                                  outputCol=\"features\")\n",
        "\n",
        "pipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,rf])\n",
        "model = pipeline.fit(df_train)\n",
        "prediction = model.transform(df_test)\n",
        "classification_metrics(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaxPm5TvomiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#GBT2\n",
        "\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "gbt = GBTClassifier(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=100)\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n",
        "                                  outputCol=\"features\")\n",
        "\n",
        "pipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,gbt])\n",
        "model = pipeline.fit(df_train)\n",
        "prediction = model.transform(df_test)\n",
        "classification_metrics(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHBgR1u3oSQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}