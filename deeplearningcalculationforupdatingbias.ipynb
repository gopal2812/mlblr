{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/gopal2812/mlblr/blob/master/deeplearningcalculationforupdatingbias.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "mbUhHuaL0zQy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "7457a783-35ce-4096-e47d-a90b5bc67cbf"
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "for x in range(19):\n",
        "  print(round(random.random(),2))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.03\n",
            "0.81\n",
            "0.78\n",
            "0.45\n",
            "1.0\n",
            "0.68\n",
            "0.07\n",
            "0.96\n",
            "0.19\n",
            "0.38\n",
            "0.7\n",
            "0.54\n",
            "0.73\n",
            "0.07\n",
            "0.11\n",
            "0.68\n",
            "0.88\n",
            "0.16\n",
            "0.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qnTof5kCLEb5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "e861f3af-4c45-4ec5-86b4-39d413600a54"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "print(\"Step 1: Initialize weights and biases with random values\")\n",
        "#(There are methods to initialize weights and biases but for now initialize with random values)\n",
        "x=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
        "wh=np.array([[0.49,0.69,0.47],[0.38,0.85,0.98],[0.02,0.91,0.05],[0.93, 0.31,0.07]])\n",
        "bh=np.array([[0.69,0.86,0.94],[0.69,0.86,0.94],[0.69,0.86,0.94]])\n",
        "w_out=np.array([0.98,0.39,0.98])\n",
        "b_out=np.array([0.91, 0.91, 0.91])\n",
        "print(x)\n",
        "print(wh)\n",
        "print(bh)\n",
        "print(w_out)\n",
        "print(b_out)\n",
        "\n",
        "\n",
        "print(\"Step 2 Calculate hidden layer input:\")\n",
        "hidden_layer_input= x.dot(w) +b\n",
        "print(hidden_layer_input)\n",
        "\n",
        "print(\"Step 3: Perform non-linear transformation on hidden linear input\")\n",
        "#hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
        "y = tf.sigmoid(hidden_layer_input)\n",
        "hiddenlayer_activations = y.eval()\n",
        "print(hiddenlayer_activations)\n",
        "\n",
        "print(\"Step 4: Perform linear and non-linear transformation of hidden layer activation at output layer\")\n",
        "w_op= np.dot(u,w_out) +b_out\n",
        "w_act=tf.sigmoid(w_op)\n",
        "output=w_act.eval()\n",
        "print(output)\n",
        "\n",
        "print(\"Step 5: Calculate gradient of Error(E) at output layer\")\n",
        "#E = y-output\n",
        "y=[1,1,0]\n",
        "E=y-output\n",
        "print(y-output)\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "   ex = np.exp(-x)\n",
        "   y = ex / (1 + ex)**2\n",
        "   return y\n",
        "  \n",
        "print(\"Step 6: Compute slope at output and hidden layer\")\n",
        "#Slope_output_layer= derivatives_sigmoid(output)\n",
        "#Slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
        "slope_output_layer= sigmoid_grad(output)\n",
        "slope_hidden_layer = sigmoid_grad(hiddenlayer_activations)\n",
        "print(slope_output_layer)\n",
        "print(slope_hidden_layer)\n",
        "\n",
        "print(\"Step 7: Compute delta at output layer\")\n",
        "#d_output = E * slope_output_layer*lr\n",
        "d_output = E * slope_output_layer*0.1\n",
        "print(d_output)\n",
        "\n",
        "print(\"Step 8: Calculate Error at hidden layer\")\n",
        "#Error_at_hidden_layer = matrix_dot_product(d_output, wout.Transpose)\n",
        "Error_at_hidden_layer = np.dot(d_output, w_out.T)\n",
        "print(Error_at_hidden_layer)\n",
        "\n",
        "print(\"Step 9: Compute delta at hidden layer\")\n",
        "#d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
        "d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
        "print(d_hiddenlayer)\n",
        "\n",
        "print(\"Step 10: Update weight at both output and hidden layer\")\n",
        "w_out = w_out + np.dot(hiddenlayer_activations.T, d_output) * 0.1\n",
        "wh = wh+ np.dot(x.T,d_hiddenlayer) * 0.1\n",
        "print(w_out)\n",
        "print(wh)\n",
        "\n",
        "print(\"Step 11: Update biases at both output and hidden layer\")\n",
        "bh = bh + np.sum(d_hiddenlayer, axis=0) * 0.1\n",
        "b_out = b_out + np.sum(d_output, axis=0)*0.1\n",
        "print(bh)\n",
        "print(b_out)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1: Initialize weights and biases with random values\n",
            "[[1 0 1 0]\n",
            " [1 0 1 1]\n",
            " [0 1 0 1]]\n",
            "[[0.49 0.69 0.47]\n",
            " [0.38 0.85 0.98]\n",
            " [0.02 0.91 0.05]\n",
            " [0.93 0.31 0.07]]\n",
            "[[0.69 0.86 0.94]\n",
            " [0.69 0.86 0.94]\n",
            " [0.69 0.86 0.94]]\n",
            "[0.98 0.39 0.98]\n",
            "[0.91 0.91 0.91]\n",
            "Step 2 Calculate hidden layer input:\n",
            "[[1.2  2.46 1.46]\n",
            " [2.13 2.77 1.53]\n",
            " [2.   2.02 1.99]]\n",
            "Step 3: Perform non-linear transformation on hidden linear input\n",
            "[[0.76852478 0.92128966 0.81153267]\n",
            " [0.89378501 0.94103299 0.82200631]\n",
            " [0.88079708 0.88288101 0.87974314]]\n",
            "Step 4: Perform linear and non-linear transformation of hidden layer activation at output layer\n",
            "[0.94362799 0.95066267 0.95164641]\n",
            "Step 5: Calculate gradient of Error(E) at output layer\n",
            "[ 0.05637201  0.04933733 -0.95164641]\n",
            "Step 6: Compute slope at output and hidden layer\n",
            "[0.20167393 0.20104913 0.2009616 ]\n",
            "[[0.21643714 0.20364384 0.21296838]\n",
            " [0.20603822 0.20190388 0.21210668]\n",
            " [0.20715623 0.20697741 0.20724658]]\n",
            "Step 7: Compute delta at output layer\n",
            "[ 0.00113688  0.00099192 -0.01912444]\n",
            "Step 8: Calculate Error at hidden layer\n",
            "-0.017240960615981543\n",
            "Step 9: Compute delta at hidden layer\n",
            "[[-0.00373158 -0.00351102 -0.00367178]\n",
            " [-0.0035523  -0.00348102 -0.00365692]\n",
            " [-0.00357157 -0.00356849 -0.00357313]]\n",
            "Step 10: Update weight at both output and hidden layer\n",
            "[0.97849155 0.38850962 0.97849134]\n",
            "[[0.48927161 0.6893008  0.46926713]\n",
            " [0.37964284 0.84964315 0.97964269]\n",
            " [0.01927161 0.9093008  0.04926713]\n",
            " [0.92928761 0.30929505 0.06927699]]\n",
            "Step 11: Update biases at both output and hidden layer\n",
            "[[0.68891445 0.85894395 0.93890982]\n",
            " [0.68891445 0.85894395 0.93890982]\n",
            " [0.68891445 0.85894395 0.93890982]]\n",
            "[0.90830044 0.90830044 0.90830044]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}