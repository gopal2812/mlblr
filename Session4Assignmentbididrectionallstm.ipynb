{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled57.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM8pOX6p/cpqlYX9Cc6X/tG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopal2812/mlblr/blob/master/Session4Assignmentbididrectionallstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-ixjGOfNMan"
      },
      "source": [
        "Instructions\n",
        "\n",
        "Use this as a reference [link text](\n",
        "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb )\n",
        "\n",
        "Change this code in such a way that:\n",
        "\n",
        "1. it has 3 LSTM layers\n",
        "\n",
        "2. it has used a for loop to do so in the forward function\n",
        "3. the dropout value used is 0.2\n",
        "4. trained on the text that is reversed (for example \"my name is Rohan\" becomes \"Rohan is name my\"\n",
        "5. achieves 87% or more accuracy\n",
        "once done, share the Github link as well (after training on Google Colab, move the file to GitHub).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_bHNQxcNytO"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import spacy\n",
        "from torchtext import data, datasets\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "text = data.Field(tokenize = 'spacy', include_lengths = True)\n",
        "label = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4qew6k-Onop"
      },
      "source": [
        "#load the IMDb dataset.\n",
        "train_data, test_data = datasets.IMDB.splits(text, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTY_S4H9O-k6"
      },
      "source": [
        "#reverse training text data in-place\n",
        "for i in range(len(train_data.examples)):\n",
        "  vars(train_data.examples[i]).get('text').reverse()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfTiU856PDMY"
      },
      "source": [
        "# create the validation set from our training set.\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMEfpRU8PZJ8"
      },
      "source": [
        "# build vocabulary with pre-trained global embedding\n",
        "\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "text.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "label.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyPqo1sPPeLQ"
      },
      "source": [
        "#Another thing for packed padded sequences all of the tensors within a batch \n",
        "#need to be sorted by their lengths. This is handled in the iterator by setting \n",
        "#sort_within_batch = True. \n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZkrspWAaFmt"
      },
      "source": [
        "\n",
        "Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THt4FDNAPki9"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "\n",
        "    #parts list for building blocks\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
        "                 n_layers, bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, \n",
        "                                      padding_idx = pad_idx)\n",
        "        #bidirectional = False\n",
        "        self.rnns = nn.ModuleList([nn.LSTM(embedding_dim, hidden_dim, \n",
        "                                           bidirectional=bidirectional)])\n",
        "        # LSTM layers = 3\n",
        "        for _ in range(n_layers - 1):\n",
        "          self.rnns.append(nn.LSTM(hidden_dim, hidden_dim, \n",
        "                                   bidirectional=bidirectional))\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    #step-by-step manual for assembling building blocks    \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "        \n",
        "        #stack multiple (3) LSTM layers with dropouts\n",
        "        x = packed_embedded\n",
        "        for rnn in self.rnns:\n",
        "          _, (x, _) = rnn(x)\n",
        "          x = self.dropout(x)\n",
        "        \n",
        "        #x = last hidden states [1, batch size, hid dim]\n",
        "        hidden = x.squeeze(0)\n",
        "\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J02e8tA4P6-9"
      },
      "source": [
        "#define model constants\n",
        "\n",
        "INPUT_DIM = len(text.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 3\n",
        "#changed from True to False         \n",
        "BIDIRECTIONAL = False\n",
        "#changed from 0.5 to 0.2\n",
        "DROPOUT = 0.2\n",
        "PAD_IDX = text.vocab.stoi[text.pad_token]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-cbEnEkQHlx"
      },
      "source": [
        "model = RNN(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            N_LAYERS, \n",
        "            BIDIRECTIONAL, \n",
        "            DROPOUT, \n",
        "            PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gp-k_mcQyMf"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(model)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPHSUZObPx89"
      },
      "source": [
        "pretrained_embeddings = text.vocab.vectors\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWEPWQBDRBtG"
      },
      "source": [
        "#copy pre-trained embeddings from vocabulary to model\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPEqg1q1RFJL"
      },
      "source": [
        "#zero weights for <unk> and <pad> tokens\n",
        "\n",
        "UNK_IDX = text.vocab.stoi[text.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyobdBvddi3f"
      },
      "source": [
        "Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trl-Ou3tRI6P"
      },
      "source": [
        "#instantiate optimizer\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEi5JKCiRMz8"
      },
      "source": [
        "#instantiate loss function\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P38WLdbRRlf"
      },
      "source": [
        "#place the model and criterion on the GPU (if available)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YLO7E8VRVZS"
      },
      "source": [
        "#define the accuracy for training, validation and testing\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch,i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xSLm0itRZs3"
      },
      "source": [
        "#define the training\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "        #runtime type is GPU but  model expects CPU tensor\n",
        "        text_lengths = text_lengths.cpu()\n",
        "\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcJ33YHtRi2t"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.text\n",
        "            \n",
        "            #runtime type is GPU but model expects CPU tensor\n",
        "            text_lengths = text_lengths.cpu()\n",
        "\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqHKsMPTRo1f"
      },
      "source": [
        "# define how to calculate time required per epoch\n",
        "\n",
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcY2qVKGRs67"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-oPtYNxR2l0"
      },
      "source": [
        "# test model using testing dataset\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BQp5tAIXR7C"
      },
      "source": [
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [text.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baCr_nTNSAdy"
      },
      "source": [
        "#test a negative sentence\n",
        "predict_sentiment(model, \"This film is terrible\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r4CVnPpfG5l",
        "outputId": "79037fe2-2b13-492a-f1f3-d6932246c2d5"
      },
      "source": [
        "#test a positive sentence\n",
        "predict_sentiment(model, \"This film is very good\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7142864465713501"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}