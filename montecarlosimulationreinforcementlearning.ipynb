{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled30.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopal2812/mlblr/blob/master/montecarlosimulationreinforcementlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHpVn8VaPqis",
        "colab_type": "text"
      },
      "source": [
        " Monte Carlo simulation can be used to estimate the outcome of non-deterministic processes where that follow some probability distribution.There are two components that we need to run a Monte Carlo simulation:. The equation to evaluate\n",
        ". The random variables for the inputIn this example we are considering leasing a machine for a manufacturing process. The one-year lease costs is $300,000. There are some estimations on the 90% confidence interval of the impact.\n",
        "savings between 5 and -2 dollars to produce each unit.\n",
        "savings between 20 and 15 dollars to transport each unit.\n",
        "production increase between 30000 and 20000.\n",
        "\n",
        "We want to know if it's worth investing in buying the machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHYQe_EVPr77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import scipy\n",
        "from scipy.stats import norm\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "machinery_cost = 300000\n",
        "\n",
        "number_of_simulations = 1000000\n",
        "confidence = 0.90\n",
        "n =1000\n",
        "\n",
        "conf_interval = (scipy.stats.t.ppf((1 + confidence) / 2., n-1))*2\n",
        "\n",
        "\n",
        "# Defining the variables\n",
        "\n",
        "# We find the distrubutions of the potential savings that we will have\n",
        "transport_saving = norm(loc = (20 + 15) / 2, scale = (20 - 15) / conf_interval)\n",
        "# Savings in labour are xtimated between 5 and -1\n",
        "labor_saving = norm(loc = (5 - (-2)) / 2, scale = (5 - (-2)) / conf_interval)\n",
        "transport_results = transport_saving.rvs(number_of_simulations)\n",
        "labor_results = labor_saving.rvs(number_of_simulations)\n",
        "prod_level = norm(loc = (30000 + 20000) / 2, scale = (30000 - 10000) / conf_interval)\n",
        "\n",
        "prod_level_results = prod_level.rvs(number_of_simulations)\n",
        "data = pd.DataFrame({\n",
        "    \"transport_saving_per_unit\": transport_results,\n",
        "    \"labor_savings_per_unit\": labor_results,\n",
        "    \"production_level\": prod_level_results\n",
        "})\n",
        "data[\"total_savings\"] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68zFTtJ4P_qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(data.total_savings, bins = 100)\n",
        "plt.axvline(x = machinery_cost, c = \"g\")\n",
        "plt.show()\n",
        "\n",
        "probability_losing_money = data[data[\"total_savings\"] < machinery_cost].count()[\"total_savings\"] / 1000000\n",
        "f\"The probability of losing money is: {probability_losing_money*100}%\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmzcduHHQKIx",
        "colab_type": "text"
      },
      "source": [
        "The Gyms provides a set of environment where it's possible to test and train different Reinforcement Learning algorithms.It's possible to find more information about the environment here: https://gym.openai.com/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqtN4SfcQIK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Choose environment\n",
        "env_names = ['FrozenLake-v0', 'CartPole-v0']\n",
        "\n",
        "env_name = env_names[0]\n",
        "# Create Environment\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# This funciotn cna be use to visualize the results directly in the notebook\n",
        "render_in_jp = lambda : plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "env.reset()\n",
        "# render_in_jp()\n",
        "print('Starting the enviroment')\n",
        "env.render()\n",
        "# Taking step 1\n",
        "env.step(1)\n",
        "print('Enviroment after step 1')\n",
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzUkbsCFQTY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_actions = env.action_space.n\n",
        "n_states = env.observation_space.n\n",
        "\n",
        "# Actions are left, up, right, down\n",
        "print(f'Number of actions {n_actions} for the {env_name} enviroment')\n",
        "# States are the 16 fields\n",
        "print(f'Number of possible states {n_states} for the {env_name} enviroment')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r098AS2QQZdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('MsPacman-v0')\n",
        "env.reset()\n",
        "for _ in range(10):\n",
        "    render_in_jp()\n",
        "    env.step(env.action_space.sample()) # take a random action\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ-vWWD8QkUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('SpaceInvaders-v0')\n",
        "env.reset()\n",
        "for _ in range(10):\n",
        "    render_in_jp()\n",
        "    env.step(env.action_space.sample()) # take a random action\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YPsxCOmQnAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is only using the observation is the RAM of the Atari machine\n",
        "# consisting of 128 bytes\n",
        "env = gym.make('BattleZone-ram-v0')\n",
        "env.reset()\n",
        "for _ in range(10):\n",
        "    render_in_jp()\n",
        "    env.step(env.action_space.sample()) # take a random action\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}