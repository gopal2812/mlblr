{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled27.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopal2812/mlblr/blob/master/bandit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFPVGHfBtWIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# # Task 1\n",
        "# \n",
        "# We want to implement the epsilon greedy algorithm. The purpose of the algorithm is to balance exploration with exploitation.\n",
        "# \n",
        "# Epsilon greedy will pick the best option except for epsilon times every 100 trials.\n",
        "# \n",
        "# In this task we want to empiricaly find out which one is the best lever to pull.\n",
        "# \n",
        "# Each lever has a specific probability to release a prize, which is distributed accordingly to the normal distribution.\n",
        "# \n",
        "# Please complete the epsilon greedy function.\n",
        "# \n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "import matplotlib.pylab as plt\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "# Multi-armed bandits definitions\n",
        "nb_bandits = 3  # Number of bandits\n",
        "# We set the probability of winning for each bandit\n",
        "p_bandits = [0.45, 0.55, 0.60]\n",
        "\n",
        "def pull(i):\n",
        "    \"\"\"\n",
        "    Pull arm of the bandit i\n",
        "    return 1 if win, otherwise 0.\n",
        "    \"\"\"\n",
        "    if np.random.rand() < p_bandits[i]:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "    \n",
        "# The iterations where we want to plot the results\n",
        "iterations_to_plot = [1, 10, 50, 100, 500, 10000]\n",
        "\n",
        "\n",
        "# In[6]:\n",
        "\n",
        "\n",
        "# \n",
        "def epsilon_greedy(q, epsilon=0.2):\n",
        "    \"\"\"\n",
        "    The epsilon greedy functions that returns a random choice\n",
        "    with a probability equal to epsilon, and the best choice\n",
        "    the rest of the time.\n",
        "    \"\"\"\n",
        "    a = np.argmax(q)\n",
        "    if np.random.rand() < epsilon:\n",
        "        a = np.random.randint(len(q))\n",
        "    return a\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "\n",
        "# Run the trail for `n` iteration\n",
        "def run_simulation(n_iterations, iterations_to_plot, epsilon=0.2):\n",
        "\n",
        "    # Setup plot\n",
        "    fig1, axs1 = plt.subplots(3, 2, figsize=(8, 10))\n",
        "    axs1 = axs1.flat\n",
        "    fig2, axs2 = plt.subplots(3, 2, figsize=(8, 10))\n",
        "    axs2 = axs2.flat\n",
        "\n",
        "    # The number of trials and wins will represent the prior for each\n",
        "    #  bandit with the help of the Beta distribution.\n",
        "    trials = [0, 0.01, 0.01]  # Number of times we tried each bandit\n",
        "    wins = [0, 0.01, 0.01]  # Number of wins for each bandit\n",
        "    \n",
        "    for iteration in range(1, n_iterations+1):\n",
        "        # Define the prior based on current observations\n",
        "        try:\n",
        "            results = [w/t for w,t in zip(wins, trials)]\n",
        "        except ZeroDivisionError as e:\n",
        "            # If we don't have any trials we just put 0 \n",
        "            results = [w/t if t else 0 for w,t in zip(wins, trials)]\n",
        "        if iteration in iterations_to_plot:\n",
        "            ax1 = next(axs1)\n",
        "            ax1.set_xticklabels(('B1', 'B2', 'B3'))\n",
        "            ax1.bar([1, 2, 3], results)\n",
        "            ax1.set_title(f'Extimated Probability at iteration {iteration:d}')\n",
        "            ax2 = next(axs2)\n",
        "            ax2.bar([1.2, 2.2, 3.2], wins)\n",
        "#             ax2.set_ylabel('Cumulative wins', color='r')\n",
        "            ax2.set_title(f'Cumulative wins at iteration {iteration:d}')\n",
        "\n",
        "        # Use epsilon greedy to choose a bandit\n",
        "        chosen_bandit = epsilon_greedy(results, epsilon)\n",
        "        # Pull the bandit\n",
        "        x = pull(chosen_bandit)\n",
        "        # Update trials and wins (defines the posterior)\n",
        "        trials[chosen_bandit] += 1\n",
        "        wins[chosen_bandit] += x\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "run_simulation(n_iterations=10_000, iterations_to_plot=iterations_to_plot)\n",
        "\n",
        "\n",
        "# # Task 2\n",
        "# \n",
        "# In a Reinforcement Learning we want to balance exploration and exploitation.\n",
        "# \n",
        "# To achieve this using our to MAB algorithm we want to decrease the number of random choices the more sure we are about our decision.\n",
        "# \n",
        "# To achieve this we can use gradually decrease epsilon as we are gaining more confidence on the estimated payout of each bandit.\n",
        "# \n",
        "# For this exercise please adapt the function run_simulation_decay After completing the task please try different values for epsilon and epsilon_decay.\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "# Run the trail for `n` iteration\n",
        "def run_simulation(n_iterations, iterations_to_plot, epsilon=0.2, decay=0.01):\n",
        "\n",
        "    # Setup plot\n",
        "    fig1, axs1 = plt.subplots(3, 2, figsize=(8, 10))\n",
        "    axs1 = axs1.flat\n",
        "    fig2, axs2 = plt.subplots(3, 2, figsize=(8, 10))\n",
        "    axs2 = axs2.flat\n",
        "\n",
        "    # The number of trials and wins will represent the prior for each\n",
        "    #  bandit with the help of the Beta distribution.\n",
        "    trials = [0.01, 0.01, 0.01]  # Number of times we tried each bandit\n",
        "    wins = [0.01, 0.01, 0.01]  # Number of wins for each bandit\n",
        "    \n",
        "    for iteration in range(1, n_iterations+1):\n",
        "        # Define the prior based on current observations\n",
        "        results = [w/t for w,t in zip(wins, trials)]\n",
        "        if iteration in iterations_to_plot:\n",
        "            ax1 = next(axs1)\n",
        "            ax1.set_xticklabels(('B1', 'B2', 'B3'))\n",
        "            ax1.bar([1, 2, 3], results)\n",
        "            ax1.set_title(f'Extimated Probability at iteration {iteration:d}')\n",
        "            ax2 = next(axs2)\n",
        "            ax2.bar([1, 2, 3], wins)\n",
        "#             ax2.set_ylabel('Cumulative wins', color='r')\n",
        "            ax2.set_title(f'Cumulative wins at iteration {iteration:d}')\n",
        "\n",
        "        # Use epsilon greedy to choose a bandit\n",
        "        chosen_bandit = epsilon_greedy(results, epsilon*decay)\n",
        "        # Pull the bandit\n",
        "        x = pull(chosen_bandit)\n",
        "        # Update trials and wins (defines the posterior)\n",
        "        trials[chosen_bandit] += 1\n",
        "        wins[chosen_bandit] += x\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "run_simulation(n_iterations=10_000, iterations_to_plot=iterations_to_plot)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}