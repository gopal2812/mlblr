{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled32.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopal2812/mlblr/blob/master/reinforcementlearningsarsa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjVEvdVsXK6d",
        "colab_type": "text"
      },
      "source": [
        "#Sarsa Algorithm\n",
        "Our agent needs to cross a lake, represented by a square divided with a 4x4 grid. At every step he needs to take an action, he can go up, down, left or right. If it's not possible to go to the chosen direction our agent will simply not move.Each grid element can be frozen, in that case our agent will be able to move, it can he a hole, in that case the episodes end and we don't get the reward or it can be the final state, the goal. In that case the agent will receive a positive reward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ey6bQ1gXRbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "# Meta parameters for the RL agent\n",
        "\n",
        "alpha = 0.1\n",
        "gamma = 0.99\n",
        "epsilon = 0.5\n",
        "epsilon_decay = 0.999\n",
        "verbose = False\n",
        "\n",
        "# Define types of algorithms\n",
        "\n",
        "# Choose methods for learning and exploration\n",
        "rl_algorithm = \"SARSA\"\n",
        "\n",
        "explore_methods = ['GREEDY', 'EPSILON_GREEDY', 'SOFTMAX']\n",
        "explore_method = explore_methods[1]\n",
        "print(f'Explore mehtod chosen: {explore_method}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuDJVSY2Xgd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Compute softmax values. \n",
        "    Returns a vector with probabilities.\n",
        "    \"\"\"\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / exp_x.sum()\n",
        "\n",
        "\n",
        "def take_action_sing_softmax(s, q):\n",
        "    \"\"\"Act with softmax\"\"\"\n",
        "    prob_a = softmax(q[s, :])\n",
        "    cumsum_a = np.cumsum(prob_a)\n",
        "    return np.where(np.random.rand() < cumsum_a)[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc_fSKWTX_Tq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def take_action_using_epsilon_greedy(s, q):\n",
        "    \"\"\"Act with epsilon greedy\"\"\"\n",
        "    a = np.argmax(q[s, :])\n",
        "    if np.random.rand() < epsilon:\n",
        "        a = np.random.randint(q.shape[1])\n",
        "    return a\n",
        "\n",
        "\n",
        "def sarsa_update(q, s, a, r, s_prime, a_prime):\n",
        "    \"\"\"Compute SARSA update\"\"\"\n",
        "\n",
        "    td = r + gamma * q[s_prime, a_prime] - q[s, a]\n",
        "    return q[s, a] + alpha * td"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwlwPdxYlaa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(q, env, n, h, explore_type):\n",
        "    \"\"\"Evaluate a policy on n runs\"\"\"\n",
        "\n",
        "    success_rate = 0.0\n",
        "    mean_return = 0.0\n",
        "\n",
        "    for i in range(n):\n",
        "        discounted_return = 0.0\n",
        "        s = env.reset()\n",
        "\n",
        "        for step in range(h):\n",
        "            if explore_type == \"GREEDY\":\n",
        "                s, r, done, info = env.step(np.argmax(q[s, :]))\n",
        "            elif explore_type == \"EPSILON_GREEDY\":\n",
        "                s, r, done, info = env.step(\n",
        "                    take_action_using_epsilon_greedy(s, q))\n",
        "            elif explore_type == \"SOFTMAX\":\n",
        "                s, r, done, info = env.step(take_action_sing_softmax(s, q))\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    \"Wrong Explore Method in evaluation:\".format(explore_type))\n",
        "\n",
        "            discounted_return += np.power(gamma, step) * r\n",
        "\n",
        "            if done:\n",
        "                success_rate += float(r) / n\n",
        "                mean_return += float(discounted_return) / n\n",
        "                break\n",
        "\n",
        "    return success_rate, mean_return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZfFnfiOllzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Choose environment\n",
        "env_name = 'FrozenLake-v0'\n",
        "\n",
        "# Random seed\n",
        "np.random.RandomState(121)\n",
        "\n",
        "# Create Environment\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# Take State-Action space size\n",
        "n_a = env.action_space.n\n",
        "n_s = env.observation_space.n\n",
        "\n",
        "# Experimental setup\n",
        "n_episode = 3000\n",
        "print(\"n_episode \", n_episode)\n",
        "max_horizon = 100\n",
        "eval_steps = 10\n",
        "\n",
        "# Monitoring performance\n",
        "window = deque(maxlen=100)\n",
        "\n",
        "greedy_success_rate_monitor = np.zeros([n_episode, 1])\n",
        "greedy_discounted_return_monitor = np.zeros([n_episode, 1])\n",
        "\n",
        "behaviour_success_rate_monitor = np.zeros([n_episode, 1])\n",
        "behaviour_discounted_return_monitor = np.zeros([n_episode, 1])\n",
        "\n",
        "# Init Q-table\n",
        "q_table = np.zeros([n_s, n_a])\n",
        "\n",
        "env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7sMr1AQlvL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train for n_episode\n",
        "for i_episode in range(n_episode):\n",
        "\n",
        "    # Reset the expisode's cumulative reward\n",
        "    total_return = 0.0\n",
        "\n",
        "    # Start fresh with a new episode state\n",
        "    s = env.reset()\n",
        "\n",
        "    # Select the first action in this episode\n",
        "\n",
        "    a = take_action_using_epsilon_greedy(s, q_table)\n",
        "\n",
        "    for i_step in range(max_horizon):\n",
        "\n",
        "        # Take action and receive the enviroment's feedback\n",
        "        s_prime, r, done, info = env.step(a)\n",
        "\n",
        "        total_return += np.power(gamma, i_step) * r\n",
        "\n",
        "        # Select an action\n",
        "        if explore_method == \"SOFTMAX\":\n",
        "            a_prime = take_action_using_softmax(s_prime, q_table)\n",
        "        elif explore_method == \"EPSILON_GREEDY\":\n",
        "            a_prime = take_action_using_epsilon_greedy(s_prime, q_table)\n",
        "        else:\n",
        "            raise ValueError(\"Wrong Explore Method:\".format(explore_method))\n",
        "\n",
        "        # Update a Q value table\n",
        "        if rl_algorithm == 'SARSA':\n",
        "            q_table[s, a] = sarsa_update(q_table, s, a, r, s_prime, a_prime)\n",
        "        else:\n",
        "            raise ValueError(\"Wrong RL algorithm:\".format(rl_algorithm))\n",
        "\n",
        "        # Transition to new state\n",
        "        s = s_prime\n",
        "        a = a_prime\n",
        "\n",
        "        if done:\n",
        "            window.append(r)\n",
        "            last_100 = window.count(1)\n",
        "\n",
        "            greedy_success_rate_monitor[i_episode - 1, 0], greedy_discounted_return_monitor[\n",
        "                i_episode - 1, 0] = evaluate_policy(q_table, env, eval_steps, max_horizon, explore_method)\n",
        "            behaviour_success_rate_monitor[i_episode - 1, 0], behaviour_discounted_return_monitor[\n",
        "                i_episode - 1, 0] = evaluate_policy(q_table, env, eval_steps, max_horizon, explore_method)\n",
        "            if verbose:\n",
        "                print(\n",
        "                    \"Episode: {0}\\t Num_Steps: {1:>4}\\tTotal_Return: {2:>5.2f}\\tFinal_Reward: {3}\\tEpsilon: {4:.3f}\\tSuccess Rate: {5:.3f}\\tLast_100: {6}\".format(\n",
        "                        i_episode, i_step, total_return, r, epsilon, greedy_success_rate_monitor[i_episode - 1, 0],\n",
        "                        last_100))\n",
        "\n",
        "            break\n",
        "\n",
        "    # Schedule for epsilon\n",
        "    epsilon = epsilon * epsilon_decay\n",
        "\n",
        "# env.render()\n",
        "\n",
        "plt.figure(0)\n",
        "plt.plot(range(0, n_episode, 10), greedy_success_rate_monitor[0::10, 0])\n",
        "plt.title(\"Greedy policy with {0} and {1}\".format(rl_algorithm, explore_method))\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Success Rate\")\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(range(0, n_episode, 10), behaviour_success_rate_monitor[0::10, 0])\n",
        "plt.title(\"Behaviour policy with {0} and {1}\".format(rl_algorithm, explore_method))\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Success Rate\")\n",
        "plt.show()\n",
        "\n",
        "# Show an episod\n",
        "\n",
        "for i_step in range(max_horizon):\n",
        "#     env.render()\n",
        "    a = np.argmax(q_table[s, :])\n",
        "    s, r, done, info = env.step(a)\n",
        "    total_return += np.power(gamma, i_step) * r\n",
        "\n",
        "    if done:\n",
        "        print(\"Episode: {0}\\t Num_Steps: {1:>4}\\tTotal_Return: {2:>5.2f}\\tFinal_Reward: {3}\".format(1, i_step,\n",
        "                                                                                                    total_return,\n",
        "                                                                                                    r))\n",
        "        break\n",
        "\n",
        "# Show Policy\n",
        "\n",
        "for s in range(n_s):\n",
        "    actions = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
        "    print(actions[np.argmax(q_table[s, :])])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}